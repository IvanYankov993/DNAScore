{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52dc179-c6f7-4d36-a20b-b99c40eb9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9c03a-caae-4ce4-ad9a-5437ab5db00b",
   "metadata": {},
   "source": [
    "# DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9a93f7-7ae0-4f5f-af9e-5f5b0894f005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/dev'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home=os.getcwd()\n",
    "home"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9e98a80-5c40-4e7c-b263-73b260fce37f",
   "metadata": {},
   "source": [
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "mc_cv=2\n",
    "n_folds=5\n",
    "n_jobs=1\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(1, test_size=test_frac, random_state=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68bd32fb-befc-494a-8756-8cce72dcbbc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# prop='Granulated' \n",
    "# GSHT='dH'\n",
    "# file=pd.read_csv(\"/users/qdb16186/CNN_stk/Lomzov_dataset_IY.csv\")\n",
    "file=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "\n",
    "# obtain y and x data\n",
    "# y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "# desc_type = ['Granulated','OHEP','LP_dec2','DNA-Groups']\n",
    "desc_type = ['Granulated']\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca81d6-2822-418a-9b01-168d97704192",
   "metadata": {},
   "source": [
    "# Data CV Block\n",
    "\n",
    "to adjust CV and hyper parameter setup access:\n",
    "cv_hp,\n",
    "adjust test_size and shuffle split\n",
    "<!--  -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f593287a-363e-4301-9d74-8b16f6f329c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_fold(home,resample,i_fold):\n",
    "    path=\"{}/CV/{}/fold_{}\".format(os.getcwd(),resample,i_fold)\n",
    "        \n",
    "    # Define the directory path\n",
    "    directory_path = Path(f\"{home}/CV/{resample}/{i_fold}\")\n",
    "    \n",
    "    # Ensure the directory exists, create it if necessary\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return directory_path\n",
    "\n",
    "def path_resample(home,resample):\n",
    "    path=\"{}/CV/{}/\".format(os.getcwd(),resample)\n",
    "        \n",
    "    # Define the directory path\n",
    "    directory_path = Path(f\"{home}/CV/{resample}\")\n",
    "    \n",
    "    # Ensure the directory exists, create it if necessary\n",
    "    directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return directory_path\n",
    "\n",
    "def cv_hp(df,home):\n",
    "    resample_split  = ShuffleSplit(50, test_size=0.3, random_state=1)\n",
    "    fold_split      = ShuffleSplit(5 , test_size=0.3, random_state=1)\n",
    "    train_val_split = ShuffleSplit(1 , test_size=0.3, random_state=1)\n",
    "    \n",
    "    for resample, (train_val_index, test_index) in enumerate(resample_split.split(df)):\n",
    "        train_val = pd.DataFrame(df['ID'].iloc[train_val_index])\n",
    "        test = pd.DataFrame(df['ID'].iloc[test_index])\n",
    "        for i, (train_index, val_index) in enumerate(train_val_split.split(train_val)):\n",
    "            train = pd.DataFrame(df['ID'].iloc[train_index])\n",
    "            val   = pd.DataFrame(df['ID'].iloc[val_index])\n",
    "        resample_path = path_resample(home,resample)\n",
    "        train.to_csv(f'{resample_path}/train.csv')\n",
    "        val.to_csv(f'{resample_path}/val.csv')\n",
    "        test.to_csv(f'{resample_path}/test.csv')\n",
    "        # train,val,test to_csv\n",
    "        for i_fold, (train_val_fold_index, test_fold_index) in enumerate(fold_split.split(train)):\n",
    "            train_val_fold = pd.DataFrame(train['ID'].iloc[train_val_fold_index])\n",
    "            test_fold = pd.DataFrame(train['ID'].iloc[test_fold_index])\n",
    "            for i, (train_fold_index, val_fold_index) in enumerate(train_val_split.split(train_val_fold)):\n",
    "                train_fold = pd.DataFrame(train_val_fold['ID'].iloc[train_fold_index])\n",
    "                val_fold   = pd.DataFrame(train_val_fold['ID'].iloc[val_fold_index])\n",
    "            i_fold_path = path_fold(home,resample,i_fold)\n",
    "            train_fold.to_csv(f'{i_fold_path}/train.csv')\n",
    "            val_fold.to_csv(f'{i_fold_path}/val.csv')\n",
    "            test_fold.to_csv(f'{i_fold_path}/test.csv')\n",
    "            \n",
    "\n",
    "    return print(\"data organised into 50 CV with 5-fold inner CV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277b0477-57d9-4ec2-b600-d83d5544c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data organised into 50 CV with 5-fold inner CV\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "home=os.getcwd()\n",
    "# pathlib.Path(\"Lomzov_dataset_IY.csv\").parent.absolute()\n",
    "cv_hp(df,home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29f92f-0281-4520-b75f-d79d8cc47326",
   "metadata": {},
   "source": [
    "# Accessing Data Via ID CV and full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a93fabd-2f38-49e0-acfd-7616aeda9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_fold_csv(df,home,resample,fold):\n",
    "    df_path = path_fold(home,resample,fold)\n",
    "    train_df=pd.read_csv(f'{df_path}/train.csv')\n",
    "    val_df=pd.read_csv(f'{df_path}/val.csv')\n",
    "    test_df=pd.read_csv(f'{df_path}/test.csv')\n",
    "\n",
    "    train_df=df[df[\"ID\"].isin(train_df['ID'])]\n",
    "    val_df=df[df[\"ID\"].isin(val_df['ID'])]\n",
    "    test_df=df[df[\"ID\"].isin(test_df['ID'])]\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def access_resample_csv(df,home,resample):\n",
    "    df_path = path_resample(home,resample)\n",
    "    train_df=pd.read_csv(f'{df_path}/train.csv')\n",
    "    val_df=pd.read_csv(f'{df_path}/val.csv')\n",
    "    test_df=pd.read_csv(f'{df_path}/test.csv')\n",
    "\n",
    "    train_df=df[df[\"ID\"].isin(train_df['ID'])]\n",
    "    val_df=df[df[\"ID\"].isin(val_df['ID'])]\n",
    "    test_df=df[df[\"ID\"].isin(test_df['ID'])]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d044b9a-bad2-4c19-a80c-216d59d73d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=1\n",
    "fold=1\n",
    "train_fold, val_fold, test_fold = access_resample_csv(df,home,resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01f87df-476e-4705-870f-3f03c8060e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 383) (64, 383) (92, 383)\n",
      "305\n"
     ]
    }
   ],
   "source": [
    "print(train_fold.shape, val_fold.shape, test_fold.shape)\n",
    "# train_fold.shape\n",
    "print(train_fold.shape[0] + val_fold.shape[0] + test_fold.shape[0])\n",
    "# train_fold.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00baf557-decb-4389-99a9-d4a953157575",
   "metadata": {},
   "source": [
    "# Load X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ec19c-d386-46b6-b113-03df1b7f6997",
   "metadata": {},
   "source": [
    "## Def Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206f944d-5b9a-4e45-a455-7138ee0e8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "\n",
    "def load_xy(file,desc):\n",
    "    # Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "\n",
    "    Y = file[['dH','dS','dG','Tm']].copy()\n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{desc}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, Y, padding(X), X\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98727f-18b4-4f41-b129-fad50fee14c1",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db9861a-c8d2-4141-9367-bc1768903c7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data organised into 50 CV with 5-fold inner CV\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "home=os.getcwd()\n",
    "# pathlib.Path(\"Lomzov_dataset_IY.csv\").parent.absolute()\n",
    "cv_hp(df,home)\n",
    "\n",
    "resample=1\n",
    "fold=1\n",
    "train, val, test = access_resample_csv(df,home,resample)\n",
    "train_fold, val_fold, test_fold = access_fold_csv(df,home,resample,fold)\n",
    "\n",
    "desc='Granulated'\n",
    "y_1_train, y_2_train, y_3_train, y_4_train, Y_train, X_padded_train, X_train = load_xy(train_fold,desc)\n",
    "y_1_val,   y_2_val,   y_3_val,   y_4_val,   Y_val,   X_padded_val,   X_val   = load_xy(val_fold,desc)\n",
    "y_1_test,  y_2_test,  y_3_test,  y_4_test,  Y_test,  X_padded_test,  X_test  = load_xy(test_fold,desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fed8c-d719-4657-ac43-c914357717bb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60afa654-5218-4d52-a889-cd123e0d3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 10:44:16.315269: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 10:44:17.156278: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 10:44:17.156976: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 10:44:23.114909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97ba1bfc-ddd0-4230-a664-4c9602885b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a12b3d-a919-4da8-94f5-3927a5f76a0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model with hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d6b4ddf3-2b70-4bab-ab94-c7ab6bf04fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "#     Hyper parameters         \n",
    "    model_type1 = hp.Choice(\"model_type1\", [\"CNN3\",\"CNN2\",\"CNN1\"])\n",
    "    model_type = hp.Choice(\"model_type\", [\"Dense3\"])\n",
    " \n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x_layer=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    # with hp.conditional_scope(\"model_type1\", [\"CNN0\"]):\n",
    "    #         if model_type1 == \"CNN0\":\n",
    "    #             pass\n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    with hp.conditional_scope(\"model_type1\", [\"CNN1\",\"CNN2\"\"CNN3\"]):\n",
    "            if model_type1 != \"CNN0\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        input_shape=(250,1),\n",
    "                        name = 'conv1d_1'\n",
    "                        )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = 'batchnorm_1')(x_layer)\n",
    "                pass\n",
    "                \n",
    "            if model_type1 != \"CNN1\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_2'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_2')(x_layer)\n",
    "\n",
    "            if model_type1 != \"CNN1\" or \"CNN2\":               \n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_3'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_3')(x_layer)\n",
    "                \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    x_layer = keras.layers.Flatten(name = 'flatten')(x_layer)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    # with hp.conditional_scope(\"model_type\", [\"Dense0\"]):\n",
    "    #     if model_type == \"Dense0\":\n",
    "    #         pass\n",
    "            \n",
    "    with hp.conditional_scope(\"model_type\", [\"Dense3\"]): #[\"Dense1\",\"Dense2\",\"Dense3\"]\n",
    "        if model_type != \"Dense0\":\n",
    "            hp_layer_1= hp.Choice(f'layer_1', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_1,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        # name='layer_1',\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "        if model_type != \"Dense1\":\n",
    "            hp_layer_2_2= hp.Choice(f'layer_2_2', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_2_2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "\n",
    "        if model_type != \"Dense1\" or \"Dense2\":\n",
    "            hp_layer_3_3= hp.Choice(f'layer_3_3',  values=[16,32,64])\n",
    "            \n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_3_3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "#     OUTPUT LAYERS\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='melting_temperature')(x_layer)\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='dH')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='dS')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='dG')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='Tm')(x_layer)\n",
    "    \n",
    "\n",
    "    # model = Model(inputs=inputs, outputs=[output_1, output_2, output_3, output_4])\n",
    "    output_1 = keras.layers.Dense(1, name='output')(x_layer)\n",
    "    model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS\n",
    "#     SETTINGS\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636ff1d-b00d-45d6-8297-c9aa1070ec06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model without hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "01cb1b9e-0146-4509-8596-62479d2c756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp=0):\n",
    "     \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x_layer=inputs\n",
    "    \n",
    "    # Conv layer 1\n",
    "    x_layer = keras.layers.Conv1D(32, \n",
    "            kernel_size=(3), \n",
    "            strides=(2), \n",
    "            padding='valid', \n",
    "            activation='relu', \n",
    "            input_shape=(250,1),\n",
    "            name = 'conv1d_1'\n",
    "            )(x_layer)\n",
    "    x_layer = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x_layer)\n",
    "    x_layer = keras.layers.BatchNormalization(name = 'batchnorm_1')(x_layer)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv layer 2\n",
    "    x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        name = f'conv1d_2'\n",
    "                        )(x_layer)\n",
    "    x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x_layer)\n",
    "    x_layer = keras.layers.BatchNormalization(name = f'batchnorm_2')(x_layer)\n",
    "\n",
    "\n",
    "    # Conv layer 3\n",
    "    x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        name = f'conv1d_3'\n",
    "                        )(x_layer)\n",
    "    x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x_layer)\n",
    "    x_layer = keras.layers.BatchNormalization(name = f'batchnorm_3')(x_layer)\n",
    "                \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    x_layer = keras.layers.Flatten(name = 'flatten')(x_layer)\n",
    "    \n",
    "    \n",
    "    #Node 1\n",
    "    node1_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node1_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node1_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                name='node1_2',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node1_layer)\n",
    "\n",
    "\n",
    "    node1_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                name='node1_3',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node1_layer)\n",
    "    \n",
    "    \n",
    "    #Node 2\n",
    "    node2_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node2_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node2_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                name='node2_2',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node2_layer)\n",
    "\n",
    "\n",
    "    node2_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                name='node2_3',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node2_layer)\n",
    "    \n",
    "    #Node 3\n",
    "    node3_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node3_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node3_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                name='node3_2',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node3_layer)\n",
    "\n",
    "\n",
    "    node3_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                name='node3_3',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node3_layer)\n",
    "    \n",
    "    #Node 4\n",
    "    node4_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node4_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node4_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node4_2',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node4_layer)\n",
    "\n",
    "\n",
    "    node4_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node4_3',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node4_layer)\n",
    "#     OUTPUT LAYERS\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='melting_temperature')(x_layer)\n",
    "\n",
    "    output_1 = keras.layers.Dense(1, name='dH')(node1_layer)\n",
    "    output_2 = keras.layers.Dense(1, name='dS')(node2_layer)\n",
    "    output_3 = keras.layers.Dense(1, name='dG')(node3_layer)\n",
    "    output_4 = keras.layers.Dense(1, name='Tm')(node4_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[output_1, output_2, output_3, output_4])\n",
    "    # output_1 = keras.layers.Dense(1, name='output')(x_layer)\n",
    "    # model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS\n",
    "\n",
    "# #     ADAPTIVE LEARNING RATE   \n",
    "    \n",
    "#     initial_learning_rate = 0.01\n",
    "#     decay_steps = 10.0\n",
    "#     decay_rate = 0.5\n",
    "#     learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "#                                     initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "# #     SETTING ADAM OPTIMISER\n",
    "#     optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "# #     COMPILE MODEl\n",
    "#     model.compile(loss = \"mse\" , \n",
    "#                   optimizer = optimiser, \n",
    "#                   metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede32498-0459-4314-a831-24e946803013",
   "metadata": {},
   "source": [
    "### 2CNN 2NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23a3c52a-1f28-4114-97ca-3383847e49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp=0):\n",
    "     \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x_layer=inputs\n",
    "    \n",
    "    # Conv layer 1\n",
    "    x_layer = keras.layers.Conv1D(32, \n",
    "            kernel_size=(3), \n",
    "            strides=(2), \n",
    "            padding='valid', \n",
    "            activation='relu', \n",
    "            input_shape=(250,1),\n",
    "            name = 'conv1d_1'\n",
    "            )(x_layer)\n",
    "    x_layer = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x_layer)\n",
    "    x_layer = keras.layers.BatchNormalization(name = 'batchnorm_1')(x_layer)\n",
    "\n",
    "\n",
    "\n",
    "    # Conv layer 2\n",
    "    x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        name = f'conv1d_2'\n",
    "                        )(x_layer)\n",
    "    x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x_layer)\n",
    "    x_layer = keras.layers.BatchNormalization(name = f'batchnorm_2')(x_layer)\n",
    "\n",
    "\n",
    "    # # Conv layer 3\n",
    "    # x_layer = keras.layers.Conv1D(32, \n",
    "    #                     kernel_size=(3), \n",
    "    #                     strides=(2), \n",
    "    #                     padding='valid', \n",
    "    #                     activation='relu', \n",
    "    #                     name = f'conv1d_3'\n",
    "    #                     )(x_layer)\n",
    "    # x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x_layer)\n",
    "    # x_layer = keras.layers.BatchNormalization(name = f'batchnorm_3')(x_layer)\n",
    "                \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    x_layer = keras.layers.Flatten(name = 'flatten')(x_layer)\n",
    "    \n",
    "    \n",
    "    #Node 1\n",
    "    node1_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node1_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node1_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                name='node1_2',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node1_layer)\n",
    "\n",
    "\n",
    "    # node1_layer = keras.layers.Dense(\n",
    "    #             16,\n",
    "    #             activation='relu',\n",
    "    #             name='node1_3',\n",
    "    #             use_bias=True,\n",
    "    #             kernel_initializer='glorot_uniform',\n",
    "    #             bias_initializer='zeros',\n",
    "    #             kernel_regularizer=None,\n",
    "    #             bias_regularizer=None,\n",
    "    #             activity_regularizer=None,\n",
    "    #             kernel_constraint=None,\n",
    "    #             bias_constraint=None\n",
    "    #         )(node1_layer)\n",
    "    \n",
    "    \n",
    "    #Node 2\n",
    "    node2_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node2_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node2_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                name='node2_2',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node2_layer)\n",
    "\n",
    "\n",
    "    # node2_layer = keras.layers.Dense(\n",
    "    #             16,\n",
    "    #             activation='relu',\n",
    "    #             name='node2_3',\n",
    "    #             use_bias=True,\n",
    "    #             kernel_initializer='glorot_uniform',\n",
    "    #             bias_initializer='zeros',\n",
    "    #             kernel_regularizer=None,\n",
    "    #             bias_regularizer=None,\n",
    "    #             activity_regularizer=None,\n",
    "    #             kernel_constraint=None,\n",
    "    #             bias_constraint=None\n",
    "    #         )(node2_layer)\n",
    "    \n",
    "    #Node 3\n",
    "    node3_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node3_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node3_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                name='node3_2',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node3_layer)\n",
    "\n",
    "\n",
    "    # node3_layer = keras.layers.Dense(\n",
    "    #             16,\n",
    "    #             activation='relu',\n",
    "    #             name='node3_3',\n",
    "    #             use_bias=True,\n",
    "    #             kernel_initializer='glorot_uniform',\n",
    "    #             bias_initializer='zeros',\n",
    "    #             kernel_regularizer=None,\n",
    "    #             bias_regularizer=None,\n",
    "    #             activity_regularizer=None,\n",
    "    #             kernel_constraint=None,\n",
    "    #             bias_constraint=None\n",
    "    #         )(node3_layer)\n",
    "    \n",
    "    #Node 4\n",
    "    node4_layer = keras.layers.Dense(\n",
    "                16,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node4_1',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(x_layer)\n",
    "\n",
    "    node4_layer = keras.layers.Dense(\n",
    "                32,\n",
    "                activation='relu',\n",
    "                use_bias=True,\n",
    "                name='node4_2',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )(node4_layer)\n",
    "\n",
    "\n",
    "    # node4_layer = keras.layers.Dense(\n",
    "    #             16,\n",
    "    #             activation='relu',\n",
    "    #             use_bias=True,\n",
    "    #             name='node4_3',\n",
    "    #             kernel_initializer='glorot_uniform',\n",
    "    #             bias_initializer='zeros',\n",
    "    #             kernel_regularizer=None,\n",
    "    #             bias_regularizer=None,\n",
    "    #             activity_regularizer=None,\n",
    "    #             kernel_constraint=None,\n",
    "    #             bias_constraint=None\n",
    "    #         )(node4_layer)\n",
    "#     OUTPUT LAYERS\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='melting_temperature')(x_layer)\n",
    "\n",
    "    output_1 = keras.layers.Dense(1, name='dH')(node1_layer)\n",
    "    output_2 = keras.layers.Dense(1, name='dS')(node2_layer)\n",
    "    output_3 = keras.layers.Dense(1, name='dG')(node3_layer)\n",
    "    output_4 = keras.layers.Dense(1, name='Tm')(node4_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[output_1, output_2, output_3, output_4])\n",
    "    # output_1 = keras.layers.Dense(1, name='output')(x_layer)\n",
    "    # model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS\n",
    "\n",
    "# #     ADAPTIVE LEARNING RATE   \n",
    "    \n",
    "#     initial_learning_rate = 0.01\n",
    "#     decay_steps = 10.0\n",
    "#     decay_rate = 0.5\n",
    "#     learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "#                                     initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "# #     SETTING ADAM OPTIMISER\n",
    "#     optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "# #     COMPILE MODEl\n",
    "#     model.compile(loss = \"mse\" , \n",
    "#                   optimizer = optimiser, \n",
    "#                   metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d068c-f7fd-4426-8966-4d9534a862f0",
   "metadata": {},
   "source": [
    "## Initiaalise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253a506-27e2-476b-8adb-97e24d15f0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77390e01-3c30-4610-a26d-023f8a48988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/tunner/16/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch  = 16\n",
    "# TODO: Use path lib to create path\n",
    "fold_path = path_fold(home,resample,fold)\n",
    "resample_path = path_resample(home,resample)\n",
    "\n",
    "# model_name = architecture of - Single task/RF/KNN + dH/dG/dS/Tm or - Multitask  \n",
    "prop = 'HGST' \n",
    "model_name = f\"1DConv_mt_{prop}\" \n",
    "\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = Path(f\"{fold_path}/{desc}/{model_name}/\")\n",
    "\n",
    "tunner_path      = Path(f'{directory_path}/tunner')\n",
    "csv_logger_path  = Path(f'{directory_path}/csv_logger/')\n",
    "cp_callback_path = Path(f'{directory_path}/model_checkpoint/')\n",
    "tensorboard_path = Path(f'{directory_path}/tensorboard_logs/')\n",
    "\n",
    "# Ensure the directory exists, create it if necessary\n",
    "tunner_path.mkdir(parents=True, exist_ok=True)\n",
    "csv_logger_path.mkdir(parents=True, exist_ok=True)\n",
    "cp_callback_path.mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "tuner = kt.GridSearch(build_model,\n",
    "                   objective=kt.Objective('val_loss', 'min'),\n",
    "                    # loss = 'val_loss',\n",
    "                   # objective = ['val_mse','val_free_energy_pred_mse'],\n",
    "                  directory=tunner_path,\n",
    "                  overwrite=False,\n",
    "                  project_name=f'{batch}')\n",
    "\n",
    "with open(f'{tunner_path}/tuner_path.txt', 'w') as f:\n",
    "    f.write(tuner.project_dir)\n",
    "f.close\n",
    "\n",
    "#### CALL BACKS!\n",
    "es = EarlyStopping(monitor      = 'val_loss', \n",
    "                        mode     = 'min', \n",
    "                        verbose  = 1, \n",
    "                        patience = 2000, \n",
    "                    restore_best_weights = True)\n",
    "# CSV Logger\n",
    "csv_logger = CSVLogger(f'{csv_logger_path}/model_history.csv' , append=True)\n",
    "\n",
    "# CP_callbacks      not required when using a tunner       \n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{cp_callback_path}/cp.ckpt',\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1)\n",
    "\n",
    "# TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_path, \n",
    "                                                       #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                      update_freq = 1,\n",
    "                                                      # histogram_freq=1, \n",
    "                                                      write_graph=False, \n",
    "                                                      write_images=False)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "\n",
    "# Covert to list and provide to Keras Regressor\n",
    "keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "\n",
    "\n",
    "# history=tuner.search(x_hp_train, y_hp_train[:],\n",
    "#             epochs = epochs,\n",
    "#             batch_size=batch,\n",
    "#             verbose = 2,\n",
    "#             validation_data =(x_hp_val, y_hp_val[:]),\n",
    "#              # validation_split = 0.2,\n",
    "#             callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd1a24-b600-4722-bfd1-3ba39b306b3e",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3c89a4-c9c3-4759-b39c-1a7403767178",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1_train, y_2_train, y_3_train, y_4_train, Y_train, X_padded_train, X_train = load_xy(train_fold,desc)\n",
    "y_1_val,   y_2_val,   y_3_val,   y_4_val,   Y_val,   X_padded_val,   X_val   = load_xy(val_fold,desc)\n",
    "y_1_test,  y_2_test,  y_3_test,  y_4_test,  Y_test,  X_padded_test,  X_test  = load_xy(test_fold,desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "916d724c-fd50-4d4c-976f-28d4c2f13328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7b9faeb-1540-4e23-b8df-2a2b46d631ec",
   "metadata": {},
   "source": [
    "history=tuner.search(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "            epochs = epochs,\n",
    "            batch_size=batch,\n",
    "            verbose = 2,\n",
    "            validation_data =(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "             # validation_split = 0.2,\n",
    "            callbacks=keras_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43994c02-6598-4ace-b0f3-c271203a234c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 10:44:31.858602: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "Epoch 2/600\n",
      "Epoch 3/600\n",
      "Epoch 4/600\n",
      "Epoch 5/600\n",
      "Epoch 6/600\n",
      "Epoch 7/600\n",
      "Epoch 8/600\n",
      "Epoch 9/600\n",
      "Epoch 10/600\n",
      "Epoch 11/600\n",
      "Epoch 12/600\n",
      "Epoch 13/600\n",
      "Epoch 14/600\n",
      "Epoch 15/600\n",
      "Epoch 16/600\n",
      "Epoch 17/600\n",
      "Epoch 18/600\n",
      "Epoch 19/600\n",
      "Epoch 20/600\n",
      "Epoch 21/600\n",
      "Epoch 22/600\n",
      "Epoch 23/600\n",
      "Epoch 24/600\n",
      "Epoch 25/600\n",
      "Epoch 26/600\n",
      "Epoch 27/600\n",
      "Epoch 28/600\n",
      "Epoch 29/600\n",
      "Epoch 30/600\n",
      "Epoch 31/600\n",
      "Epoch 32/600\n",
      "Epoch 33/600\n",
      "Epoch 34/600\n",
      "Epoch 35/600\n",
      "Epoch 36/600\n",
      "Epoch 37/600\n",
      "Epoch 38/600\n",
      "Epoch 39/600\n",
      "Epoch 40/600\n",
      "Epoch 41/600\n",
      "Epoch 42/600\n",
      "Epoch 43/600\n",
      "Epoch 44/600\n",
      "Epoch 45/600\n",
      "Epoch 46/600\n",
      "Epoch 47/600\n",
      "Epoch 48/600\n",
      "Epoch 49/600\n",
      "Epoch 50/600\n",
      "Epoch 51/600\n",
      "Epoch 52/600\n",
      "Epoch 53/600\n",
      "Epoch 54/600\n",
      "Epoch 55/600\n",
      "Epoch 56/600\n",
      "Epoch 57/600\n",
      "Epoch 58/600\n",
      "Epoch 59/600\n",
      "Epoch 60/600\n",
      "Epoch 61/600\n",
      "Epoch 62/600\n",
      "Epoch 63/600\n",
      "Epoch 64/600\n",
      "Epoch 65/600\n",
      "Epoch 66/600\n",
      "Epoch 67/600\n",
      "Epoch 68/600\n",
      "Epoch 69/600\n",
      "Epoch 70/600\n",
      "Epoch 71/600\n",
      "Epoch 72/600\n",
      "Epoch 73/600\n",
      "Epoch 74/600\n",
      "Epoch 75/600\n",
      "Epoch 76/600\n",
      "Epoch 77/600\n",
      "Epoch 78/600\n",
      "Epoch 79/600\n",
      "Epoch 80/600\n",
      "Epoch 81/600\n",
      "Epoch 82/600\n",
      "Epoch 83/600\n",
      "Epoch 84/600\n",
      "Epoch 85/600\n",
      "Epoch 86/600\n",
      "Epoch 87/600\n",
      "Epoch 88/600\n",
      "Epoch 89/600\n",
      "Epoch 90/600\n",
      "Epoch 91/600\n",
      "Epoch 92/600\n",
      "Epoch 93/600\n",
      "Epoch 94/600\n",
      "Epoch 95/600\n",
      "Epoch 96/600\n",
      "Epoch 97/600\n",
      "Epoch 98/600\n",
      "Epoch 99/600\n",
      "Epoch 100/600\n",
      "Epoch 101/600\n",
      "Epoch 102/600\n",
      "Epoch 103/600\n",
      "Epoch 104/600\n",
      "Epoch 105/600\n",
      "Epoch 106/600\n",
      "Epoch 107/600\n",
      "Epoch 108/600\n",
      "Epoch 109/600\n",
      "Epoch 110/600\n",
      "Epoch 111/600\n",
      "Epoch 112/600\n",
      "Epoch 113/600\n",
      "Epoch 114/600\n",
      "Epoch 115/600\n",
      "Epoch 116/600\n",
      "Epoch 117/600\n",
      "Epoch 118/600\n",
      "Epoch 119/600\n",
      "Epoch 120/600\n",
      "Epoch 121/600\n",
      "Epoch 122/600\n",
      "Epoch 123/600\n",
      "Epoch 124/600\n",
      "Epoch 125/600\n",
      "Epoch 126/600\n",
      "Epoch 127/600\n",
      "Epoch 128/600\n",
      "Epoch 129/600\n",
      "Epoch 130/600\n",
      "Epoch 131/600\n",
      "Epoch 132/600\n",
      "Epoch 133/600\n",
      "Epoch 134/600\n",
      "Epoch 135/600\n",
      "Epoch 136/600\n",
      "Epoch 137/600\n",
      "Epoch 138/600\n",
      "Epoch 139/600\n",
      "Epoch 140/600\n",
      "Epoch 141/600\n",
      "Epoch 142/600\n",
      "Epoch 143/600\n",
      "Epoch 144/600\n",
      "Epoch 145/600\n",
      "Epoch 146/600\n",
      "Epoch 147/600\n",
      "Epoch 148/600\n",
      "Epoch 149/600\n",
      "Epoch 150/600\n",
      "Epoch 151/600\n",
      "Epoch 152/600\n",
      "Epoch 153/600\n",
      "Epoch 154/600\n",
      "Epoch 155/600\n",
      "Epoch 156/600\n",
      "Epoch 157/600\n",
      "Epoch 158/600\n",
      "Epoch 159/600\n",
      "Epoch 160/600\n",
      "Epoch 161/600\n",
      "Epoch 162/600\n",
      "Epoch 163/600\n",
      "Epoch 164/600\n",
      "Epoch 165/600\n",
      "Epoch 166/600\n",
      "Epoch 167/600\n",
      "Epoch 168/600\n",
      "Epoch 169/600\n",
      "Epoch 170/600\n",
      "Epoch 171/600\n",
      "Epoch 172/600\n",
      "Epoch 173/600\n",
      "Epoch 174/600\n",
      "Epoch 175/600\n",
      "Epoch 176/600\n",
      "Epoch 177/600\n",
      "Epoch 178/600\n",
      "Epoch 179/600\n",
      "Epoch 180/600\n",
      "Epoch 181/600\n",
      "Epoch 182/600\n",
      "Epoch 183/600\n",
      "Epoch 184/600\n",
      "Epoch 185/600\n",
      "Epoch 186/600\n",
      "Epoch 187/600\n",
      "Epoch 188/600\n",
      "Epoch 189/600\n",
      "Epoch 190/600\n",
      "Epoch 191/600\n",
      "Epoch 192/600\n",
      "Epoch 193/600\n",
      "Epoch 194/600\n",
      "Epoch 195/600\n",
      "Epoch 196/600\n",
      "Epoch 197/600\n",
      "Epoch 198/600\n",
      "Epoch 199/600\n",
      "Epoch 200/600\n",
      "Epoch 201/600\n",
      "Epoch 202/600\n",
      "Epoch 203/600\n",
      "Epoch 204/600\n",
      "Epoch 205/600\n",
      "Epoch 206/600\n",
      "Epoch 207/600\n",
      "Epoch 208/600\n",
      "Epoch 209/600\n",
      "Epoch 210/600\n",
      "Epoch 211/600\n",
      "Epoch 212/600\n",
      "Epoch 213/600\n",
      "Epoch 214/600\n",
      "Epoch 215/600\n",
      "Epoch 216/600\n",
      "Epoch 217/600\n",
      "Epoch 218/600\n",
      "Epoch 219/600\n",
      "Epoch 220/600\n",
      "Epoch 221/600\n",
      "Epoch 222/600\n",
      "Epoch 223/600\n",
      "Epoch 224/600\n",
      "Epoch 225/600\n",
      "Epoch 226/600\n",
      "Epoch 227/600\n",
      "Epoch 228/600\n",
      "Epoch 229/600\n",
      "Epoch 230/600\n",
      "Epoch 231/600\n",
      "Epoch 232/600\n",
      "Epoch 233/600\n",
      "Epoch 234/600\n",
      "Epoch 235/600\n",
      "Epoch 236/600\n",
      "Epoch 237/600\n",
      "Epoch 238/600\n",
      "Epoch 239/600\n",
      "Epoch 240/600\n",
      "Epoch 241/600\n",
      "Epoch 242/600\n",
      "Epoch 243/600\n",
      "Epoch 244/600\n",
      "Epoch 245/600\n",
      "Epoch 246/600\n",
      "Epoch 247/600\n",
      "Epoch 248/600\n",
      "Epoch 249/600\n",
      "Epoch 250/600\n",
      "Epoch 251/600\n",
      "Epoch 252/600\n",
      "Epoch 253/600\n",
      "Epoch 254/600\n",
      "Epoch 255/600\n",
      "Epoch 256/600\n",
      "Epoch 257/600\n",
      "Epoch 258/600\n",
      "Epoch 259/600\n",
      "Epoch 260/600\n",
      "Epoch 261/600\n",
      "Epoch 262/600\n",
      "Epoch 263/600\n",
      "Epoch 264/600\n",
      "Epoch 265/600\n",
      "Epoch 266/600\n",
      "Epoch 267/600\n",
      "Epoch 268/600\n",
      "Epoch 269/600\n",
      "Epoch 270/600\n",
      "Epoch 271/600\n",
      "Epoch 272/600\n",
      "Epoch 273/600\n",
      "Epoch 274/600\n",
      "Epoch 275/600\n",
      "Epoch 276/600\n",
      "Epoch 277/600\n",
      "Epoch 278/600\n",
      "Epoch 279/600\n",
      "Epoch 280/600\n",
      "Epoch 281/600\n",
      "Epoch 282/600\n",
      "Epoch 283/600\n",
      "Epoch 284/600\n",
      "Epoch 285/600\n",
      "Epoch 286/600\n",
      "Epoch 287/600\n",
      "Epoch 288/600\n",
      "Epoch 289/600\n",
      "Epoch 290/600\n",
      "Epoch 291/600\n",
      "Epoch 292/600\n",
      "Epoch 293/600\n",
      "Epoch 294/600\n",
      "Epoch 295/600\n",
      "Epoch 296/600\n",
      "Epoch 297/600\n",
      "Epoch 298/600\n",
      "Epoch 299/600\n",
      "Epoch 300/600\n",
      "Epoch 301/600\n",
      "Epoch 302/600\n",
      "Epoch 303/600\n",
      "Epoch 304/600\n",
      "Epoch 305/600\n",
      "Epoch 306/600\n",
      "Epoch 307/600\n",
      "Epoch 308/600\n",
      "Epoch 309/600\n",
      "Epoch 310/600\n",
      "Epoch 311/600\n",
      "Epoch 312/600\n",
      "Epoch 313/600\n",
      "Epoch 314/600\n",
      "Epoch 315/600\n",
      "Epoch 316/600\n",
      "Epoch 317/600\n",
      "Epoch 318/600\n",
      "Epoch 319/600\n",
      "Epoch 320/600\n",
      "Epoch 321/600\n",
      "Epoch 322/600\n",
      "Epoch 323/600\n",
      "Epoch 324/600\n",
      "Epoch 325/600\n",
      "Epoch 326/600\n",
      "Epoch 327/600\n",
      "Epoch 328/600\n",
      "Epoch 329/600\n",
      "Epoch 330/600\n",
      "Epoch 331/600\n",
      "Epoch 332/600\n",
      "Epoch 333/600\n",
      "Epoch 334/600\n",
      "Epoch 335/600\n",
      "Epoch 336/600\n",
      "Epoch 337/600\n",
      "Epoch 338/600\n",
      "Epoch 339/600\n",
      "Epoch 340/600\n",
      "Epoch 341/600\n",
      "Epoch 342/600\n",
      "Epoch 343/600\n",
      "Epoch 344/600\n",
      "Epoch 345/600\n",
      "Epoch 346/600\n",
      "Epoch 347/600\n",
      "Epoch 348/600\n",
      "Epoch 349/600\n",
      "Epoch 350/600\n",
      "Epoch 351/600\n",
      "Epoch 352/600\n",
      "Epoch 353/600\n",
      "Epoch 354/600\n",
      "Epoch 355/600\n",
      "Epoch 356/600\n",
      "Epoch 357/600\n",
      "Epoch 358/600\n",
      "Epoch 359/600\n",
      "Epoch 360/600\n",
      "Epoch 361/600\n",
      "Epoch 362/600\n",
      "Epoch 363/600\n",
      "Epoch 364/600\n",
      "Epoch 365/600\n",
      "Epoch 366/600\n",
      "Epoch 367/600\n",
      "Epoch 368/600\n",
      "Epoch 369/600\n",
      "Epoch 370/600\n",
      "Epoch 371/600\n",
      "Epoch 372/600\n",
      "Epoch 373/600\n",
      "Epoch 374/600\n",
      "Epoch 375/600\n",
      "Epoch 376/600\n",
      "Epoch 377/600\n",
      "Epoch 378/600\n",
      "Epoch 379/600\n",
      "Epoch 380/600\n",
      "Epoch 381/600\n",
      "Epoch 382/600\n",
      "Epoch 383/600\n",
      "Epoch 384/600\n",
      "Epoch 385/600\n",
      "Epoch 386/600\n",
      "Epoch 387/600\n",
      "Epoch 388/600\n",
      "Epoch 389/600\n",
      "Epoch 390/600\n",
      "Epoch 391/600\n",
      "Epoch 392/600\n",
      "Epoch 393/600\n",
      "Epoch 394/600\n",
      "Epoch 395/600\n",
      "Epoch 396/600\n",
      "Epoch 397/600\n",
      "Epoch 398/600\n",
      "Epoch 399/600\n",
      "Epoch 400/600\n",
      "Epoch 401/600\n",
      "Epoch 402/600\n",
      "Epoch 403/600\n",
      "Epoch 404/600\n",
      "Epoch 405/600\n",
      "Epoch 406/600\n",
      "Epoch 407/600\n",
      "Epoch 408/600\n",
      "Epoch 409/600\n",
      "Epoch 410/600\n",
      "Epoch 411/600\n",
      "Epoch 412/600\n",
      "Epoch 413/600\n",
      "Epoch 414/600\n",
      "Epoch 415/600\n",
      "Epoch 416/600\n",
      "Epoch 417/600\n",
      "Epoch 418/600\n",
      "Epoch 419/600\n",
      "Epoch 420/600\n",
      "Epoch 421/600\n",
      "Epoch 422/600\n",
      "Epoch 423/600\n",
      "Epoch 424/600\n",
      "Epoch 425/600\n",
      "Epoch 426/600\n",
      "Epoch 427/600\n",
      "Epoch 428/600\n",
      "Epoch 429/600\n",
      "Epoch 430/600\n",
      "Epoch 431/600\n",
      "Epoch 432/600\n",
      "Epoch 433/600\n",
      "Epoch 434/600\n",
      "Epoch 435/600\n",
      "Epoch 436/600\n",
      "Epoch 437/600\n",
      "Epoch 438/600\n",
      "Epoch 439/600\n",
      "Epoch 440/600\n",
      "Epoch 441/600\n",
      "Epoch 442/600\n",
      "Epoch 443/600\n",
      "Epoch 444/600\n",
      "Epoch 445/600\n",
      "Epoch 446/600\n",
      "Epoch 447/600\n",
      "Epoch 448/600\n",
      "Epoch 449/600\n",
      "Epoch 450/600\n",
      "Epoch 451/600\n",
      "Epoch 452/600\n",
      "Epoch 453/600\n",
      "Epoch 454/600\n",
      "Epoch 455/600\n",
      "Epoch 456/600\n",
      "Epoch 457/600\n",
      "Epoch 458/600\n",
      "Epoch 459/600\n",
      "Epoch 460/600\n",
      "Epoch 461/600\n",
      "Epoch 462/600\n",
      "Epoch 463/600\n",
      "Epoch 464/600\n",
      "Epoch 465/600\n",
      "Epoch 466/600\n",
      "Epoch 467/600\n",
      "Epoch 468/600\n",
      "Epoch 469/600\n",
      "Epoch 470/600\n",
      "Epoch 471/600\n",
      "Epoch 472/600\n",
      "Epoch 473/600\n",
      "Epoch 474/600\n",
      "Epoch 475/600\n",
      "Epoch 476/600\n",
      "Epoch 477/600\n",
      "Epoch 478/600\n",
      "Epoch 479/600\n",
      "Epoch 480/600\n",
      "Epoch 481/600\n",
      "Epoch 482/600\n",
      "Epoch 483/600\n",
      "Epoch 484/600\n",
      "Epoch 485/600\n",
      "Epoch 486/600\n",
      "Epoch 487/600\n",
      "Epoch 488/600\n",
      "Epoch 489/600\n",
      "Epoch 490/600\n",
      "Epoch 491/600\n",
      "Epoch 492/600\n",
      "Epoch 493/600\n",
      "Epoch 494/600\n",
      "Epoch 495/600\n",
      "Epoch 496/600\n",
      "Epoch 497/600\n",
      "Epoch 498/600\n",
      "Epoch 499/600\n",
      "Epoch 500/600\n",
      "Epoch 501/600\n",
      "Epoch 502/600\n",
      "Epoch 503/600\n",
      "Epoch 504/600\n",
      "Epoch 505/600\n",
      "Epoch 506/600\n",
      "Epoch 507/600\n",
      "Epoch 508/600\n",
      "Epoch 509/600\n",
      "Epoch 510/600\n",
      "Epoch 511/600\n",
      "Epoch 512/600\n",
      "Epoch 513/600\n",
      "Epoch 514/600\n",
      "Epoch 515/600\n",
      "Epoch 516/600\n",
      "Epoch 517/600\n",
      "Epoch 518/600\n",
      "Epoch 519/600\n",
      "Epoch 520/600\n",
      "Epoch 521/600\n",
      "Epoch 522/600\n",
      "Epoch 523/600\n",
      "Epoch 524/600\n",
      "Epoch 525/600\n",
      "Epoch 526/600\n",
      "Epoch 527/600\n",
      "Epoch 528/600\n",
      "Epoch 529/600\n",
      "Epoch 530/600\n",
      "Epoch 531/600\n",
      "Epoch 532/600\n",
      "Epoch 533/600\n",
      "Epoch 534/600\n",
      "Epoch 535/600\n",
      "Epoch 536/600\n",
      "Epoch 537/600\n",
      "Epoch 538/600\n",
      "Epoch 539/600\n",
      "Epoch 540/600\n",
      "Epoch 541/600\n",
      "Epoch 542/600\n",
      "Epoch 543/600\n",
      "Epoch 544/600\n",
      "Epoch 545/600\n",
      "Epoch 546/600\n",
      "Epoch 547/600\n",
      "Epoch 548/600\n",
      "Epoch 549/600\n",
      "Epoch 550/600\n",
      "Epoch 551/600\n",
      "Epoch 552/600\n",
      "Epoch 553/600\n",
      "Epoch 554/600\n",
      "Epoch 555/600\n",
      "Epoch 556/600\n",
      "Epoch 557/600\n",
      "Epoch 558/600\n",
      "Epoch 559/600\n",
      "Epoch 560/600\n",
      "Epoch 561/600\n",
      "Epoch 562/600\n",
      "Epoch 563/600\n",
      "Epoch 564/600\n",
      "Epoch 565/600\n",
      "Epoch 566/600\n",
      "Epoch 567/600\n",
      "Epoch 568/600\n",
      "Epoch 569/600\n",
      "Epoch 570/600\n",
      "Epoch 571/600\n",
      "Epoch 572/600\n",
      "Epoch 573/600\n",
      "Epoch 574/600\n",
      "Epoch 575/600\n",
      "Epoch 576/600\n",
      "Epoch 577/600\n",
      "Epoch 578/600\n",
      "Epoch 579/600\n",
      "Epoch 580/600\n",
      "Epoch 581/600\n",
      "Epoch 582/600\n",
      "Epoch 583/600\n",
      "Epoch 584/600\n",
      "Epoch 585/600\n",
      "Epoch 586/600\n",
      "Epoch 587/600\n",
      "Epoch 588/600\n",
      "Epoch 589/600\n",
      "Epoch 590/600\n",
      "Epoch 591/600\n",
      "Epoch 592/600\n",
      "Epoch 593/600\n",
      "Epoch 594/600\n",
      "Epoch 595/600\n",
      "Epoch 596/600\n",
      "Epoch 597/600\n",
      "Epoch 598/600\n",
      "Epoch 599/600\n",
      "Epoch 600/600\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "history = model.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                    epochs=600,\n",
    "                    batch_size=16, \n",
    "                    validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                    verbose = 3,\n",
    "                    callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "857cac65-6c24-499d-a5aa-07be1f7b235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 3, 'epochs': 600, 'steps': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6e0181d-e3f5-472d-9378-50cd8544e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e856f44-e5c6-49b0-94f8-4d774c9818b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 250, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 124, 32)      128         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " maxpooling_1 (MaxPooling1D)    (None, 62, 32)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_1 (BatchNormalizatio  (None, 62, 32)      128         ['maxpooling_1[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 30, 32)       3104        ['batchnorm_1[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_2 (MaxPooling1D)    (None, 15, 32)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_2 (BatchNormalizatio  (None, 15, 32)      128         ['maxpooling_2[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 7, 32)        3104        ['batchnorm_2[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_3 (MaxPooling1D)    (None, 3, 32)        0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_3 (BatchNormalizatio  (None, 3, 32)       128         ['maxpooling_3[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['batchnorm_3[0][0]']            \n",
      "                                                                                                  \n",
      " node1_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node2_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node3_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node4_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node1_2 (Dense)                (None, 32)           544         ['node1_1[0][0]']                \n",
      "                                                                                                  \n",
      " node2_2 (Dense)                (None, 32)           544         ['node2_1[0][0]']                \n",
      "                                                                                                  \n",
      " node3_2 (Dense)                (None, 32)           544         ['node3_1[0][0]']                \n",
      "                                                                                                  \n",
      " node4_2 (Dense)                (None, 32)           544         ['node4_1[0][0]']                \n",
      "                                                                                                  \n",
      " node1_3 (Dense)                (None, 16)           528         ['node1_2[0][0]']                \n",
      "                                                                                                  \n",
      " node2_3 (Dense)                (None, 16)           528         ['node2_2[0][0]']                \n",
      "                                                                                                  \n",
      " node3_3 (Dense)                (None, 16)           528         ['node3_2[0][0]']                \n",
      "                                                                                                  \n",
      " node4_3 (Dense)                (None, 16)           528         ['node4_2[0][0]']                \n",
      "                                                                                                  \n",
      " dH (Dense)                     (None, 1)            17          ['node1_3[0][0]']                \n",
      "                                                                                                  \n",
      " dS (Dense)                     (None, 1)            17          ['node2_3[0][0]']                \n",
      "                                                                                                  \n",
      " dG (Dense)                     (None, 1)            17          ['node3_3[0][0]']                \n",
      "                                                                                                  \n",
      " Tm (Dense)                     (None, 1)            17          ['node4_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,284\n",
      "Trainable params: 17,092\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7764739-2299-4955-9427-bdd839457def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.predict of <keras.engine.functional.Functional object at 0x14ab000d8cd0>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb4aea0b-cd43-4acc-a6ae-063b9c321115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8ffcd18-b48b-4781-b64d-6b8d404028c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_hp(y_test_pred, Y_test, prop):\n",
    "    y_test_np = Y_test[f'{prop}'].to_numpy()\n",
    "    y_pred_test_np = y_test_pred\n",
    "\n",
    "    r2_test = r2_score(y_test_np, y_pred_test_np)\n",
    "    rmsd_test = (mean_squared_error(y_test_np, y_pred_test_np))**0.5\n",
    "    bias_test = np.mean(y_pred_test_np - y_test_np)\n",
    "    sdep_test = (np.mean((y_pred_test_np - y_test_np - bias_test)**2))**0.5\n",
    "    r2 = '{:.3f}'.format(r2_test)\n",
    "    rmsd = '{:.3f}'.format(rmsd_test)\n",
    "    bias = '{:.3f}'.format(bias_test)\n",
    "    sdep = '{:.3f}'.format(sdep_test)\n",
    "    \n",
    "    \n",
    "    mse=mean_squared_error(y_test_np, y_pred_test_np)\n",
    "    mse='{:.3f}'.format(mse)\n",
    "    mae=mean_absolute_error(y_test_np, y_pred_test_np)\n",
    "    mae='{:.3f}'.format(mae)\n",
    "    try:\n",
    "        a, b = np.polyfit(y_test_np, y_pred_test_np, 1)\n",
    "        plot_a = '{:.3f}'.format(a[0])\n",
    "        plot_b = '{:.3f}'.format(b[0])\n",
    "    except np.linalg.LinAlgError:\n",
    "        pass\n",
    "    \n",
    "    return r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "537a88ab-4a0c-45bd-9d84-546d80a36304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.889'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[3], Y_test, 'Tm')\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "276debb6-4f86-434a-9fd5-f6eb70c1065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92d081ec-9edd-4f5a-8cde-7710aa94f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1_1 <keras.layers.core.dense.Dense object at 0x14a8231a4340>\n",
      "node1_2 <keras.layers.core.dense.Dense object at 0x14a8231ab790>\n",
      "node1_3 <keras.layers.core.dense.Dense object at 0x14a8231abbb0>\n"
     ]
    }
   ],
   "source": [
    "for layer in model2.layers:\n",
    "    layer.trainable=True\n",
    "    if layer.name in ['node1_1','node1_2','node1_3']:\n",
    "        layer.trainable=False\n",
    "        print(layer.name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6296dae3-8c1f-4aa1-91ef-e4cecd5727d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_callback_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c53c383-ae97-40c9-887e-4d5fe84ddb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5dfd714-1f8f-410d-805a-31aff16623ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 9049.55664, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 9049.55664\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 9049.55664\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 9049.55664\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 9049.55664\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 9049.55664\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 9049.55664\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 9049.55664\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 9049.55664\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 9049.55664\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 9049.55664\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 9049.55664\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 9049.55664\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 9049.55664\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 9049.55664\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss improved from 9049.55664 to 7260.85352, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 7260.85352\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss improved from 7260.85352 to 6189.45898, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss improved from 6189.45898 to 3510.87817, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss improved from 3510.87817 to 2333.31396, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss improved from 2333.31396 to 2156.02441, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 2156.02441\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss improved from 2156.02441 to 1136.21143, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss improved from 1136.21143 to 499.68954, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 499.68954\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 499.68954\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 499.68954\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 499.68954\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss improved from 499.68954 to 295.72119, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss improved from 295.72119 to 288.60550, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 288.60550\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 288.60550\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 288.60550\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 288.60550\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 288.60550\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 288.60550\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 288.60550\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 288.60550\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 288.60550\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 288.60550\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 288.60550\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 288.60550\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 288.60550\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 288.60550\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 288.60550\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 288.60550\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 288.60550\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 288.60550\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 288.60550\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 288.60550\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 288.60550\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 288.60550\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 288.60550\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 288.60550\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 288.60550\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 288.60550\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 288.60550\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 288.60550\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 288.60550\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 288.60550\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 288.60550\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 288.60550\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 288.60550\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 288.60550\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 288.60550\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 288.60550\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 288.60550\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 288.60550\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 288.60550\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 288.60550\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 288.60550\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 288.60550\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 288.60550\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 288.60550\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 288.60550\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 288.60550\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 288.60550\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 288.60550\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 288.60550\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 288.60550\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 288.60550\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 288.60550\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 288.60550\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 288.60550\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 288.60550\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 288.60550\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 288.60550\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 288.60550\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 288.60550\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 288.60550\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 288.60550\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 288.60550\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 288.60550\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 288.60550\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 288.60550\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 288.60550\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 288.60550\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 288.60550\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 288.60550\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 288.60550\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 288.60550\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 288.60550\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 288.60550\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 288.60550\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 288.60550\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 288.60550\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 288.60550\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 288.60550\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 288.60550\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 288.60550\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 288.60550\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 288.60550\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 288.60550\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 288.60550\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 288.60550\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 288.60550\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 288.60550\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 288.60550\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 288.60550\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 288.60550\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 288.60550\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 288.60550\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 288.60550\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 288.60550\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 288.60550\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 288.60550\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 288.60550\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 288.60550\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 288.60550\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 288.60550\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 288.60550\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 288.60550\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 288.60550\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 288.60550\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 288.60550\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 288.60550\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 288.60550\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 288.60550\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 288.60550\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 288.60550\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 288.60550\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 288.60550\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 288.60550\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 288.60550\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 288.60550\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 288.60550\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 288.60550\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 288.60550\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 288.60550\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 288.60550\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 288.60550\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 288.60550\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 288.60550\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 288.60550\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 288.60550\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 288.60550\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 288.60550\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 288.60550\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 288.60550\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 288.60550\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 288.60550\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 288.60550\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 288.60550\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 288.60550\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 288.60550\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 288.60550\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 288.60550\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 288.60550\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 288.60550\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 288.60550\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 288.60550\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 288.60550\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 288.60550\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 288.60550\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 288.60550\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 288.60550\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 288.60550\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 288.60550\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 288.60550\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 288.60550\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 288.60550\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 288.60550\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 288.60550\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 288.60550\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 288.60550\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 288.60550\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 288.60550\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 288.60550\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 288.60550\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 288.60550\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 288.60550\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 288.60550\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 288.60550\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 288.60550\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 288.60550\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 288.60550\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 288.60550\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 288.60550\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 288.60550\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 288.60550\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 288.60550\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 288.60550\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 288.60550\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 288.60550\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 288.60550\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 288.60550\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 288.60550\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 288.60550\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 288.60550\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 288.60550\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 288.60550\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 288.60550\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 288.60550\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 288.60550\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 288.60550\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 288.60550\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 288.60550\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 288.60550\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 288.60550\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 288.60550\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 288.60550\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 288.60550\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 288.60550\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 288.60550\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 288.60550\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 288.60550\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 288.60550\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 288.60550\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 288.60550\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 288.60550\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 288.60550\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 288.60550\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 288.60550\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 288.60550\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 288.60550\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 288.60550\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 288.60550\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 288.60550\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 288.60550\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 288.60550\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 288.60550\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 288.60550\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 288.60550\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 288.60550\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 288.60550\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 288.60550\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 288.60550\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 288.60550\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 288.60550\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 288.60550\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 288.60550\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 288.60550\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 288.60550\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 288.60550\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 288.60550\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 288.60550\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 288.60550\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 288.60550\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 288.60550\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 288.60550\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 288.60550\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 288.60550\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 288.60550\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 288.60550\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 288.60550\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 288.60550\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 288.60550\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 288.60550\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 288.60550\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 288.60550\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 288.60550\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 288.60550\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 288.60550\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 288.60550\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 288.60550\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 288.60550\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 288.60550\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 288.60550\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 288.60550\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 288.60550\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 288.60550\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 288.60550\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 288.60550\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 288.60550\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 288.60550\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 288.60550\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 288.60550\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 288.60550\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 288.60550\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 288.60550\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 288.60550\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 288.60550\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 288.60550\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 288.60550\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 288.60550\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 288.60550\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 288.60550\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 288.60550\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 288.60550\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 288.60550\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 288.60550\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 288.60550\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 288.60550\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 288.60550\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 288.60550\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 288.60550\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 288.60550\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 288.60550\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 288.60550\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 288.60550\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 288.60550\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 288.60550\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 288.60550\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 288.60550\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 288.60550\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 288.60550\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 288.60550\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 288.60550\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 288.60550\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 288.60550\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 288.60550\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 288.60550\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 288.60550\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 288.60550\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 288.60550\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 288.60550\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 288.60550\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 288.60550\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 288.60550\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 288.60550\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 288.60550\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 288.60550\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 288.60550\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 288.60550\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 288.60550\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 288.60550\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 288.60550\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 288.60550\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 288.60550\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 288.60550\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 288.60550\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 288.60550\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 288.60550\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 288.60550\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 288.60550\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 288.60550\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 288.60550\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 288.60550\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 288.60550\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 288.60550\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 288.60550\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 288.60550\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 288.60550\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 288.60550\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 288.60550\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 288.60550\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 288.60550\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 288.60550\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 288.60550\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 288.60550\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 288.60550\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 288.60550\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 288.60550\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 288.60550\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 288.60550\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 288.60550\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 288.60550\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 288.60550\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 288.60550\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 288.60550\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 288.60550\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 288.60550\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 288.60550\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 288.60550\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 288.60550\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 288.60550\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 288.60550\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 288.60550\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 288.60550\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 288.60550\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 288.60550\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 288.60550\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 288.60550\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 288.60550\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 288.60550\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 288.60550\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 288.60550\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 288.60550\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 288.60550\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 288.60550\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 288.60550\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 288.60550\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 288.60550\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 288.60550\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 288.60550\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 288.60550\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 288.60550\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 288.60550\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 288.60550\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 288.60550\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 288.60550\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 288.60550\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 288.60550\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 288.60550\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 288.60550\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 288.60550\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 288.60550\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 288.60550\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 288.60550\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 288.60550\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 288.60550\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 288.60550\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 288.60550\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 288.60550\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 288.60550\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 288.60550\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 288.60550\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 288.60550\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 288.60550\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 288.60550\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 288.60550\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 288.60550\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 288.60550\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 288.60550\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 288.60550\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 288.60550\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 288.60550\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 288.60550\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 288.60550\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 288.60550\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 288.60550\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 288.60550\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 288.60550\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 288.60550\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 288.60550\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 288.60550\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 288.60550\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 288.60550\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 288.60550\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 288.60550\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 288.60550\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 288.60550\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 288.60550\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 288.60550\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 288.60550\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 288.60550\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 288.60550\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 288.60550\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 288.60550\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 288.60550\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 288.60550\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 288.60550\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 288.60550\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 288.60550\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 288.60550\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 288.60550\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 288.60550\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 288.60550\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 288.60550\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 288.60550\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 288.60550\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 288.60550\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 288.60550\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 288.60550\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 288.60550\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 288.60550\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 288.60550\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 288.60550\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 288.60550\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 288.60550\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 288.60550\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 288.60550\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 288.60550\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 288.60550\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 288.60550\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 288.60550\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 288.60550\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 288.60550\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 288.60550\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 288.60550\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 288.60550\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 288.60550\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 288.60550\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 288.60550\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 288.60550\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 288.60550\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 288.60550\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 288.60550\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 288.60550\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 288.60550\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 288.60550\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 288.60550\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 288.60550\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 288.60550\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 288.60550\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 288.60550\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 288.60550\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 288.60550\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 288.60550\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 288.60550\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 288.60550\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 288.60550\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 288.60550\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 288.60550\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 288.60550\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 288.60550\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 288.60550\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 288.60550\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 288.60550\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 288.60550\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 288.60550\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 288.60550\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 288.60550\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 288.60550\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 288.60550\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 288.60550\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 288.60550\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 288.60550\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 288.60550\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 288.60550\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 288.60550\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 288.60550\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 288.60550\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 288.60550\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 288.60550\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 288.60550\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 288.60550\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 288.60550\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 288.60550\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 288.60550\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 288.60550\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 288.60550\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 288.60550\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 288.60550\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 288.60550\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 288.60550\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 288.60550\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 288.60550\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 288.60550\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 288.60550\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 288.60550\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 288.60550\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 288.60550\n",
      "Epoch 543: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### CALL BACKS!\n",
    "es = EarlyStopping(monitor      = 'loss', \n",
    "                        mode     = 'min', \n",
    "                        verbose  = 1, \n",
    "                        patience = 100,)\n",
    "                    # restore_best_weights = True)\n",
    "# CSV Logger\n",
    "csv_logger = CSVLogger(f'{csv_logger_path}/model_history.csv' , append=True)\n",
    "\n",
    "# CP_callbacks      not required when using a tunner       \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{cp_callback_path}/cp.h5',#ckpt',\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                save_best_only = True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_path, \n",
    "                                                       #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                      update_freq = 1,\n",
    "                                                      # histogram_freq=1, \n",
    "                                                      write_graph=False, \n",
    "                                                      write_images=False)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "\n",
    "# Covert to list and provide to Keras Regressor\n",
    "keras_callbacks = [es, csv_logger, cp_callback, tensorboard_callback]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialise model for one single resample/hyper param tuning (train cycle)\n",
    "model2 = build_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit convolutional layers on all properties and optimisied initial fingures.\n",
    "history = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                        epochs=600,\n",
    "                        batch_size=16, \n",
    "                        validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                        verbose = 3,\n",
    "                        callbacks=keras_callbacks)\n",
    "# fine tune each finger, keep convolutional layers frozen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab93396e-fa84-49a4-8799-eb01bb5e6b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1_1 <keras.layers.core.dense.Dense object at 0x14a822303c70>\n",
      "node1_2 <keras.layers.core.dense.Dense object at 0x14a822303490>\n",
      "node1_3 <keras.layers.core.dense.Dense object at 0x14a822303700>\n",
      "dH <keras.layers.core.dense.Dense object at 0x14a822303940>\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 288.60550\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 288.60550\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 288.60550\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 288.60550\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 288.60550\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 288.60550\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 288.60550\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 288.60550\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 288.60550\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 288.60550\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 288.60550\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 288.60550\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 288.60550\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 288.60550\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 288.60550\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 288.60550\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 288.60550\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 288.60550\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 288.60550\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 288.60550\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 288.60550\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 288.60550\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 288.60550\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 288.60550\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 288.60550\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 288.60550\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 288.60550\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 288.60550\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss improved from 288.60550 to 285.21716, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 285.21716\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss improved from 285.21716 to 278.32291, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 278.32291\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 278.32291\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 278.32291\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 278.32291\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 278.32291\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 278.32291\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 278.32291\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 278.32291\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 278.32291\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 278.32291\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 278.32291\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 278.32291\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 278.32291\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 278.32291\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 278.32291\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 278.32291\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 278.32291\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 278.32291\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 278.32291\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 278.32291\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 278.32291\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 278.32291\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 278.32291\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 278.32291\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 278.32291\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 278.32291\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 278.32291\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 278.32291\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 278.32291\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 278.32291\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 278.32291\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 278.32291\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 278.32291\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 278.32291\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 278.32291\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 278.32291\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 278.32291\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 278.32291\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 278.32291\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 278.32291\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 278.32291\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 278.32291\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 278.32291\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 278.32291\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 278.32291\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 278.32291\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 278.32291\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 278.32291\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 278.32291\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 278.32291\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 278.32291\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 278.32291\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 278.32291\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 278.32291\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 278.32291\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 278.32291\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 278.32291\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 278.32291\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 278.32291\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 278.32291\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 278.32291\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 278.32291\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 278.32291\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 278.32291\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 278.32291\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 278.32291\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 278.32291\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 278.32291\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 278.32291\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 278.32291\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 278.32291\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 278.32291\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 278.32291\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 278.32291\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 278.32291\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 278.32291\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 278.32291\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 278.32291\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 278.32291\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 278.32291\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 278.32291\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 278.32291\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 278.32291\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 278.32291\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 278.32291\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 278.32291\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 278.32291\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 278.32291\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 278.32291\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 278.32291\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 278.32291\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 278.32291\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 278.32291\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 278.32291\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 278.32291\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 278.32291\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 278.32291\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 278.32291\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 278.32291\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 278.32291\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 278.32291\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 278.32291\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 278.32291\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 278.32291\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 278.32291\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 278.32291\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 278.32291\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 278.32291\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 278.32291\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 278.32291\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 278.32291\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 278.32291\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 278.32291\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 278.32291\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 278.32291\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 278.32291\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 278.32291\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 278.32291\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 278.32291\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 278.32291\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 278.32291\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 278.32291\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 278.32291\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 278.32291\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 278.32291\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 278.32291\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 278.32291\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 278.32291\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 278.32291\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 278.32291\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 278.32291\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 278.32291\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 278.32291\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 278.32291\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 278.32291\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 278.32291\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 278.32291\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 278.32291\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 278.32291\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 278.32291\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 278.32291\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 278.32291\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 278.32291\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 278.32291\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 278.32291\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 278.32291\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 278.32291\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 278.32291\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 278.32291\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 278.32291\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 278.32291\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 278.32291\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 278.32291\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 278.32291\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 278.32291\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 278.32291\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 278.32291\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 278.32291\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 278.32291\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 278.32291\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 278.32291\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 278.32291\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 278.32291\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 278.32291\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 278.32291\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 278.32291\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 278.32291\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 278.32291\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 278.32291\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 278.32291\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 278.32291\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 278.32291\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 278.32291\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 278.32291\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 278.32291\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 278.32291\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 278.32291\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 278.32291\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 278.32291\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 278.32291\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 278.32291\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 278.32291\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 278.32291\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 278.32291\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 278.32291\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 278.32291\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 278.32291\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 278.32291\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 278.32291\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 278.32291\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 278.32291\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 278.32291\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 278.32291\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 278.32291\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 278.32291\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 278.32291\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 278.32291\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 278.32291\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 278.32291\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 278.32291\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 278.32291\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 278.32291\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 278.32291\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 278.32291\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 278.32291\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 278.32291\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 278.32291\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 278.32291\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 278.32291\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 278.32291\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 278.32291\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 278.32291\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 278.32291\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 278.32291\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 278.32291\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 278.32291\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 278.32291\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 278.32291\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 278.32291\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 278.32291\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 278.32291\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 278.32291\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 278.32291\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 278.32291\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 278.32291\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 278.32291\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 278.32291\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 278.32291\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 278.32291\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 278.32291\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 278.32291\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 278.32291\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 278.32291\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 278.32291\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 278.32291\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 278.32291\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 278.32291\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 278.32291\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 278.32291\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 278.32291\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 278.32291\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 278.32291\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 278.32291\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 278.32291\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 278.32291\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 278.32291\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 278.32291\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 278.32291\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 278.32291\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 278.32291\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 278.32291\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 278.32291\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 278.32291\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 278.32291\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 278.32291\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 278.32291\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 278.32291\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 278.32291\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 278.32291\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 278.32291\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 278.32291\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 278.32291\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 278.32291\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 278.32291\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 278.32291\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 278.32291\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 278.32291\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 278.32291\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 278.32291\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 278.32291\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 278.32291\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 278.32291\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 278.32291\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 278.32291\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 278.32291\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 278.32291\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 278.32291\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 278.32291\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 278.32291\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 278.32291\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 278.32291\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 278.32291\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 278.32291\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 278.32291\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 278.32291\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 278.32291\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 278.32291\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 278.32291\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 278.32291\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 278.32291\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 278.32291\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 278.32291\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 278.32291\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 278.32291\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 278.32291\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 278.32291\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 278.32291\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 278.32291\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 278.32291\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 278.32291\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 278.32291\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 278.32291\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 278.32291\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 278.32291\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 278.32291\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 278.32291\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 278.32291\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 278.32291\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 278.32291\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 278.32291\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 278.32291\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 278.32291\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 278.32291\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 278.32291\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 278.32291\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 278.32291\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 278.32291\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 278.32291\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 278.32291\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 278.32291\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 278.32291\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 278.32291\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 278.32291\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 278.32291\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 278.32291\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 278.32291\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 278.32291\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 278.32291\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 278.32291\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 278.32291\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 278.32291\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 278.32291\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 278.32291\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 278.32291\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 278.32291\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 278.32291\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 278.32291\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 278.32291\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 278.32291\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 278.32291\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 278.32291\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 278.32291\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 278.32291\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 278.32291\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 278.32291\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 278.32291\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 278.32291\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 278.32291\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 278.32291\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 278.32291\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 278.32291\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 278.32291\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 278.32291\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 278.32291\n",
      "Epoch 385: early stopping\n",
      "node2_1 <keras.layers.core.dense.Dense object at 0x14abb8fc85b0>\n",
      "node2_2 <keras.layers.core.dense.Dense object at 0x14abb8fa75e0>\n",
      "node2_3 <keras.layers.core.dense.Dense object at 0x14abb8fc8520>\n",
      "dS <keras.layers.core.dense.Dense object at 0x14ac579db760>\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 278.32291\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 278.32291\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 278.32291\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 278.32291\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 278.32291\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 278.32291\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 278.32291\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 278.32291\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 278.32291\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 278.32291\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 278.32291\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 278.32291\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 278.32291\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 278.32291\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 278.32291\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 278.32291\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 278.32291\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 278.32291\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 278.32291\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 278.32291\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 278.32291\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 278.32291\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 278.32291\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 278.32291\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 278.32291\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 278.32291\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 278.32291\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 278.32291\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 278.32291\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 278.32291\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 278.32291\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 278.32291\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 278.32291\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 278.32291\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 278.32291\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 278.32291\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 278.32291\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 278.32291\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 278.32291\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 278.32291\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 278.32291\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 278.32291\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 278.32291\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 278.32291\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 278.32291\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 278.32291\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 278.32291\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 278.32291\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 278.32291\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 278.32291\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 278.32291\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 278.32291\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 278.32291\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 278.32291\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 278.32291\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 278.32291\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 278.32291\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 278.32291\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 278.32291\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 278.32291\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 278.32291\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 278.32291\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss improved from 278.32291 to 275.14700, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 275.14700\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 275.14700\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 275.14700\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 275.14700\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 275.14700\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 275.14700\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 275.14700\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 275.14700\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 275.14700\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 275.14700\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 275.14700\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 275.14700\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 275.14700\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 275.14700\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 275.14700\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 275.14700\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 275.14700\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 275.14700\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 275.14700\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 275.14700\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 275.14700\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 275.14700\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 275.14700\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 275.14700\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 275.14700\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 275.14700\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 275.14700\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 275.14700\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 275.14700\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 275.14700\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 275.14700\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 275.14700\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 275.14700\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 275.14700\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 275.14700\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 275.14700\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 275.14700\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 275.14700\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 275.14700\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 275.14700\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 275.14700\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 275.14700\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 275.14700\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 275.14700\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 275.14700\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 275.14700\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 275.14700\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 275.14700\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 275.14700\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 275.14700\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 275.14700\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 275.14700\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 275.14700\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 275.14700\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 275.14700\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 275.14700\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 275.14700\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 275.14700\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 275.14700\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 275.14700\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 275.14700\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 275.14700\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 275.14700\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 275.14700\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 275.14700\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 275.14700\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 275.14700\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 275.14700\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 275.14700\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 275.14700\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 275.14700\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 275.14700\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 275.14700\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 275.14700\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 275.14700\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 275.14700\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 275.14700\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 275.14700\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 275.14700\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 275.14700\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 275.14700\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 275.14700\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 275.14700\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 275.14700\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 275.14700\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 275.14700\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 275.14700\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 275.14700\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 275.14700\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 275.14700\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 275.14700\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 275.14700\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 275.14700\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 275.14700\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 275.14700\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 275.14700\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 275.14700\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 275.14700\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 275.14700\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 275.14700\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 275.14700\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 275.14700\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 275.14700\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 275.14700\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 275.14700\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 275.14700\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 275.14700\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 275.14700\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 275.14700\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 275.14700\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 275.14700\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 275.14700\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 275.14700\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 275.14700\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 275.14700\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 275.14700\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 275.14700\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 275.14700\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 275.14700\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 275.14700\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 275.14700\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 275.14700\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 275.14700\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 275.14700\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 275.14700\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 275.14700\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 275.14700\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 275.14700\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 275.14700\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 275.14700\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 275.14700\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 275.14700\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 275.14700\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 275.14700\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 275.14700\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 275.14700\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 275.14700\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 275.14700\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 275.14700\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 275.14700\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 275.14700\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 275.14700\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 275.14700\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 275.14700\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 275.14700\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 275.14700\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 275.14700\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 275.14700\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 275.14700\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 275.14700\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 275.14700\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 275.14700\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 275.14700\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 275.14700\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 275.14700\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 275.14700\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 275.14700\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 275.14700\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 275.14700\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 275.14700\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 275.14700\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 275.14700\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 275.14700\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 275.14700\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 275.14700\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 275.14700\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 275.14700\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 275.14700\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 275.14700\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 275.14700\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 275.14700\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 275.14700\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 275.14700\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 275.14700\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 275.14700\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 275.14700\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 275.14700\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 275.14700\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 275.14700\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 275.14700\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 275.14700\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 275.14700\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 275.14700\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 275.14700\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 275.14700\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 275.14700\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 275.14700\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 275.14700\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 275.14700\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 275.14700\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 275.14700\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 275.14700\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 275.14700\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 275.14700\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 275.14700\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 275.14700\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 275.14700\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 275.14700\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 275.14700\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 275.14700\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 275.14700\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 275.14700\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 275.14700\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 275.14700\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 275.14700\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 275.14700\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 275.14700\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 275.14700\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 275.14700\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 275.14700\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 275.14700\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 275.14700\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 275.14700\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 275.14700\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 275.14700\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 275.14700\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 275.14700\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 275.14700\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 275.14700\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 275.14700\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 275.14700\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 275.14700\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 275.14700\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 275.14700\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 275.14700\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 275.14700\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 275.14700\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 275.14700\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 275.14700\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 275.14700\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 275.14700\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 275.14700\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 275.14700\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 275.14700\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 275.14700\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 275.14700\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 275.14700\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 275.14700\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 275.14700\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 275.14700\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 275.14700\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 275.14700\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 275.14700\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 275.14700\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 275.14700\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 275.14700\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 275.14700\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 275.14700\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 275.14700\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 275.14700\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 275.14700\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 275.14700\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 275.14700\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 275.14700\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 275.14700\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 275.14700\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 275.14700\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 275.14700\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 275.14700\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 275.14700\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 275.14700\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 275.14700\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 275.14700\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 275.14700\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 275.14700\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 275.14700\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 275.14700\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 275.14700\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 275.14700\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 275.14700\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 275.14700\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 275.14700\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 275.14700\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 275.14700\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 275.14700\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 275.14700\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 275.14700\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 275.14700\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 275.14700\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 275.14700\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 275.14700\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 275.14700\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 275.14700\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 275.14700\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 275.14700\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 275.14700\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 275.14700\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 275.14700\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 275.14700\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 275.14700\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 275.14700\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 275.14700\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 275.14700\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 275.14700\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 275.14700\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 275.14700\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 275.14700\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 275.14700\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 275.14700\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 275.14700\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 275.14700\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 275.14700\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 275.14700\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 275.14700\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 275.14700\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 275.14700\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 275.14700\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 275.14700\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 275.14700\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 275.14700\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 275.14700\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 275.14700\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 275.14700\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 275.14700\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 275.14700\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 275.14700\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 275.14700\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 275.14700\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 275.14700\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 275.14700\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 275.14700\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 275.14700\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 275.14700\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 275.14700\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 275.14700\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 275.14700\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 275.14700\n",
      "Epoch 390: early stopping\n",
      "node3_1 <keras.layers.core.dense.Dense object at 0x14ab683ded60>\n",
      "node3_2 <keras.layers.core.dense.Dense object at 0x14ab683de7c0>\n",
      "node3_3 <keras.layers.core.dense.Dense object at 0x14ab683afa90>\n",
      "dG <keras.layers.core.dense.Dense object at 0x14ac557bfb80>\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 275.14700\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 275.14700\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 275.14700\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 275.14700\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 275.14700\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 275.14700\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 275.14700\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 275.14700\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 275.14700\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 275.14700\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 275.14700\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 275.14700\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 275.14700\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 275.14700\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 275.14700\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 275.14700\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss improved from 275.14700 to 259.14981, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 259.14981\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 259.14981\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 259.14981\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss improved from 259.14981 to 251.76727, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 251.76727\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 251.76727\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss improved from 251.76727 to 249.52132, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp.h5\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 249.52132\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 249.52132\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 249.52132\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 249.52132\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 249.52132\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 249.52132\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 249.52132\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 249.52132\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 249.52132\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 249.52132\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 249.52132\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 249.52132\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 249.52132\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 249.52132\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 249.52132\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 249.52132\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 249.52132\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 249.52132\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 249.52132\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 249.52132\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 249.52132\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 249.52132\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 249.52132\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 249.52132\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 249.52132\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 249.52132\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 249.52132\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 249.52132\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 249.52132\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 249.52132\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 249.52132\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 249.52132\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 249.52132\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 249.52132\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 249.52132\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 249.52132\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 249.52132\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 249.52132\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 249.52132\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 249.52132\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 249.52132\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 249.52132\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 249.52132\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 249.52132\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 249.52132\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 249.52132\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 249.52132\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 249.52132\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 249.52132\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 249.52132\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 249.52132\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 249.52132\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 249.52132\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 249.52132\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 249.52132\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 249.52132\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 249.52132\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 249.52132\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 249.52132\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 249.52132\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 249.52132\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 249.52132\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 249.52132\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 249.52132\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 249.52132\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 249.52132\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 249.52132\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 249.52132\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 249.52132\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 249.52132\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 249.52132\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 249.52132\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 249.52132\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 249.52132\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 249.52132\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 249.52132\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 249.52132\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 249.52132\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 249.52132\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 249.52132\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 249.52132\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 249.52132\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 249.52132\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 249.52132\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 249.52132\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 249.52132\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 249.52132\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 249.52132\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 249.52132\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 249.52132\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 249.52132\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 249.52132\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 249.52132\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 249.52132\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 249.52132\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 249.52132\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 249.52132\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 249.52132\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 249.52132\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 249.52132\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 249.52132\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 249.52132\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 249.52132\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 249.52132\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 249.52132\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 249.52132\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 249.52132\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 249.52132\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 249.52132\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 249.52132\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 249.52132\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 249.52132\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 249.52132\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 249.52132\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 249.52132\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 249.52132\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 249.52132\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 249.52132\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 249.52132\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 249.52132\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 249.52132\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 249.52132\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 249.52132\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 249.52132\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 249.52132\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 249.52132\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 249.52132\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 249.52132\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 249.52132\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 249.52132\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 249.52132\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 249.52132\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 249.52132\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 249.52132\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 249.52132\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 249.52132\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 249.52132\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 249.52132\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 249.52132\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 249.52132\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 249.52132\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 249.52132\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 249.52132\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 249.52132\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 249.52132\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 249.52132\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 249.52132\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 249.52132\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 249.52132\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 249.52132\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 249.52132\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 249.52132\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 249.52132\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 249.52132\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 249.52132\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 249.52132\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 249.52132\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 249.52132\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 249.52132\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 249.52132\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 249.52132\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 249.52132\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 249.52132\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 249.52132\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.52132\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 249.52132\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.52132\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.52132\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.52132\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.52132\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.52132\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.52132\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.52132\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.52132\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.52132\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.52132\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.52132\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.52132\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.52132\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.52132\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.52132\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.52132\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.52132\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.52132\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.52132\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.52132\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.52132\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.52132\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.52132\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.52132\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.52132\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.52132\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.52132\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.52132\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.52132\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.52132\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.52132\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.52132\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.52132\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.52132\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.52132\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.52132\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.52132\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.52132\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.52132\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.52132\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.52132\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.52132\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.52132\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.52132\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.52132\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.52132\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.52132\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.52132\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.52132\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.52132\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.52132\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.52132\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.52132\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.52132\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.52132\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.52132\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.52132\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.52132\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.52132\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.52132\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.52132\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.52132\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.52132\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.52132\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.52132\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.52132\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.52132\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.52132\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.52132\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.52132\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.52132\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.52132\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.52132\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.52132\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.52132\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.52132\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.52132\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.52132\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.52132\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.52132\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.52132\n",
      "Epoch 271: early stopping\n",
      "node4_1 <keras.layers.core.dense.Dense object at 0x14ab9f6a6e20>\n",
      "node4_2 <keras.layers.core.dense.Dense object at 0x14ab9f6a6be0>\n",
      "node4_3 <keras.layers.core.dense.Dense object at 0x14ab94cf8d90>\n",
      "Tm <keras.layers.core.dense.Dense object at 0x14ab9f6eb1f0>\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 249.52132\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 249.52132\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 249.52132\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 249.52132\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 249.52132\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 249.52132\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 249.52132\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 249.52132\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 249.52132\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 249.52132\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 249.52132\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 249.52132\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 249.52132\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 249.52132\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 249.52132\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 249.52132\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 249.52132\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 249.52132\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 249.52132\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 249.52132\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 249.52132\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 249.52132\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 249.52132\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 249.52132\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 249.52132\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 249.52132\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 249.52132\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 249.52132\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 249.52132\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 249.52132\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 249.52132\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 249.52132\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 249.52132\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 249.52132\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 249.52132\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 249.52132\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 249.52132\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 249.52132\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 249.52132\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 249.52132\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 249.52132\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 249.52132\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 249.52132\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 249.52132\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 249.52132\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 249.52132\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 249.52132\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 249.52132\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 249.52132\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 249.52132\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 249.52132\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 249.52132\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 249.52132\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 249.52132\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 249.52132\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 249.52132\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 249.52132\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 249.52132\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 249.52132\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 249.52132\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 249.52132\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 249.52132\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 249.52132\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 249.52132\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 249.52132\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 249.52132\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 249.52132\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 249.52132\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 249.52132\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 249.52132\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 249.52132\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 249.52132\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 249.52132\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 249.52132\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 249.52132\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 249.52132\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 249.52132\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 249.52132\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 249.52132\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 249.52132\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 249.52132\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 249.52132\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 249.52132\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 249.52132\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 249.52132\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 249.52132\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 249.52132\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 249.52132\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 249.52132\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 249.52132\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 249.52132\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 249.52132\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 249.52132\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 249.52132\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 249.52132\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 249.52132\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 249.52132\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 249.52132\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 249.52132\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 249.52132\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 249.52132\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 249.52132\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 249.52132\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 249.52132\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 249.52132\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 249.52132\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 249.52132\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 249.52132\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 249.52132\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 249.52132\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 249.52132\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 249.52132\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 249.52132\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 249.52132\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 249.52132\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 249.52132\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 249.52132\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 249.52132\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 249.52132\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 249.52132\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 249.52132\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 249.52132\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 249.52132\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 249.52132\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 249.52132\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 249.52132\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 249.52132\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 249.52132\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 249.52132\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 249.52132\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 249.52132\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 249.52132\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 249.52132\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 249.52132\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 249.52132\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 249.52132\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 249.52132\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 249.52132\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 249.52132\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 249.52132\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 249.52132\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 249.52132\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 249.52132\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 249.52132\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 249.52132\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 249.52132\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 249.52132\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 249.52132\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 249.52132\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 249.52132\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 249.52132\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 249.52132\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 249.52132\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 249.52132\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 249.52132\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 249.52132\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 249.52132\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 249.52132\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 249.52132\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 249.52132\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 249.52132\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 249.52132\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 249.52132\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 249.52132\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 249.52132\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 249.52132\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 249.52132\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 249.52132\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 249.52132\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 249.52132\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 249.52132\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 249.52132\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 249.52132\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 249.52132\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 249.52132\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 249.52132\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 249.52132\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 249.52132\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 249.52132\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 249.52132\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 249.52132\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 249.52132\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 249.52132\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 249.52132\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 249.52132\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 249.52132\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 249.52132\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 249.52132\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.52132\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 249.52132\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.52132\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.52132\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.52132\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.52132\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.52132\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.52132\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.52132\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.52132\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.52132\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.52132\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.52132\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.52132\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.52132\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.52132\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.52132\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.52132\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.52132\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.52132\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.52132\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.52132\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.52132\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.52132\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.52132\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.52132\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.52132\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.52132\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.52132\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.52132\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.52132\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.52132\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.52132\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.52132\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.52132\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.52132\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.52132\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.52132\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.52132\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.52132\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.52132\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.52132\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.52132\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.52132\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.52132\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.52132\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.52132\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.52132\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.52132\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.52132\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.52132\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.52132\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.52132\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.52132\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.52132\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.52132\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.52132\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.52132\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.52132\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.52132\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.52132\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.52132\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.52132\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.52132\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.52132\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.52132\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.52132\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.52132\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.52132\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.52132\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.52132\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.52132\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.52132\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.52132\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.52132\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.52132\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.52132\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.52132\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.52132\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.52132\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.52132\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.52132\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.52132\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 249.52132\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 249.52132\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 249.52132\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 249.52132\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 249.52132\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 249.52132\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 249.52132\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 249.52132\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 249.52132\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 249.52132\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 249.52132\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 249.52132\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 249.52132\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 249.52132\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 249.52132\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 249.52132\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 249.52132\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 249.52132\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 249.52132\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 249.52132\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 249.52132\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 249.52132\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 249.52132\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 249.52132\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 249.52132\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 249.52132\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 249.52132\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 249.52132\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 249.52132\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 249.52132\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 249.52132\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 249.52132\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 249.52132\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 249.52132\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 249.52132\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 249.52132\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 249.52132\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 249.52132\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 249.52132\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 249.52132\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 249.52132\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 249.52132\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 249.52132\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 249.52132\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 249.52132\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 249.52132\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 249.52132\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 249.52132\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 249.52132\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 249.52132\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 249.52132\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 249.52132\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 249.52132\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 249.52132\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 249.52132\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 249.52132\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 249.52132\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 249.52132\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 249.52132\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 249.52132\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 249.52132\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 249.52132\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 249.52132\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 249.52132\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 249.52132\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 249.52132\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 249.52132\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 249.52132\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 249.52132\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 249.52132\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 249.52132\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 249.52132\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 249.52132\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 249.52132\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 249.52132\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 249.52132\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 249.52132\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 249.52132\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 249.52132\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 249.52132\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 249.52132\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 249.52132\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 249.52132\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 249.52132\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 249.52132\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 249.52132\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 249.52132\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 249.52132\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 249.52132\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 249.52132\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 249.52132\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 249.52132\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 249.52132\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 249.52132\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 249.52132\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 249.52132\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 249.52132\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 249.52132\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 249.52132\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 249.52132\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 249.52132\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 249.52132\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 249.52132\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 249.52132\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 249.52132\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 249.52132\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 249.52132\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 249.52132\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 249.52132\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 249.52132\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 249.52132\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 249.52132\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 249.52132\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 249.52132\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 249.52132\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 249.52132\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 249.52132\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 249.52132\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 249.52132\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 249.52132\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 249.52132\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 249.52132\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 249.52132\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 249.52132\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 249.52132\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 249.52132\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 249.52132\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 249.52132\n",
      "Epoch 399: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x14ab9f68ed30>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 20\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28mprint\u001b[39m(layer\u001b[38;5;241m.\u001b[39mname, layer)\n\u001b[1;32m     12\u001b[0m     history \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mfit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n\u001b[1;32m     13\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m     14\u001b[0m                         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[1;32m     15\u001b[0m                         validation_data\u001b[38;5;241m=\u001b[39m(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n\u001b[1;32m     16\u001b[0m                         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     17\u001b[0m                         callbacks\u001b[38;5;241m=\u001b[39mkeras_callbacks)\n\u001b[0;32m---> 20\u001b[0m model2\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcp_callback_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/cp.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/saving/legacy/hdf5_format.py:186\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    184\u001b[0m model_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model config found in the file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    190\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x14ab9f68ed30>."
     ]
    }
   ],
   "source": [
    "for finger in range(1,5):\n",
    "    model2 = build_model()\n",
    "    model2.load_weights(f'{cp_callback_path}/cp.h5')\n",
    "    # finger = 4\n",
    "    for layer in model2.layers:\n",
    "        layer.trainable=False\n",
    "        if layer.name in [f'node{finger}_1',f'node{finger}_2',f'node{finger}_3',prop[finger-1]]:\n",
    "            layer.trainable=True\n",
    "            print(layer.name, layer)\n",
    "    \n",
    "    \n",
    "    history = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                        epochs=600,\n",
    "                        batch_size=16, \n",
    "                        validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                        verbose = 3,\n",
    "                        callbacks=keras_callbacks)\n",
    "\n",
    "\n",
    "model2 = build_model()\n",
    "model2.load_weights(f'{cp_callback_path}/cp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d478ab7e-cae9-434d-99af-f3219dfc03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model()\n",
    "model2.load_weights(f'{cp_callback_path}/cp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32856381-e16e-4179-a619-1b4a8087c6ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 29197.44531, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 29197.44531\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 29197.44531\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 29197.44531\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss improved from 29197.44531 to 22776.90234, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 22776.90234\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 22776.90234\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 22776.90234\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 22776.90234\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 22776.90234\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 22776.90234\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 22776.90234\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 22776.90234\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 22776.90234\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss improved from 22776.90234 to 20344.41992, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss improved from 20344.41992 to 17264.28906, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss improved from 17264.28906 to 12847.39648, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss improved from 12847.39648 to 10206.52441, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss improved from 10206.52441 to 8494.08594, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss improved from 8494.08594 to 8024.31738, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss improved from 8024.31738 to 6444.26855, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss improved from 6444.26855 to 4715.79688, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss improved from 4715.79688 to 4128.26611, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss improved from 4128.26611 to 3478.29150, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss improved from 3478.29150 to 3087.04785, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss improved from 3087.04785 to 2314.24390, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss improved from 2314.24390 to 1837.09009, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss improved from 1837.09009 to 1461.82959, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss improved from 1461.82959 to 1010.99249, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss improved from 1010.99249 to 863.45642, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss improved from 863.45642 to 635.02295, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 635.02295\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss improved from 635.02295 to 569.32764, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss improved from 569.32764 to 434.11920, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss improved from 434.11920 to 415.32080, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss improved from 415.32080 to 354.40472, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 354.40472\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss improved from 354.40472 to 348.40698, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss improved from 348.40698 to 326.82019, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 326.82019\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 326.82019\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 326.82019\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss improved from 326.82019 to 317.34802, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 317.34802\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss improved from 317.34802 to 316.92322, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss improved from 316.92322 to 314.87161, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss improved from 314.87161 to 309.61041, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss improved from 309.61041 to 305.32489, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss improved from 305.32489 to 302.62268, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss improved from 302.62268 to 301.94925, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 301.94925\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 301.94925\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 301.94925\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 301.94925\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 301.94925\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 301.94925\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 301.94925\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 301.94925\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 301.94925\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 301.94925\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 301.94925\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 301.94925\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 301.94925\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 301.94925\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 301.94925\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 301.94925\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 301.94925\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 301.94925\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 301.94925\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 301.94925\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 301.94925\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 301.94925\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 301.94925\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 301.94925\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 301.94925\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 301.94925\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 301.94925\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 301.94925\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 301.94925\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 301.94925\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 301.94925\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 301.94925\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 301.94925\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 301.94925\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 301.94925\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 301.94925\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 301.94925\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 301.94925\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 301.94925\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 301.94925\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 301.94925\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 301.94925\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 301.94925\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 301.94925\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 301.94925\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 301.94925\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 301.94925\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 301.94925\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 301.94925\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 301.94925\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 301.94925\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 301.94925\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 301.94925\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 301.94925\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 301.94925\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 301.94925\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 301.94925\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 301.94925\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 301.94925\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 301.94925\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss improved from 301.94925 to 287.24777, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss improved from 287.24777 to 287.00989, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 287.00989\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 287.00989\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 287.00989\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 287.00989\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 287.00989\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 287.00989\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 287.00989\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 287.00989\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 287.00989\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 287.00989\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 287.00989\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 287.00989\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 287.00989\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 287.00989\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 287.00989\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 287.00989\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 287.00989\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 287.00989\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 287.00989\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 287.00989\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 287.00989\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 287.00989\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 287.00989\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 287.00989\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 287.00989\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 287.00989\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 287.00989\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 287.00989\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 287.00989\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 287.00989\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 287.00989\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 287.00989\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 287.00989\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 287.00989\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 287.00989\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 287.00989\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 287.00989\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 287.00989\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 287.00989\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 287.00989\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 287.00989\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 287.00989\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 287.00989\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 287.00989\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 287.00989\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 287.00989\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 287.00989\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 287.00989\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 287.00989\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 287.00989\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 287.00989\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 287.00989\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 287.00989\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 287.00989\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 287.00989\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 287.00989\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 287.00989\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 287.00989\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 287.00989\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 287.00989\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 287.00989\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 287.00989\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 287.00989\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 287.00989\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 287.00989\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 287.00989\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 287.00989\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 287.00989\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 287.00989\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 287.00989\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 287.00989\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 287.00989\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 287.00989\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 287.00989\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 287.00989\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 287.00989\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 287.00989\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 287.00989\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 287.00989\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 287.00989\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 287.00989\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 287.00989\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 287.00989\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 287.00989\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 287.00989\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 287.00989\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 287.00989\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 287.00989\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 287.00989\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 287.00989\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 287.00989\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 287.00989\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 287.00989\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 287.00989\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 287.00989\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 287.00989\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 287.00989\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 287.00989\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 287.00989\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 287.00989\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 287.00989\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 287.00989\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 287.00989\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 287.00989\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 287.00989\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 287.00989\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 287.00989\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 287.00989\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 287.00989\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 287.00989\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 287.00989\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 287.00989\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 287.00989\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 287.00989\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 287.00989\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 287.00989\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 287.00989\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 287.00989\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 287.00989\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 287.00989\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 287.00989\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 287.00989\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 287.00989\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 287.00989\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 287.00989\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 287.00989\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 287.00989\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 287.00989\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 287.00989\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 287.00989\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 287.00989\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 287.00989\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 287.00989\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 287.00989\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 287.00989\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 287.00989\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 287.00989\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 287.00989\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 287.00989\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 287.00989\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 287.00989\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 287.00989\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 287.00989\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 287.00989\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 287.00989\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 287.00989\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 287.00989\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 287.00989\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 287.00989\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 287.00989\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 287.00989\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 287.00989\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 287.00989\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 287.00989\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 287.00989\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 287.00989\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 287.00989\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 287.00989\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 287.00989\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 287.00989\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 287.00989\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 287.00989\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 287.00989\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 287.00989\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 287.00989\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 287.00989\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 287.00989\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 287.00989\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 287.00989\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 287.00989\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 287.00989\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 287.00989\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 287.00989\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 287.00989\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 287.00989\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 287.00989\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 287.00989\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 287.00989\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 287.00989\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 287.00989\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 287.00989\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 287.00989\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 287.00989\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 287.00989\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 287.00989\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 287.00989\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 287.00989\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 287.00989\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 287.00989\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 287.00989\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 287.00989\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 287.00989\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 287.00989\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 287.00989\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 287.00989\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 287.00989\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 287.00989\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 287.00989\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 287.00989\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 287.00989\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 287.00989\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 287.00989\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 287.00989\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 287.00989\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 287.00989\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 287.00989\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 287.00989\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 287.00989\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 287.00989\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 287.00989\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 287.00989\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 287.00989\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 287.00989\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 287.00989\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 287.00989\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 287.00989\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 287.00989\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 287.00989\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 287.00989\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 287.00989\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 287.00989\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 287.00989\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 287.00989\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 287.00989\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 287.00989\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 287.00989\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 287.00989\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 287.00989\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 287.00989\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 287.00989\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 287.00989\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 287.00989\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 287.00989\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 287.00989\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 287.00989\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 287.00989\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 287.00989\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 287.00989\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 287.00989\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 287.00989\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 287.00989\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 287.00989\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 287.00989\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 287.00989\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 287.00989\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 287.00989\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 287.00989\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 287.00989\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 287.00989\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 287.00989\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 287.00989\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 287.00989\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 287.00989\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 287.00989\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 287.00989\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 287.00989\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 287.00989\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 287.00989\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 287.00989\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 287.00989\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 287.00989\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 287.00989\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 287.00989\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 287.00989\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 287.00989\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 287.00989\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 287.00989\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 287.00989\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 287.00989\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 287.00989\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 287.00989\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 287.00989\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 287.00989\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 287.00989\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 287.00989\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 287.00989\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 287.00989\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 287.00989\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 287.00989\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 287.00989\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 287.00989\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 287.00989\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 287.00989\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 287.00989\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 287.00989\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 287.00989\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 287.00989\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 287.00989\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 287.00989\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 287.00989\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 287.00989\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 287.00989\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 287.00989\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 287.00989\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 287.00989\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 287.00989\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 287.00989\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 287.00989\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 287.00989\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 287.00989\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 287.00989\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 287.00989\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 287.00989\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 287.00989\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 287.00989\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 287.00989\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 287.00989\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 287.00989\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 287.00989\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 287.00989\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 287.00989\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 287.00989\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 287.00989\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 287.00989\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 287.00989\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 287.00989\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 287.00989\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 287.00989\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 287.00989\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 287.00989\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 287.00989\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 287.00989\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 287.00989\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 287.00989\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 287.00989\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 287.00989\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 287.00989\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 287.00989\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 287.00989\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 287.00989\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 287.00989\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 287.00989\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 287.00989\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 287.00989\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 287.00989\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 287.00989\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 287.00989\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 287.00989\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 287.00989\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 287.00989\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 287.00989\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 287.00989\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 287.00989\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 287.00989\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 287.00989\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 287.00989\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 287.00989\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 287.00989\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 287.00989\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 287.00989\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 287.00989\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 287.00989\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 287.00989\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 287.00989\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 287.00989\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 287.00989\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 287.00989\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 287.00989\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 287.00989\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 287.00989\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 287.00989\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 287.00989\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 287.00989\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 287.00989\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 287.00989\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 287.00989\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 287.00989\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 287.00989\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 287.00989\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 287.00989\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 287.00989\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 287.00989\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 287.00989\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 287.00989\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 287.00989\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 287.00989\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 287.00989\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 287.00989\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 287.00989\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 287.00989\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 287.00989\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 287.00989\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 287.00989\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 287.00989\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 287.00989\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 287.00989\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 287.00989\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 287.00989\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 287.00989\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 287.00989\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 287.00989\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 287.00989\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 287.00989\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 287.00989\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 287.00989\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 287.00989\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 287.00989\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 287.00989\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 287.00989\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 287.00989\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 287.00989\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 287.00989\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 287.00989\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 287.00989\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 287.00989\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 287.00989\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 287.00989\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 287.00989\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 287.00989\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 287.00989\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 287.00989\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 287.00989\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 287.00989\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 287.00989\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 287.00989\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 287.00989\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 287.00989\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 287.00989\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 287.00989\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 287.00989\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 287.00989\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 287.00989\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 287.00989\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 287.00989\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 287.00989\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 287.00989\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 287.00989\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 287.00989\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 287.00989\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 287.00989\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 287.00989\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 287.00989\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 287.00989\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 287.00989\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 287.00989\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 287.00989\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 287.00989\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 287.00989\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 287.00989\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 287.00989\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 287.00989\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 287.00989\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 287.00989\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 287.00989\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 287.00989\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 287.00989\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 287.00989\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 287.00989\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 287.00989\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 287.00989\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 287.00989\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 287.00989\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 287.00989\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 287.00989\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 287.00989\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 287.00989\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 287.00989\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 287.00989\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 287.00989\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 287.00989\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 287.00989\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 287.00989\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 287.00989\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 287.00989\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 287.00989\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 287.00989\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 287.00989\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 287.00989\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 287.00989\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 287.00989\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 287.00989\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 287.00989\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 287.00989\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 287.00989\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 287.00989\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 287.00989\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 287.00989\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 287.00989\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 287.00989\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 287.00989\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 287.00989\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 287.00989\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 287.00989\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 287.00989\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 287.00989\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 287.00989\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 287.00989\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 287.00989\n"
     ]
    }
   ],
   "source": [
    "#### CALL BACKS!\n",
    "es = EarlyStopping(monitor      = 'val_loss', \n",
    "                        mode     = 'min', \n",
    "                        verbose  = 1, \n",
    "                        patience = 1000,\n",
    "                    restore_best_weights = True)\n",
    "# CSV Logger\n",
    "csv_logger = CSVLogger(f'{csv_logger_path}/model_history1.csv' , append=True)\n",
    "\n",
    "# CP_callbacks      not required when using a tunner       \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{cp_callback_path}/cp1.h5',#ckpt',\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                save_best_only = True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_path, \n",
    "                                                       #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                      update_freq = 1,\n",
    "                                                      # histogram_freq=1, \n",
    "                                                      write_graph=False, \n",
    "                                                      write_images=False)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "\n",
    "# Covert to list and provide to Keras Regressor\n",
    "keras_callbacks = [es, csv_logger, cp_callback, tensorboard_callback]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialise model for one single resample/hyper param tuning (train cycle)\n",
    "model2 = build_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit convolutional layers on all properties and optimisied initial fingures.\n",
    "history = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                        epochs=600,\n",
    "                        batch_size=16, \n",
    "                        validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                        verbose = 3,\n",
    "                        callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5292ab2-0a7b-4833-8311-49ecf0b021d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da451205-9ac4-47b0-907f-ff3d4ba169da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "34a06890-8bcc-46f3-8d94-1a8badeece81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(f'{cp_callback_path}/cp1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06230483-9f7f-4dc7-9d16-a710d991b99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "dH: r2=0.928, rmsd=6.600, mse=43.563, mae=5.132\n",
      "dS: r2=0.913, rmsd=19.455, mse=378.502, mae=15.228\n",
      "dG: r2=0.950, rmsd=0.988, mse=0.975, mae=0.742\n",
      "Tm: r2=0.870, rmsd=4.874, mse=23.758, mae=3.674\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02166631-0ef2-4ca3-a1a1-aee0ab726b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "dH: r2=0.932, rmsd=6.431, mse=41.354, mae=5.031\n",
      "dS: r2=0.919, rmsd=18.766, mse=352.147, mae=14.747\n",
      "dG: r2=0.962, rmsd=0.860, mse=0.739, mae=0.638\n",
      "Tm: r2=0.905, rmsd=4.178, mse=17.455, mae=3.105\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55a67c02-adcb-4223-bf48-0c174f405825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_12\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "conv1d_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_1\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_1\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "conv1d_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_2\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_2\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "conv1d_3\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_3\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_3\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "flatten\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "node1_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_1\n",
      "node2_1 <keras.layers.core.dense.Dense object at 0x14ac555e7880>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node3_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node1_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_2\n",
      "node2_2 <keras.layers.core.dense.Dense object at 0x14ac555f2f70>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node3_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node1_3\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_3\n",
      "node2_3 <keras.layers.core.dense.Dense object at 0x14ac555f2610>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node3_3\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_3\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dH\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dS\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dG\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Tm\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 287.00989\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 287.00989\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 287.00989\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 287.00989\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 287.00989\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 287.00989\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 287.00989\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 287.00989\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 287.00989\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 287.00989\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 287.00989\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 287.00989\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 287.00989\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 287.00989\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 287.00989\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 287.00989\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 287.00989\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 287.00989\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 287.00989\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 287.00989\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 287.00989\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 287.00989\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 287.00989\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 287.00989\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 287.00989\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 287.00989\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 287.00989\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 287.00989\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 287.00989\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 287.00989\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 287.00989\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 287.00989\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 287.00989\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 287.00989\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 287.00989\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 287.00989\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 287.00989\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 287.00989\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 287.00989\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 287.00989\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 287.00989\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 287.00989\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 287.00989\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 287.00989\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 287.00989\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 287.00989\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 287.00989\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 287.00989\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 287.00989\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 287.00989\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 287.00989\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 287.00989\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 287.00989\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 287.00989\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 287.00989\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 287.00989\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 287.00989\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 287.00989\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 287.00989\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 287.00989\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 287.00989\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 287.00989\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 287.00989\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 287.00989\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 287.00989\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 287.00989\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 287.00989\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 287.00989\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 287.00989\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 287.00989\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 287.00989\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 287.00989\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 287.00989\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 287.00989\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 287.00989\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 287.00989\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 287.00989\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 287.00989\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 287.00989\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 287.00989\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 287.00989\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 287.00989\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 287.00989\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 287.00989\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 287.00989\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 287.00989\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 287.00989\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 287.00989\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 287.00989\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 287.00989\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 287.00989\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 287.00989\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 287.00989\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 287.00989\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 287.00989\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 287.00989\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 287.00989\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 287.00989\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 287.00989\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 287.00989\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 287.00989\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 287.00989\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 287.00989\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 287.00989\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 287.00989\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 287.00989\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 287.00989\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 287.00989\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 287.00989\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 287.00989\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 287.00989\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 287.00989\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 287.00989\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 287.00989\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 287.00989\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 287.00989\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 287.00989\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 287.00989\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 287.00989\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 287.00989\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 287.00989\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 287.00989\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 287.00989\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 287.00989\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 287.00989\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 287.00989\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 287.00989\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 287.00989\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 287.00989\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 287.00989\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 287.00989\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 287.00989\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 287.00989\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 287.00989\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 287.00989\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 287.00989\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 287.00989\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 287.00989\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 287.00989\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 287.00989\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 287.00989\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 287.00989\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 287.00989\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 287.00989\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 287.00989\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 287.00989\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 287.00989\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 287.00989\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 287.00989\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 287.00989\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 287.00989\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 287.00989\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 287.00989\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 287.00989\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 287.00989\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 287.00989\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 287.00989\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 287.00989\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 287.00989\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 287.00989\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 287.00989\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 287.00989\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 287.00989\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 287.00989\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 287.00989\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 287.00989\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 287.00989\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 287.00989\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 287.00989\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 287.00989\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 287.00989\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 287.00989\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 287.00989\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 287.00989\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 287.00989\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 287.00989\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 287.00989\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 287.00989\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 287.00989\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 287.00989\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 287.00989\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 287.00989\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 287.00989\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 287.00989\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 287.00989\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 287.00989\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 287.00989\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 287.00989\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 287.00989\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 287.00989\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 287.00989\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 287.00989\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 287.00989\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 287.00989\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 287.00989\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 287.00989\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 287.00989\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 287.00989\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 287.00989\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 287.00989\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 287.00989\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 287.00989\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 287.00989\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 287.00989\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 287.00989\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 287.00989\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 287.00989\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 287.00989\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 287.00989\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 287.00989\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 287.00989\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 287.00989\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 287.00989\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 287.00989\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 287.00989\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 287.00989\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 287.00989\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 287.00989\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 287.00989\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 287.00989\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 287.00989\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 287.00989\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 287.00989\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 287.00989\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 287.00989\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 287.00989\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 287.00989\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 287.00989\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 287.00989\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 287.00989\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 287.00989\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 287.00989\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 287.00989\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 287.00989\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 287.00989\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 287.00989\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 287.00989\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 287.00989\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 287.00989\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 287.00989\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 287.00989\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 287.00989\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 287.00989\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 287.00989\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 287.00989\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 287.00989\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 287.00989\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 287.00989\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 287.00989\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 287.00989\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 287.00989\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 287.00989\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 287.00989\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 287.00989\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 287.00989\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 287.00989\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 287.00989\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 287.00989\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 287.00989\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 287.00989\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 287.00989\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 287.00989\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 287.00989\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 287.00989\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 287.00989\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 287.00989\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 287.00989\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 287.00989\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 287.00989\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 287.00989\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 287.00989\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 287.00989\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 287.00989\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 287.00989\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 287.00989\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 287.00989\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 287.00989\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 287.00989\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 287.00989\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 287.00989\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 287.00989\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 287.00989\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 287.00989\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 287.00989\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 287.00989\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 287.00989\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 287.00989\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 287.00989\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 287.00989\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 287.00989\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 287.00989\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 287.00989\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 287.00989\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 287.00989\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 287.00989\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 287.00989\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 287.00989\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 287.00989\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 287.00989\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 287.00989\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 287.00989\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 287.00989\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 287.00989\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 287.00989\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 287.00989\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 287.00989\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 287.00989\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 287.00989\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 287.00989\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 287.00989\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 287.00989\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 287.00989\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 287.00989\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 287.00989\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 287.00989\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 287.00989\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 287.00989\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 287.00989\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 287.00989\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 287.00989\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 287.00989\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 287.00989\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 287.00989\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 287.00989\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 287.00989\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 287.00989\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 287.00989\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 287.00989\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 287.00989\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 287.00989\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 287.00989\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 287.00989\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 287.00989\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 287.00989\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 287.00989\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 287.00989\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 287.00989\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 287.00989\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 287.00989\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 287.00989\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 287.00989\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 287.00989\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 287.00989\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 287.00989\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 287.00989\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 287.00989\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 287.00989\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 287.00989\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 287.00989\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 287.00989\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 287.00989\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 287.00989\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 287.00989\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 287.00989\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 287.00989\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 287.00989\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 287.00989\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 287.00989\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 287.00989\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 287.00989\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 287.00989\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 287.00989\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 287.00989\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 287.00989\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 287.00989\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 287.00989\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 287.00989\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 287.00989\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 287.00989\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 287.00989\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 287.00989\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 287.00989\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 287.00989\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 287.00989\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 287.00989\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 287.00989\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 287.00989\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 287.00989\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 287.00989\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 287.00989\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 287.00989\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 287.00989\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 287.00989\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 287.00989\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 287.00989\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 287.00989\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 287.00989\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 287.00989\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 287.00989\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 287.00989\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 287.00989\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 287.00989\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 287.00989\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 287.00989\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 287.00989\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 287.00989\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 287.00989\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 287.00989\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 287.00989\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 287.00989\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 287.00989\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 287.00989\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 287.00989\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 287.00989\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 287.00989\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 287.00989\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 287.00989\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 287.00989\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 287.00989\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 287.00989\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 287.00989\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 287.00989\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 287.00989\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 287.00989\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 287.00989\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 287.00989\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 287.00989\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 287.00989\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 287.00989\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 287.00989\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 287.00989\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 287.00989\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 287.00989\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 287.00989\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 287.00989\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 287.00989\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 287.00989\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 287.00989\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 287.00989\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 287.00989\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 287.00989\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 287.00989\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 287.00989\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 287.00989\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 287.00989\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 287.00989\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 287.00989\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 287.00989\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 287.00989\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 287.00989\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 287.00989\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 287.00989\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 287.00989\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 287.00989\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 287.00989\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 287.00989\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 287.00989\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 287.00989\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 287.00989\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 287.00989\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 287.00989\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 287.00989\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 287.00989\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 287.00989\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 287.00989\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 287.00989\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 287.00989\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 287.00989\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 287.00989\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 287.00989\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 287.00989\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 287.00989\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 287.00989\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 287.00989\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 287.00989\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 287.00989\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 287.00989\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 287.00989\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 287.00989\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 287.00989\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 287.00989\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 287.00989\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 287.00989\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 287.00989\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 287.00989\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 287.00989\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 287.00989\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 287.00989\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 287.00989\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 287.00989\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 287.00989\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 287.00989\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 287.00989\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 287.00989\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 287.00989\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 287.00989\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 287.00989\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 287.00989\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 287.00989\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 287.00989\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 287.00989\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 287.00989\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 287.00989\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 287.00989\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 287.00989\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 287.00989\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 287.00989\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 287.00989\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 287.00989\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 287.00989\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 287.00989\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 287.00989\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 287.00989\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 287.00989\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 287.00989\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 287.00989\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 287.00989\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 287.00989\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 287.00989\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 287.00989\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 287.00989\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 287.00989\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 287.00989\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 287.00989\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 287.00989\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 287.00989\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 287.00989\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 287.00989\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 287.00989\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 287.00989\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 287.00989\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 287.00989\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 287.00989\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 287.00989\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 287.00989\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 287.00989\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 287.00989\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 287.00989\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 287.00989\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 287.00989\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 287.00989\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 287.00989\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 287.00989\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 287.00989\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 287.00989\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 287.00989\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 287.00989\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 287.00989\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 287.00989\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 287.00989\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 287.00989\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 287.00989\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 287.00989\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 287.00989\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 287.00989\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 287.00989\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 287.00989\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 287.00989\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 287.00989\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 287.00989\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 287.00989\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 287.00989\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 287.00989\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 287.00989\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 287.00989\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 287.00989\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 287.00989\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 287.00989\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 287.00989\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 287.00989\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 287.00989\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 287.00989\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 287.00989\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 287.00989\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 287.00989\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 287.00989\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 287.00989\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 287.00989\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 287.00989\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 287.00989\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 287.00989\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 287.00989\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 287.00989\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 287.00989\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 287.00989\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 287.00989\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 287.00989\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 287.00989\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 287.00989\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 287.00989\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 287.00989\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 287.00989\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 287.00989\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 287.00989\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 287.00989\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 287.00989\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 287.00989\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 287.00989\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 287.00989\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 287.00989\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 287.00989\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 287.00989\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 287.00989\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 287.00989\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 287.00989\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 287.00989\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 287.00989\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 287.00989\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 287.00989\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 287.00989\n"
     ]
    }
   ],
   "source": [
    "finger = 2\n",
    "for layer in model2.layers:\n",
    "    layer.trainable=False\n",
    "    print(layer.name)\n",
    "    if layer.name in [f'node{finger}_1',f'node{finger}_2',f'node{finger}_3']:#,prop[finger-1]]:#\n",
    "        layer.trainable=True\n",
    "        print(layer.name, layer)\n",
    "    print(\"weights:\", len(layer.weights))\n",
    "    print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "    print(\"non_trainable_weights:\", len(layer.non_trainable_weights))\n",
    "    # print(layer.trainable_weights)\n",
    "history = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                        epochs=600,\n",
    "                        batch_size=16, \n",
    "                        validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                        verbose = 3,\n",
    "                        callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94fdbdc2-374e-4f4d-85bd-5af576d16415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.896, rmsd=7.945, mse=63.126, mae=6.136\n",
      "dS: r2=0.880, rmsd=22.845, mse=521.878, mae=17.711\n",
      "dG: r2=0.919, rmsd=1.254, mse=1.572, mae=0.900\n",
      "Tm: r2=0.885, rmsd=4.581, mse=20.987, mae=3.574\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fda1c818-f5dd-463a-81b7-4d3425a887df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.892, rmsd=8.105, mse=65.696, mae=6.262\n",
      "dS: r2=0.876, rmsd=23.230, mse=539.622, mae=18.022\n",
      "dG: r2=0.917, rmsd=1.267, mse=1.605, mae=0.908\n",
      "Tm: r2=0.885, rmsd=4.591, mse=21.078, mae=3.583\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3dbb5310-efa0-438a-a08a-f2d64a88cea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.913, rmsd=7.267, mse=52.805, mae=5.735\n",
      "dS: r2=0.897, rmsd=21.193, mse=449.156, mae=16.610\n",
      "dG: r2=0.932, rmsd=1.148, mse=1.317, mae=0.835\n",
      "Tm: r2=0.886, rmsd=4.562, mse=20.809, mae=3.537\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e279196-4cb6-4ebd-ae26-e5bded0523ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.896, rmsd=7.957, mse=63.318, mae=6.154\n",
      "dS: r2=0.882, rmsd=22.678, mse=514.312, mae=17.694\n",
      "dG: r2=0.912, rmsd=1.306, mse=1.705, mae=0.938\n",
      "Tm: r2=0.866, rmsd=4.958, mse=24.587, mae=3.925\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2883b2a3-d36c-4976-8fe9-224c89ecada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14abc597de50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.896, rmsd=7.965, mse=63.448, mae=6.141\n",
      "dS: r2=0.886, rmsd=22.263, mse=495.643, mae=17.274\n",
      "dG: r2=0.897, rmsd=1.411, mse=1.990, mae=1.056\n",
      "Tm: r2=0.827, rmsd=5.621, mse=31.601, mae=4.411\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "feb4fd39-f1c4-43b7-845f-434b54cf0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.848, rmsd=9.631, mse=92.766, mae=7.426\n",
      "dS: r2=0.824, rmsd=27.700, mse=767.280, mae=21.583\n",
      "dG: r2=0.742, rmsd=2.232, mse=4.980, mae=1.756\n",
      "Tm: r2=0.659, rmsd=7.895, mse=62.334, mae=6.340\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d46455dd-ee0b-47e5-ae67-1899d8e92835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.930, rmsd=6.547, mse=42.870, mae=5.099\n",
      "dS: r2=0.917, rmsd=18.979, mse=360.203, mae=15.003\n",
      "dG: r2=0.936, rmsd=1.112, mse=1.236, mae=0.893\n",
      "Tm: r2=0.818, rmsd=5.768, mse=33.265, mae=4.127\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61369949-4c0e-4d37-a26b-b067a88dda77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 250, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 124, 32)      128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " maxpooling_1 (MaxPooling1D)    (None, 62, 32)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_1 (BatchNormalizatio  (None, 62, 32)      128         ['maxpooling_1[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 30, 32)       3104        ['batchnorm_1[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_2 (MaxPooling1D)    (None, 15, 32)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_2 (BatchNormalizatio  (None, 15, 32)      128         ['maxpooling_2[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 7, 32)        3104        ['batchnorm_2[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_3 (MaxPooling1D)    (None, 3, 32)        0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_3 (BatchNormalizatio  (None, 3, 32)       128         ['maxpooling_3[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['batchnorm_3[0][0]']            \n",
      "                                                                                                  \n",
      " node1_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node2_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node3_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node4_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node1_2 (Dense)                (None, 32)           544         ['node1_1[0][0]']                \n",
      "                                                                                                  \n",
      " node2_2 (Dense)                (None, 32)           544         ['node2_1[0][0]']                \n",
      "                                                                                                  \n",
      " node3_2 (Dense)                (None, 32)           544         ['node3_1[0][0]']                \n",
      "                                                                                                  \n",
      " node4_2 (Dense)                (None, 32)           544         ['node4_1[0][0]']                \n",
      "                                                                                                  \n",
      " node1_3 (Dense)                (None, 16)           528         ['node1_2[0][0]']                \n",
      "                                                                                                  \n",
      " node2_3 (Dense)                (None, 16)           528         ['node2_2[0][0]']                \n",
      "                                                                                                  \n",
      " node3_3 (Dense)                (None, 16)           528         ['node3_2[0][0]']                \n",
      "                                                                                                  \n",
      " node4_3 (Dense)                (None, 16)           528         ['node4_2[0][0]']                \n",
      "                                                                                                  \n",
      " dH (Dense)                     (None, 1)            17          ['node1_3[0][0]']                \n",
      "                                                                                                  \n",
      " dS (Dense)                     (None, 1)            17          ['node2_3[0][0]']                \n",
      "                                                                                                  \n",
      " dG (Dense)                     (None, 1)            17          ['node3_3[0][0]']                \n",
      "                                                                                                  \n",
      " Tm (Dense)                     (None, 1)            17          ['node4_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,284\n",
      "Trainable params: 14,468\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d8f8ce8-6d9b-485a-bddc-d0eb1dfe67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node4_1 <keras.layers.core.dense.Dense object at 0x14a8b0fda400>\n",
      "node4_2 <keras.layers.core.dense.Dense object at 0x14a8b0fc1e50>\n",
      "node4_3 <keras.layers.core.dense.Dense object at 0x14a8b0fc1e80>\n",
      "Tm <keras.layers.core.dense.Dense object at 0x14a8b0ff1220>\n"
     ]
    }
   ],
   "source": [
    "finger = 4\n",
    "for layer in model2.layers:\n",
    "    layer.trainable=False\n",
    "    if layer.name in [f'node{finger}_1',f'node{finger}_2',f'node{finger}_3',prop[finger-1]]:\n",
    "        layer.trainable=True\n",
    "        print(layer.name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d75956a-b0da-49fa-9961-f6bcdafc4ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 250, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 124, 32)      128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " maxpooling_1 (MaxPooling1D)    (None, 62, 32)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_1 (BatchNormalizatio  (None, 62, 32)      128         ['maxpooling_1[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 30, 32)       3104        ['batchnorm_1[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_2 (MaxPooling1D)    (None, 15, 32)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_2 (BatchNormalizatio  (None, 15, 32)      128         ['maxpooling_2[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 7, 32)        3104        ['batchnorm_2[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_3 (MaxPooling1D)    (None, 3, 32)        0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_3 (BatchNormalizatio  (None, 3, 32)       128         ['maxpooling_3[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['batchnorm_3[0][0]']            \n",
      "                                                                                                  \n",
      " node1_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node2_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node3_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node4_1 (Dense)                (None, 16)           1552        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " node1_2 (Dense)                (None, 32)           544         ['node1_1[0][0]']                \n",
      "                                                                                                  \n",
      " node2_2 (Dense)                (None, 32)           544         ['node2_1[0][0]']                \n",
      "                                                                                                  \n",
      " node3_2 (Dense)                (None, 32)           544         ['node3_1[0][0]']                \n",
      "                                                                                                  \n",
      " node4_2 (Dense)                (None, 32)           544         ['node4_1[0][0]']                \n",
      "                                                                                                  \n",
      " node1_3 (Dense)                (None, 16)           528         ['node1_2[0][0]']                \n",
      "                                                                                                  \n",
      " node2_3 (Dense)                (None, 16)           528         ['node2_2[0][0]']                \n",
      "                                                                                                  \n",
      " node3_3 (Dense)                (None, 16)           528         ['node3_2[0][0]']                \n",
      "                                                                                                  \n",
      " node4_3 (Dense)                (None, 16)           528         ['node4_2[0][0]']                \n",
      "                                                                                                  \n",
      " dH (Dense)                     (None, 1)            17          ['node1_3[0][0]']                \n",
      "                                                                                                  \n",
      " dS (Dense)                     (None, 1)            17          ['node2_3[0][0]']                \n",
      "                                                                                                  \n",
      " dG (Dense)                     (None, 1)            17          ['node3_3[0][0]']                \n",
      "                                                                                                  \n",
      " Tm (Dense)                     (None, 1)            17          ['node4_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,284\n",
      "Trainable params: 2,624\n",
      "Non-trainable params: 14,660\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93b06cc5-78f2-44e2-a9fc-7302075cb049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "Epoch 2/600\n",
      "Epoch 3/600\n",
      "Epoch 4/600\n",
      "Epoch 5/600\n",
      "Epoch 6/600\n",
      "Epoch 7/600\n",
      "Epoch 8/600\n",
      "Epoch 9/600\n",
      "Epoch 10/600\n",
      "Epoch 11/600\n",
      "Epoch 12/600\n",
      "Epoch 13/600\n",
      "Epoch 14/600\n",
      "Epoch 15/600\n",
      "Epoch 16/600\n",
      "Epoch 17/600\n",
      "Epoch 18/600\n",
      "Epoch 19/600\n",
      "Epoch 20/600\n",
      "Epoch 21/600\n",
      "Epoch 22/600\n",
      "Epoch 23/600\n",
      "Epoch 24/600\n",
      "Epoch 25/600\n",
      "Epoch 26/600\n",
      "Epoch 27/600\n",
      "Epoch 28/600\n",
      "Epoch 29/600\n",
      "Epoch 30/600\n",
      "Epoch 31/600\n",
      "Epoch 32/600\n",
      "Epoch 33/600\n",
      "Epoch 34/600\n",
      "Epoch 35/600\n",
      "Epoch 36/600\n",
      "Epoch 37/600\n",
      "Epoch 38/600\n",
      "Epoch 39/600\n",
      "Epoch 40/600\n",
      "Epoch 41/600\n",
      "Epoch 42/600\n",
      "Epoch 43/600\n",
      "Epoch 44/600\n",
      "Epoch 45/600\n",
      "Epoch 46/600\n",
      "Epoch 47/600\n",
      "Epoch 48/600\n",
      "Epoch 49/600\n",
      "Epoch 50/600\n",
      "Epoch 51/600\n",
      "Epoch 52/600\n",
      "Epoch 53/600\n",
      "Epoch 54/600\n",
      "Epoch 55/600\n",
      "Epoch 56/600\n",
      "Epoch 57/600\n",
      "Epoch 58/600\n",
      "Epoch 59/600\n",
      "Epoch 60/600\n",
      "Epoch 61/600\n",
      "Epoch 62/600\n",
      "Epoch 63/600\n",
      "Epoch 64/600\n",
      "Epoch 65/600\n",
      "Epoch 66/600\n",
      "Epoch 67/600\n",
      "Epoch 68/600\n",
      "Epoch 69/600\n",
      "Epoch 70/600\n",
      "Epoch 71/600\n",
      "Epoch 72/600\n",
      "Epoch 73/600\n",
      "Epoch 74/600\n",
      "Epoch 75/600\n",
      "Epoch 76/600\n",
      "Epoch 77/600\n",
      "Epoch 78/600\n",
      "Epoch 79/600\n",
      "Epoch 80/600\n",
      "Epoch 81/600\n",
      "Epoch 82/600\n",
      "Epoch 83/600\n",
      "Epoch 84/600\n",
      "Epoch 85/600\n",
      "Epoch 86/600\n",
      "Epoch 87/600\n",
      "Epoch 88/600\n",
      "Epoch 89/600\n",
      "Epoch 90/600\n",
      "Epoch 91/600\n",
      "Epoch 92/600\n",
      "Epoch 93/600\n",
      "Epoch 94/600\n",
      "Epoch 95/600\n",
      "Epoch 96/600\n",
      "Epoch 97/600\n",
      "Epoch 98/600\n",
      "Epoch 99/600\n",
      "Epoch 100/600\n",
      "Epoch 101/600\n",
      "Epoch 102/600\n",
      "Epoch 103/600\n",
      "Epoch 104/600\n",
      "Epoch 105/600\n",
      "Epoch 106/600\n",
      "Epoch 107/600\n",
      "Epoch 108/600\n",
      "Epoch 109/600\n",
      "Epoch 110/600\n",
      "Epoch 111/600\n",
      "Epoch 112/600\n",
      "Epoch 113/600\n",
      "Epoch 114/600\n",
      "Epoch 115/600\n",
      "Epoch 116/600\n",
      "Epoch 117/600\n",
      "Epoch 118/600\n",
      "Epoch 119/600\n",
      "Epoch 120/600\n",
      "Epoch 121/600\n",
      "Epoch 122/600\n",
      "Epoch 123/600\n",
      "Epoch 124/600\n",
      "Epoch 125/600\n",
      "Epoch 126/600\n",
      "Epoch 127/600\n",
      "Epoch 128/600\n",
      "Epoch 129/600\n",
      "Epoch 130/600\n",
      "Epoch 131/600\n",
      "Epoch 132/600\n",
      "Epoch 133/600\n",
      "Epoch 134/600\n",
      "Epoch 135/600\n",
      "Epoch 136/600\n",
      "Epoch 137/600\n",
      "Epoch 138/600\n",
      "Epoch 139/600\n",
      "Epoch 140/600\n",
      "Epoch 141/600\n",
      "Epoch 142/600\n",
      "Epoch 143/600\n",
      "Epoch 144/600\n",
      "Epoch 145/600\n",
      "Epoch 146/600\n",
      "Epoch 147/600\n",
      "Epoch 148/600\n",
      "Epoch 149/600\n",
      "Epoch 150/600\n",
      "Epoch 151/600\n",
      "Epoch 152/600\n",
      "Epoch 153/600\n",
      "Epoch 154/600\n",
      "Epoch 155/600\n",
      "Epoch 156/600\n",
      "Epoch 157/600\n",
      "Epoch 158/600\n",
      "Epoch 159/600\n",
      "Epoch 160/600\n",
      "Epoch 161/600\n",
      "Epoch 162/600\n",
      "Epoch 163/600\n",
      "Epoch 164/600\n",
      "Epoch 165/600\n",
      "Epoch 166/600\n",
      "Epoch 167/600\n",
      "Epoch 168/600\n",
      "Epoch 169/600\n",
      "Epoch 170/600\n",
      "Epoch 171/600\n",
      "Epoch 172/600\n",
      "Epoch 173/600\n",
      "Epoch 174/600\n",
      "Epoch 175/600\n",
      "Epoch 176/600\n",
      "Epoch 177/600\n",
      "Epoch 178/600\n",
      "Epoch 179/600\n",
      "Epoch 180/600\n",
      "Epoch 181/600\n",
      "Epoch 182/600\n",
      "Epoch 183/600\n",
      "Epoch 184/600\n",
      "Epoch 185/600\n",
      "Epoch 186/600\n",
      "Epoch 187/600\n",
      "Epoch 188/600\n",
      "Epoch 189/600\n",
      "Epoch 190/600\n",
      "Epoch 191/600\n",
      "Epoch 192/600\n",
      "Epoch 193/600\n",
      "Epoch 194/600\n",
      "Epoch 195/600\n",
      "Epoch 196/600\n",
      "Epoch 197/600\n",
      "Epoch 198/600\n",
      "Epoch 199/600\n",
      "Epoch 200/600\n",
      "Epoch 201/600\n",
      "Epoch 202/600\n",
      "Epoch 203/600\n",
      "Epoch 204/600\n",
      "Epoch 205/600\n",
      "Epoch 206/600\n",
      "Epoch 207/600\n",
      "Epoch 208/600\n",
      "Epoch 209/600\n",
      "Epoch 210/600\n",
      "Epoch 211/600\n",
      "Epoch 212/600\n",
      "Epoch 213/600\n",
      "Epoch 214/600\n",
      "Epoch 215/600\n",
      "Epoch 216/600\n",
      "Epoch 217/600\n",
      "Epoch 218/600\n",
      "Epoch 219/600\n",
      "Epoch 220/600\n",
      "Epoch 221/600\n",
      "Epoch 222/600\n",
      "Epoch 223/600\n",
      "Epoch 224/600\n",
      "Epoch 225/600\n",
      "Epoch 226/600\n",
      "Epoch 227/600\n",
      "Epoch 228/600\n",
      "Epoch 229/600\n",
      "Epoch 230/600\n",
      "Epoch 231/600\n",
      "Epoch 232/600\n",
      "Epoch 233/600\n",
      "Epoch 234/600\n",
      "Epoch 235/600\n",
      "Epoch 236/600\n",
      "Epoch 237/600\n",
      "Epoch 238/600\n",
      "Epoch 239/600\n",
      "Epoch 240/600\n",
      "Epoch 241/600\n",
      "Epoch 242/600\n",
      "Epoch 243/600\n",
      "Epoch 244/600\n",
      "Epoch 245/600\n",
      "Epoch 246/600\n",
      "Epoch 247/600\n",
      "Epoch 248/600\n",
      "Epoch 249/600\n",
      "Epoch 250/600\n",
      "Epoch 251/600\n",
      "Epoch 252/600\n",
      "Epoch 253/600\n",
      "Epoch 254/600\n",
      "Epoch 255/600\n",
      "Epoch 256/600\n",
      "Epoch 257/600\n",
      "Epoch 258/600\n",
      "Epoch 259/600\n",
      "Epoch 260/600\n",
      "Epoch 261/600\n",
      "Epoch 262/600\n",
      "Epoch 263/600\n",
      "Epoch 264/600\n",
      "Epoch 265/600\n",
      "Epoch 266/600\n",
      "Epoch 267/600\n",
      "Epoch 268/600\n",
      "Epoch 269/600\n",
      "Epoch 270/600\n",
      "Epoch 271/600\n",
      "Epoch 272/600\n",
      "Epoch 273/600\n",
      "Epoch 274/600\n",
      "Epoch 275/600\n",
      "Epoch 276/600\n",
      "Epoch 277/600\n",
      "Epoch 278/600\n",
      "Epoch 279/600\n",
      "Epoch 280/600\n",
      "Epoch 281/600\n",
      "Epoch 282/600\n",
      "Epoch 283/600\n",
      "Epoch 284/600\n",
      "Epoch 285/600\n",
      "Epoch 286/600\n",
      "Epoch 287/600\n",
      "Epoch 288/600\n",
      "Epoch 289/600\n",
      "Epoch 290/600\n",
      "Epoch 291/600\n",
      "Epoch 292/600\n",
      "Epoch 293/600\n",
      "Epoch 294/600\n",
      "Epoch 295/600\n",
      "Epoch 296/600\n",
      "Epoch 297/600\n",
      "Epoch 298/600\n",
      "Epoch 299/600\n",
      "Epoch 300/600\n",
      "Epoch 301/600\n",
      "Epoch 302/600\n",
      "Epoch 303/600\n",
      "Epoch 304/600\n",
      "Epoch 305/600\n",
      "Epoch 306/600\n",
      "Epoch 307/600\n",
      "Epoch 308/600\n",
      "Epoch 309/600\n",
      "Epoch 310/600\n",
      "Epoch 311/600\n",
      "Epoch 312/600\n",
      "Epoch 313/600\n",
      "Epoch 314/600\n",
      "Epoch 315/600\n",
      "Epoch 316/600\n",
      "Epoch 317/600\n",
      "Epoch 318/600\n",
      "Epoch 319/600\n",
      "Epoch 320/600\n",
      "Epoch 321/600\n",
      "Epoch 322/600\n",
      "Epoch 323/600\n",
      "Epoch 324/600\n",
      "Epoch 325/600\n",
      "Epoch 326/600\n",
      "Epoch 327/600\n",
      "Epoch 328/600\n",
      "Epoch 329/600\n",
      "Epoch 330/600\n",
      "Epoch 331/600\n",
      "Epoch 332/600\n",
      "Epoch 333/600\n",
      "Epoch 334/600\n",
      "Epoch 335/600\n",
      "Epoch 336/600\n",
      "Epoch 337/600\n",
      "Epoch 338/600\n",
      "Epoch 339/600\n",
      "Epoch 340/600\n",
      "Epoch 341/600\n",
      "Epoch 342/600\n",
      "Epoch 343/600\n",
      "Epoch 344/600\n",
      "Epoch 345/600\n",
      "Epoch 346/600\n",
      "Epoch 347/600\n",
      "Epoch 348/600\n",
      "Epoch 349/600\n",
      "Epoch 350/600\n",
      "Epoch 351/600\n",
      "Epoch 352/600\n",
      "Epoch 353/600\n",
      "Epoch 354/600\n",
      "Epoch 355/600\n",
      "Epoch 356/600\n",
      "Epoch 357/600\n",
      "Epoch 358/600\n",
      "Epoch 359/600\n",
      "Epoch 360/600\n",
      "Epoch 361/600\n",
      "Epoch 362/600\n",
      "Epoch 363/600\n",
      "Epoch 364/600\n",
      "Epoch 365/600\n",
      "Epoch 366/600\n",
      "Epoch 367/600\n",
      "Epoch 368/600\n",
      "Epoch 369/600\n",
      "Epoch 370/600\n",
      "Epoch 371/600\n",
      "Epoch 372/600\n",
      "Epoch 373/600\n",
      "Epoch 374/600\n",
      "Epoch 375/600\n",
      "Epoch 376/600\n",
      "Epoch 377/600\n",
      "Epoch 378/600\n",
      "Epoch 379/600\n",
      "Epoch 380/600\n",
      "Epoch 381/600\n",
      "Epoch 382/600\n",
      "Epoch 383/600\n",
      "Epoch 384/600\n",
      "Epoch 385/600\n",
      "Epoch 386/600\n",
      "Epoch 387/600\n",
      "Epoch 388/600\n",
      "Epoch 389/600\n",
      "Epoch 390/600\n",
      "Epoch 391/600\n",
      "Epoch 392/600\n",
      "Epoch 393/600\n",
      "Epoch 394/600\n",
      "Epoch 395/600\n",
      "Epoch 396/600\n",
      "Epoch 397/600\n",
      "Epoch 398/600\n",
      "Epoch 399/600\n",
      "Epoch 400/600\n",
      "Epoch 401/600\n",
      "Epoch 402/600\n",
      "Epoch 403/600\n",
      "Epoch 404/600\n",
      "Epoch 405/600\n",
      "Epoch 406/600\n",
      "Epoch 407/600\n",
      "Epoch 408/600\n",
      "Epoch 409/600\n",
      "Epoch 410/600\n",
      "Epoch 411/600\n",
      "Epoch 412/600\n",
      "Epoch 413/600\n",
      "Epoch 414/600\n",
      "Epoch 415/600\n",
      "Epoch 416/600\n",
      "Epoch 417/600\n",
      "Epoch 418/600\n",
      "Epoch 419/600\n",
      "Epoch 420/600\n",
      "Epoch 421/600\n",
      "Epoch 422/600\n",
      "Epoch 423/600\n",
      "Epoch 424/600\n",
      "Epoch 425/600\n",
      "Epoch 426/600\n",
      "Epoch 427/600\n",
      "Epoch 428/600\n",
      "Epoch 429/600\n",
      "Epoch 430/600\n",
      "Epoch 431/600\n",
      "Epoch 432/600\n",
      "Epoch 433/600\n",
      "Epoch 434/600\n",
      "Epoch 435/600\n",
      "Epoch 436/600\n",
      "Epoch 437/600\n",
      "Epoch 438/600\n",
      "Epoch 439/600\n",
      "Epoch 440/600\n",
      "Epoch 441/600\n",
      "Epoch 442/600\n",
      "Epoch 443/600\n",
      "Epoch 444/600\n",
      "Epoch 445/600\n",
      "Epoch 446/600\n",
      "Epoch 447/600\n",
      "Epoch 448/600\n",
      "Epoch 449/600\n",
      "Epoch 450/600\n",
      "Epoch 451/600\n",
      "Epoch 452/600\n",
      "Epoch 453/600\n",
      "Epoch 454/600\n",
      "Epoch 455/600\n",
      "Epoch 456/600\n",
      "Epoch 457/600\n",
      "Epoch 458/600\n",
      "Epoch 459/600\n",
      "Epoch 460/600\n",
      "Epoch 461/600\n",
      "Epoch 462/600\n",
      "Epoch 463/600\n",
      "Epoch 464/600\n",
      "Epoch 465/600\n",
      "Epoch 466/600\n",
      "Epoch 467/600\n",
      "Epoch 468/600\n",
      "Epoch 469/600\n",
      "Epoch 470/600\n",
      "Epoch 471/600\n",
      "Epoch 472/600\n",
      "Epoch 473/600\n",
      "Epoch 474/600\n",
      "Epoch 475/600\n",
      "Epoch 476/600\n",
      "Epoch 477/600\n",
      "Epoch 478/600\n",
      "Epoch 479/600\n",
      "Epoch 480/600\n",
      "Epoch 481/600\n",
      "Epoch 482/600\n",
      "Epoch 483/600\n",
      "Epoch 484/600\n",
      "Epoch 485/600\n",
      "Epoch 486/600\n",
      "Epoch 487/600\n",
      "Epoch 488/600\n",
      "Epoch 489/600\n",
      "Epoch 490/600\n",
      "Epoch 491/600\n",
      "Epoch 492/600\n",
      "Epoch 493/600\n",
      "Epoch 494/600\n",
      "Epoch 495/600\n",
      "Epoch 496/600\n",
      "Epoch 497/600\n",
      "Epoch 498/600\n",
      "Epoch 499/600\n",
      "Epoch 500/600\n",
      "Epoch 501/600\n",
      "Epoch 502/600\n",
      "Epoch 503/600\n",
      "Epoch 504/600\n",
      "Epoch 505/600\n",
      "Epoch 506/600\n",
      "Epoch 507/600\n",
      "Epoch 508/600\n",
      "Epoch 509/600\n",
      "Epoch 510/600\n",
      "Epoch 511/600\n",
      "Epoch 512/600\n",
      "Epoch 513/600\n",
      "Epoch 514/600\n",
      "Epoch 515/600\n",
      "Epoch 516/600\n",
      "Epoch 517/600\n",
      "Epoch 518/600\n",
      "Epoch 519/600\n",
      "Epoch 520/600\n",
      "Epoch 521/600\n",
      "Epoch 522/600\n",
      "Epoch 523/600\n",
      "Epoch 524/600\n",
      "Epoch 525/600\n",
      "Epoch 526/600\n",
      "Epoch 527/600\n",
      "Epoch 528/600\n",
      "Epoch 529/600\n",
      "Epoch 530/600\n",
      "Epoch 531/600\n",
      "Epoch 532/600\n",
      "Epoch 533/600\n",
      "Epoch 534/600\n",
      "Epoch 535/600\n",
      "Epoch 536/600\n",
      "Epoch 537/600\n",
      "Epoch 538/600\n",
      "Epoch 539/600\n",
      "Epoch 540/600\n",
      "Epoch 541/600\n",
      "Epoch 542/600\n",
      "Epoch 543/600\n",
      "Epoch 544/600\n",
      "Epoch 545/600\n",
      "Epoch 546/600\n",
      "Epoch 547/600\n",
      "Epoch 548/600\n",
      "Epoch 549/600\n",
      "Epoch 550/600\n",
      "Epoch 551/600\n",
      "Epoch 552/600\n",
      "Epoch 553/600\n",
      "Epoch 554/600\n",
      "Epoch 555/600\n",
      "Epoch 556/600\n",
      "Epoch 557/600\n",
      "Epoch 558/600\n",
      "Epoch 559/600\n",
      "Epoch 560/600\n",
      "Epoch 561/600\n",
      "Epoch 562/600\n",
      "Epoch 563/600\n",
      "Epoch 564/600\n",
      "Epoch 565/600\n",
      "Epoch 566/600\n",
      "Epoch 567/600\n",
      "Epoch 568/600\n",
      "Epoch 569/600\n",
      "Epoch 570/600\n",
      "Epoch 571/600\n",
      "Epoch 572/600\n",
      "Epoch 573/600\n",
      "Epoch 574/600\n",
      "Epoch 575/600\n",
      "Epoch 576/600\n",
      "Epoch 577/600\n",
      "Epoch 578/600\n",
      "Epoch 579/600\n",
      "Epoch 580/600\n",
      "Epoch 581/600\n",
      "Epoch 582/600\n",
      "Epoch 583/600\n",
      "Epoch 584/600\n",
      "Epoch 585/600\n",
      "Epoch 586/600\n",
      "Epoch 587/600\n",
      "Epoch 588/600\n",
      "Epoch 589/600\n",
      "Epoch 590/600\n",
      "Epoch 591/600\n",
      "Epoch 592/600\n",
      "Epoch 593/600\n",
      "Epoch 594/600\n",
      "Epoch 595/600\n",
      "Epoch 596/600\n",
      "Epoch 597/600\n",
      "Epoch 598/600\n",
      "Epoch 599/600\n",
      "Epoch 600/600\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                    epochs=600,\n",
    "                    batch_size=16, \n",
    "                    validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                    verbose = 3,\n",
    "                    callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f819a79b-28ab-4575-aed2-e462b06a131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.934, rmsd=6.322, mse=39.964, mae=5.054\n",
      "dS: r2=0.919, rmsd=18.750, mse=351.553, mae=14.970\n",
      "dG: r2=0.970, rmsd=0.756, mse=0.571, mae=0.568\n",
      "Tm: r2=0.924, rmsd=3.725, mse=13.879, mae=2.731\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "for n in range(4):\n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34646a07-71e9-42ab-bec8-3b6864635678",
   "metadata": {},
   "source": [
    "# Model Eval 2CNN 2NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fbbfd-34e4-4f28-aa59-287082523cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5d3a9901-7a5e-4d00-904e-390a9099da20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1640.50903, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1640.50903\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1640.50903\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1640.50903\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1640.50903\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1640.50903\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1640.50903\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1640.50903\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1640.50903\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1640.50903\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1640.50903\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss improved from 1640.50903 to 1582.37573, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss improved from 1582.37573 to 1254.80725, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1254.80725\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1254.80725\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1254.80725\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss improved from 1254.80725 to 934.24890, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss improved from 934.24890 to 721.80066, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 721.80066\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss improved from 721.80066 to 526.87933, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 526.87933\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 526.87933\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 526.87933\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss improved from 526.87933 to 431.48920, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss improved from 431.48920 to 347.36325, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss improved from 347.36325 to 296.50104, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss improved from 296.50104 to 290.05658, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 290.05658\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 290.05658\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 290.05658\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 290.05658\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 290.05658\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss improved from 290.05658 to 281.00421, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 281.00421\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss improved from 281.00421 to 275.20465, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss improved from 275.20465 to 274.83325, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss improved from 274.83325 to 271.45184, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 271.45184\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss improved from 271.45184 to 271.03290, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 271.03290\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 271.03290\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 271.03290\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 271.03290\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 271.03290\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 271.03290\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 271.03290\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 271.03290\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 271.03290\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 271.03290\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 271.03290\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 271.03290\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss improved from 271.03290 to 270.75861, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 270.75861\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 270.75861\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 270.75861\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 270.75861\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 270.75861\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 270.75861\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 270.75861\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 270.75861\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss improved from 270.75861 to 269.20996, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss improved from 269.20996 to 263.74356, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 263.74356\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 263.74356\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 263.74356\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 263.74356\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 263.74356\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 263.74356\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 263.74356\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 263.74356\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 263.74356\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 263.74356\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 263.74356\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 263.74356\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 263.74356\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 263.74356\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 263.74356\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 263.74356\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 263.74356\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 263.74356\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 263.74356\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 263.74356\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 263.74356\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 263.74356\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 263.74356\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 263.74356\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 263.74356\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 263.74356\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 263.74356\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 263.74356\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 263.74356\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 263.74356\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 263.74356\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 263.74356\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 263.74356\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 263.74356\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 263.74356\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 263.74356\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 263.74356\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 263.74356\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 263.74356\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 263.74356\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 263.74356\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 263.74356\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss improved from 263.74356 to 257.15524, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 257.15524\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 257.15524\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 257.15524\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 257.15524\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 257.15524\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 257.15524\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 257.15524\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 257.15524\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss improved from 257.15524 to 254.66393, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss improved from 254.66393 to 253.27481, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 253.27481\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 253.27481\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 253.27481\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 253.27481\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 253.27481\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 253.27481\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 253.27481\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 253.27481\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss improved from 253.27481 to 252.99559, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss improved from 252.99559 to 251.34775, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 251.34775\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 251.34775\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 251.34775\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 251.34775\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 251.34775\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 251.34775\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 251.34775\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 251.34775\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 251.34775\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 251.34775\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 251.34775\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 251.34775\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 251.34775\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 251.34775\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 251.34775\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 251.34775\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 251.34775\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 251.34775\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 251.34775\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 251.34775\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 251.34775\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 251.34775\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 251.34775\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 251.34775\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 251.34775\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 251.34775\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 251.34775\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 251.34775\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 251.34775\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 251.34775\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 251.34775\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 251.34775\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 251.34775\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 251.34775\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 251.34775\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 251.34775\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 251.34775\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 251.34775\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 251.34775\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss improved from 251.34775 to 251.30688, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 251.30688\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 251.30688\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 251.30688\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 251.30688\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 251.30688\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 251.30688\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 251.30688\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 251.30688\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 251.30688\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 251.30688\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 251.30688\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 251.30688\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 251.30688\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 251.30688\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 251.30688\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 251.30688\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 251.30688\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 251.30688\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 251.30688\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss improved from 251.30688 to 250.07123, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 250.07123\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss improved from 250.07123 to 249.96588, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss improved from 249.96588 to 249.72601, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.72601\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss improved from 249.72601 to 249.29295, saving model to /users/qdb16186/dev/CV/1/1/Granulated/1DConv_mt_HGST/model_checkpoint/cp1.h5\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.29295\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.29295\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.29295\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.29295\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.29295\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.29295\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.29295\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.29295\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.29295\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.29295\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.29295\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.29295\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.29295\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.29295\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.29295\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.29295\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.29295\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.29295\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.29295\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.29295\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.29295\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.29295\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.29295\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.29295\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.29295\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.29295\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.29295\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.29295\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.29295\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.29295\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.29295\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.29295\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.29295\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.29295\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.29295\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.29295\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.29295\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.29295\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.29295\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.29295\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.29295\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.29295\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.29295\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.29295\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.29295\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.29295\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.29295\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.29295\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.29295\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.29295\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.29295\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.29295\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.29295\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.29295\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.29295\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.29295\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.29295\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.29295\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.29295\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.29295\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.29295\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.29295\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.29295\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.29295\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.29295\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.29295\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.29295\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.29295\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.29295\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.29295\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.29295\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.29295\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.29295\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.29295\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.29295\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.29295\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.29295\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.29295\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.29295\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.29295\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.29295\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 249.29295\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 249.29295\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 249.29295\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 249.29295\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 249.29295\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 249.29295\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 249.29295\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 249.29295\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 249.29295\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 249.29295\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 249.29295\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 249.29295\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 249.29295\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 249.29295\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 249.29295\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 249.29295\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 249.29295\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 249.29295\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 249.29295\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 249.29295\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 249.29295\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 249.29295\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 249.29295\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 249.29295\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 249.29295\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 249.29295\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 249.29295\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 249.29295\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 249.29295\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 249.29295\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 249.29295\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 249.29295\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 249.29295\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 249.29295\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 249.29295\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 249.29295\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 249.29295\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 249.29295\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 249.29295\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 249.29295\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 249.29295\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 249.29295\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 249.29295\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 249.29295\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 249.29295\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 249.29295\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 249.29295\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 249.29295\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 249.29295\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 249.29295\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 249.29295\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 249.29295\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 249.29295\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 249.29295\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 249.29295\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 249.29295\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 249.29295\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 249.29295\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 249.29295\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 249.29295\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 249.29295\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 249.29295\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 249.29295\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 249.29295\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 249.29295\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 249.29295\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 249.29295\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 249.29295\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 249.29295\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 249.29295\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 249.29295\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 249.29295\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 249.29295\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 249.29295\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 249.29295\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 249.29295\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 249.29295\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 249.29295\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 249.29295\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 249.29295\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 249.29295\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 249.29295\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 249.29295\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 249.29295\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 249.29295\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 249.29295\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 249.29295\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 249.29295\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 249.29295\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 249.29295\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 249.29295\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 249.29295\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 249.29295\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 249.29295\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 249.29295\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 249.29295\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 249.29295\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 249.29295\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 249.29295\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 249.29295\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 249.29295\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 249.29295\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 249.29295\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 249.29295\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 249.29295\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 249.29295\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 249.29295\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 249.29295\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 249.29295\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 249.29295\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 249.29295\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 249.29295\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 249.29295\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 249.29295\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 249.29295\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 249.29295\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 249.29295\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 249.29295\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 249.29295\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 249.29295\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 249.29295\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 249.29295\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 249.29295\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 249.29295\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 249.29295\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 249.29295\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 249.29295\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 249.29295\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 249.29295\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 249.29295\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 249.29295\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 249.29295\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 249.29295\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 249.29295\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 249.29295\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 249.29295\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 249.29295\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 249.29295\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 249.29295\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 249.29295\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 249.29295\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 249.29295\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 249.29295\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 249.29295\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 249.29295\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 249.29295\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 249.29295\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 249.29295\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 249.29295\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 249.29295\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 249.29295\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 249.29295\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 249.29295\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 249.29295\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 249.29295\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 249.29295\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 249.29295\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 249.29295\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 249.29295\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 249.29295\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 249.29295\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 249.29295\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 249.29295\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 249.29295\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 249.29295\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 249.29295\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 249.29295\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 249.29295\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 249.29295\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 249.29295\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 249.29295\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 249.29295\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 249.29295\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 249.29295\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 249.29295\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 249.29295\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 249.29295\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 249.29295\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 249.29295\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 249.29295\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 249.29295\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 249.29295\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 249.29295\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 249.29295\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 249.29295\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 249.29295\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 249.29295\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 249.29295\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 249.29295\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 249.29295\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 249.29295\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 249.29295\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 249.29295\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 249.29295\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 249.29295\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 249.29295\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 249.29295\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 249.29295\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 249.29295\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 249.29295\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 249.29295\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 249.29295\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 249.29295\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 249.29295\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 249.29295\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 249.29295\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 249.29295\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 249.29295\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 249.29295\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 249.29295\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 249.29295\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 249.29295\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 249.29295\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 249.29295\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 249.29295\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 249.29295\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 249.29295\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 249.29295\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 249.29295\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 249.29295\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 249.29295\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 249.29295\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 249.29295\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 249.29295\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 249.29295\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 249.29295\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 249.29295\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 249.29295\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 249.29295\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 249.29295\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 249.29295\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 249.29295\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 249.29295\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 249.29295\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 249.29295\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 249.29295\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 249.29295\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 249.29295\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 249.29295\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 249.29295\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 249.29295\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 249.29295\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 249.29295\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 249.29295\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 249.29295\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 249.29295\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 249.29295\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 249.29295\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 249.29295\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 249.29295\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 249.29295\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 249.29295\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 249.29295\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 249.29295\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 249.29295\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 249.29295\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 249.29295\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 249.29295\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 249.29295\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 249.29295\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 249.29295\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 249.29295\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 249.29295\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 249.29295\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 249.29295\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 249.29295\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 249.29295\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 249.29295\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 249.29295\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 249.29295\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 249.29295\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 249.29295\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 249.29295\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 249.29295\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 249.29295\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 249.29295\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 249.29295\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 249.29295\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 249.29295\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 249.29295\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 249.29295\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 249.29295\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 249.29295\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 249.29295\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 249.29295\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 249.29295\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 249.29295\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 249.29295\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 249.29295\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 249.29295\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 249.29295\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 249.29295\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 249.29295\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 249.29295\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 249.29295\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 249.29295\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 249.29295\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 249.29295\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 249.29295\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 249.29295\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 249.29295\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 249.29295\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 249.29295\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 249.29295\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 249.29295\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 249.29295\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 249.29295\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 249.29295\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 249.29295\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 249.29295\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 249.29295\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 249.29295\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 249.29295\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 249.29295\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 249.29295\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 249.29295\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 249.29295\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 249.29295\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 249.29295\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 249.29295\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 249.29295\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 249.29295\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 249.29295\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 249.29295\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 249.29295\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 249.29295\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 249.29295\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 249.29295\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 249.29295\n"
     ]
    }
   ],
   "source": [
    "#### CALL BACKS!\n",
    "es = EarlyStopping(monitor      = 'val_loss', \n",
    "                        mode     = 'min', \n",
    "                        verbose  = 1, \n",
    "                        patience = 1000,\n",
    "                    restore_best_weights = True)\n",
    "# CSV Logger\n",
    "csv_logger = CSVLogger(f'{csv_logger_path}/model_history1.csv' , append=True)\n",
    "\n",
    "# CP_callbacks      not required when using a tunner       \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{cp_callback_path}/cp1.h5',#ckpt',\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                save_best_only = True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# TensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_path, \n",
    "                                                       #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                      update_freq = 1,\n",
    "                                                      # histogram_freq=1, \n",
    "                                                      write_graph=False, \n",
    "                                                      write_images=False)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "\n",
    "# Covert to list and provide to Keras Regressor\n",
    "keras_callbacks = [es, csv_logger, cp_callback, tensorboard_callback]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialise model for one single resample/hyper param tuning (train cycle)\n",
    "# model2 = build_model()\n",
    "\n",
    "#     ADAPTIVE LEARNING RATE   \n",
    "    \n",
    "initial_learning_rate = 0.01\n",
    "decay_steps = 10.0\n",
    "decay_rate = 0.5\n",
    "learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "#     SETTING ADAM OPTIMISER\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "\n",
    "for layer in model2.layers:\n",
    "        layer.trainable=True\n",
    "\n",
    "#     COMPILE MODEl\n",
    "model2.compile(loss = \"mse\" , \n",
    "              optimizer = optimiser, \n",
    "              metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Fit convolutional layers on all properties and optimisied initial fingures.\n",
    "history = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                        epochs=600,\n",
    "                        batch_size=16, \n",
    "                        validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                        verbose = 3,\n",
    "                        callbacks=keras_callbacks)\n",
    "\n",
    "prior=model2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552b3e9-3e6e-411e-bf4e-b40d372d9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results_output.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"appended text/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "985744e4-e7dc-48ea-b8ca-0a2d63e1fae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.923, rmsd=6.861, mse=47.067, mae=5.229\n",
      "dS: r2=0.909, rmsd=19.943, mse=397.738, mae=15.514\n",
      "dG: r2=0.942, rmsd=1.056, mse=1.116, mae=0.755\n",
      "Tm: r2=0.882, rmsd=4.642, mse=21.548, mae=3.476\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "with open(\"Results_output_weights_freeze_compile.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"appended text\\n\")\n",
    "    for n in range(4):\n",
    "        r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "        print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "        myfile.write(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4067e07-ccb1-41f9-825e-7444c93279e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6702eb9c-de3b-47a6-bac5-89275e060182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_17\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "conv1d_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_1\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_1\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "conv1d_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_2\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_2\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "flatten\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "node1_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node3_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_1\n",
      "node4_1 <keras.layers.core.dense.Dense object at 0x14a6d983d850>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node1_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node3_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_2\n",
      "node4_2 <keras.layers.core.dense.Dense object at 0x14a6d8f1cdf0>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "dH\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dS\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dG\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Tm\n",
      "Tm <keras.layers.core.dense.Dense object at 0x14abc5920ca0>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 249.29295\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 249.29295\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 249.29295\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 249.29295\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 249.29295\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 249.29295\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 249.29295\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 249.29295\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 249.29295\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 249.29295\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 249.29295\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 249.29295\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 249.29295\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 249.29295\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 249.29295\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 249.29295\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 249.29295\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 249.29295\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 249.29295\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 249.29295\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 249.29295\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 249.29295\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 249.29295\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 249.29295\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 249.29295\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 249.29295\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 249.29295\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 249.29295\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 249.29295\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 249.29295\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 249.29295\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 249.29295\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 249.29295\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 249.29295\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 249.29295\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 249.29295\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 249.29295\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 249.29295\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 249.29295\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 249.29295\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 249.29295\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 249.29295\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 249.29295\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 249.29295\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 249.29295\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 249.29295\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 249.29295\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 249.29295\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 249.29295\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 249.29295\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 249.29295\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 249.29295\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 249.29295\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 249.29295\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 249.29295\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 249.29295\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 249.29295\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 249.29295\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 249.29295\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 249.29295\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 249.29295\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 249.29295\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 249.29295\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 249.29295\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 249.29295\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 249.29295\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 249.29295\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 249.29295\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 249.29295\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 249.29295\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 249.29295\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 249.29295\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 249.29295\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 249.29295\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 249.29295\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 249.29295\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 249.29295\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 249.29295\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 249.29295\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 249.29295\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 249.29295\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 249.29295\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 249.29295\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 249.29295\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 249.29295\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 249.29295\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 249.29295\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 249.29295\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 249.29295\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 249.29295\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 249.29295\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 249.29295\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 249.29295\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 249.29295\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 249.29295\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 249.29295\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 249.29295\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 249.29295\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 249.29295\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 249.29295\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 249.29295\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 249.29295\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 249.29295\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 249.29295\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 249.29295\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 249.29295\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 249.29295\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 249.29295\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 249.29295\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 249.29295\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 249.29295\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 249.29295\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 249.29295\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 249.29295\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 249.29295\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 249.29295\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 249.29295\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 249.29295\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 249.29295\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 249.29295\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 249.29295\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 249.29295\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 249.29295\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 249.29295\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 249.29295\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 249.29295\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 249.29295\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 249.29295\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 249.29295\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 249.29295\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 249.29295\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 249.29295\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 249.29295\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 249.29295\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 249.29295\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 249.29295\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 249.29295\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 249.29295\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 249.29295\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 249.29295\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 249.29295\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 249.29295\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 249.29295\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 249.29295\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 249.29295\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 249.29295\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 249.29295\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 249.29295\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 249.29295\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 249.29295\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 249.29295\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 249.29295\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 249.29295\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 249.29295\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 249.29295\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 249.29295\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 249.29295\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 249.29295\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 249.29295\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 249.29295\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 249.29295\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 249.29295\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 249.29295\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 249.29295\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 249.29295\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 249.29295\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 249.29295\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 249.29295\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 249.29295\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 249.29295\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 249.29295\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 249.29295\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 249.29295\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 249.29295\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 249.29295\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 249.29295\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 249.29295\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 249.29295\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 249.29295\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 249.29295\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 249.29295\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 249.29295\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 249.29295\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 249.29295\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 249.29295\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 249.29295\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 249.29295\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 249.29295\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.29295\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 249.29295\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.29295\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.29295\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.29295\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.29295\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.29295\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.29295\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.29295\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.29295\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.29295\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.29295\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.29295\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.29295\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.29295\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.29295\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.29295\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.29295\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.29295\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.29295\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.29295\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.29295\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.29295\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.29295\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.29295\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.29295\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.29295\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.29295\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.29295\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.29295\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.29295\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.29295\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.29295\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.29295\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.29295\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.29295\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.29295\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.29295\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.29295\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.29295\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.29295\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.29295\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.29295\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.29295\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.29295\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.29295\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.29295\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.29295\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.29295\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.29295\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.29295\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.29295\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.29295\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.29295\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.29295\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.29295\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.29295\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.29295\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.29295\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.29295\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.29295\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.29295\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.29295\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.29295\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.29295\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.29295\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.29295\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.29295\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.29295\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.29295\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.29295\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.29295\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.29295\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.29295\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.29295\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.29295\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.29295\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.29295\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.29295\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.29295\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.29295\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.29295\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.29295\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 249.29295\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 249.29295\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 249.29295\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 249.29295\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 249.29295\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 249.29295\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 249.29295\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 249.29295\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 249.29295\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 249.29295\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 249.29295\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 249.29295\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 249.29295\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 249.29295\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 249.29295\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 249.29295\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 249.29295\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 249.29295\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 249.29295\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 249.29295\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 249.29295\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 249.29295\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 249.29295\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 249.29295\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 249.29295\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 249.29295\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 249.29295\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 249.29295\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 249.29295\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 249.29295\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 249.29295\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 249.29295\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 249.29295\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 249.29295\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 249.29295\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 249.29295\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 249.29295\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 249.29295\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 249.29295\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 249.29295\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 249.29295\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 249.29295\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 249.29295\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 249.29295\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 249.29295\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 249.29295\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 249.29295\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 249.29295\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 249.29295\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 249.29295\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 249.29295\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 249.29295\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 249.29295\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 249.29295\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 249.29295\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 249.29295\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 249.29295\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 249.29295\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 249.29295\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 249.29295\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 249.29295\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 249.29295\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 249.29295\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 249.29295\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 249.29295\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 249.29295\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 249.29295\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 249.29295\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 249.29295\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 249.29295\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 249.29295\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 249.29295\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 249.29295\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 249.29295\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 249.29295\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 249.29295\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 249.29295\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 249.29295\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 249.29295\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 249.29295\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 249.29295\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 249.29295\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 249.29295\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 249.29295\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 249.29295\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 249.29295\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 249.29295\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 249.29295\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 249.29295\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 249.29295\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 249.29295\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 249.29295\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 249.29295\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 249.29295\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 249.29295\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 249.29295\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 249.29295\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 249.29295\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 249.29295\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 249.29295\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 249.29295\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 249.29295\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 249.29295\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 249.29295\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 249.29295\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 249.29295\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 249.29295\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 249.29295\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 249.29295\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 249.29295\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 249.29295\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 249.29295\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 249.29295\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 249.29295\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 249.29295\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 249.29295\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 249.29295\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 249.29295\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 249.29295\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 249.29295\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 249.29295\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 249.29295\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 249.29295\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 249.29295\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 249.29295\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 249.29295\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 249.29295\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 249.29295\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 249.29295\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 249.29295\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 249.29295\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 249.29295\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 249.29295\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 249.29295\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 249.29295\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 249.29295\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 249.29295\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 249.29295\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 249.29295\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 249.29295\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 249.29295\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 249.29295\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 249.29295\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 249.29295\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 249.29295\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 249.29295\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 249.29295\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 249.29295\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 249.29295\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 249.29295\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 249.29295\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 249.29295\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 249.29295\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 249.29295\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 249.29295\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 249.29295\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 249.29295\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 249.29295\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 249.29295\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 249.29295\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 249.29295\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 249.29295\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 249.29295\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 249.29295\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 249.29295\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 249.29295\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 249.29295\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 249.29295\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 249.29295\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 249.29295\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 249.29295\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 249.29295\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 249.29295\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 249.29295\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 249.29295\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 249.29295\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 249.29295\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 249.29295\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 249.29295\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 249.29295\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 249.29295\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 249.29295\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 249.29295\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 249.29295\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 249.29295\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 249.29295\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 249.29295\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 249.29295\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 249.29295\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 249.29295\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 249.29295\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 249.29295\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 249.29295\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 249.29295\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 249.29295\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 249.29295\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 249.29295\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 249.29295\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 249.29295\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 249.29295\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 249.29295\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 249.29295\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 249.29295\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 249.29295\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 249.29295\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 249.29295\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 249.29295\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 249.29295\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 249.29295\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 249.29295\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 249.29295\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 249.29295\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 249.29295\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 249.29295\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 249.29295\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 249.29295\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 249.29295\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 249.29295\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 249.29295\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 249.29295\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 249.29295\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 249.29295\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 249.29295\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 249.29295\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 249.29295\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 249.29295\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 249.29295\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 249.29295\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 249.29295\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 249.29295\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 249.29295\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 249.29295\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 249.29295\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 249.29295\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 249.29295\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 249.29295\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 249.29295\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 249.29295\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 249.29295\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 249.29295\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 249.29295\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 249.29295\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 249.29295\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 249.29295\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 249.29295\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 249.29295\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 249.29295\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 249.29295\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 249.29295\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 249.29295\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 249.29295\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 249.29295\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 249.29295\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 249.29295\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 249.29295\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 249.29295\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 249.29295\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 249.29295\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 249.29295\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 249.29295\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 249.29295\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 249.29295\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 249.29295\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 249.29295\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 249.29295\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 249.29295\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 249.29295\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 249.29295\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 249.29295\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 249.29295\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 249.29295\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 249.29295\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 249.29295\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 249.29295\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 249.29295\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 249.29295\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 249.29295\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 249.29295\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 249.29295\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 249.29295\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 249.29295\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 249.29295\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 249.29295\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 249.29295\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 249.29295\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 249.29295\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 249.29295\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 249.29295\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 249.29295\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 249.29295\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 249.29295\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 249.29295\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 249.29295\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 249.29295\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 249.29295\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 249.29295\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 249.29295\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 249.29295\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 249.29295\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 249.29295\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 249.29295\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 249.29295\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 249.29295\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 249.29295\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 249.29295\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 249.29295\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 249.29295\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 249.29295\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 249.29295\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 249.29295\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 249.29295\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 249.29295\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 249.29295\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 249.29295\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 249.29295\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 249.29295\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 249.29295\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 249.29295\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 249.29295\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 249.29295\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 249.29295\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 249.29295\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 249.29295\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 249.29295\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 249.29295\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 249.29295\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 249.29295\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 249.29295\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 249.29295\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "dH: r2=0.930, rmsd=6.539, mse=42.759, mae=5.083\n",
      "dS: r2=0.916, rmsd=19.093, mse=364.556, mae=15.125\n",
      "dG: r2=0.952, rmsd=0.967, mse=0.936, mae=0.697\n",
      "Tm: r2=0.901, rmsd=4.263, mse=18.173, mae=3.169\n",
      "input_17\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "conv1d_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_1\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_1\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "conv1d_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_2\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_2\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "flatten\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "node1_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node3_1\n",
      "node3_1 <keras.layers.core.dense.Dense object at 0x14a8383376d0>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node4_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node1_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node3_2\n",
      "node3_2 <keras.layers.core.dense.Dense object at 0x14a820f3f670>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node4_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dH\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dS\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dG\n",
      "dG <keras.layers.core.dense.Dense object at 0x14a82194b100>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "Tm\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 249.29295\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 249.29295\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 249.29295\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 249.29295\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 249.29295\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 249.29295\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 249.29295\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 249.29295\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 249.29295\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 249.29295\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 249.29295\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 249.29295\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 249.29295\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 249.29295\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 249.29295\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 249.29295\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 249.29295\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 249.29295\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 249.29295\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 249.29295\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 249.29295\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 249.29295\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 249.29295\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 249.29295\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 249.29295\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 249.29295\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 249.29295\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 249.29295\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 249.29295\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 249.29295\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 249.29295\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 249.29295\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 249.29295\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 249.29295\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 249.29295\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 249.29295\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 249.29295\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 249.29295\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 249.29295\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 249.29295\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 249.29295\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 249.29295\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 249.29295\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 249.29295\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 249.29295\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 249.29295\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 249.29295\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 249.29295\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 249.29295\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 249.29295\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 249.29295\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 249.29295\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 249.29295\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 249.29295\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 249.29295\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 249.29295\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 249.29295\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 249.29295\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 249.29295\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 249.29295\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 249.29295\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 249.29295\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 249.29295\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 249.29295\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 249.29295\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 249.29295\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 249.29295\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 249.29295\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 249.29295\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 249.29295\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 249.29295\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 249.29295\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 249.29295\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 249.29295\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 249.29295\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 249.29295\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 249.29295\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 249.29295\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 249.29295\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 249.29295\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 249.29295\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 249.29295\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 249.29295\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 249.29295\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 249.29295\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 249.29295\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 249.29295\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 249.29295\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 249.29295\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 249.29295\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 249.29295\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 249.29295\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 249.29295\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 249.29295\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 249.29295\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 249.29295\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 249.29295\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 249.29295\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 249.29295\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 249.29295\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 249.29295\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 249.29295\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 249.29295\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 249.29295\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 249.29295\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 249.29295\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 249.29295\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 249.29295\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 249.29295\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 249.29295\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 249.29295\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 249.29295\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 249.29295\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 249.29295\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 249.29295\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 249.29295\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 249.29295\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 249.29295\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 249.29295\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 249.29295\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 249.29295\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 249.29295\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 249.29295\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 249.29295\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 249.29295\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 249.29295\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 249.29295\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 249.29295\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 249.29295\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 249.29295\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 249.29295\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 249.29295\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 249.29295\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 249.29295\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 249.29295\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 249.29295\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 249.29295\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 249.29295\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 249.29295\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 249.29295\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 249.29295\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 249.29295\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 249.29295\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 249.29295\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 249.29295\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 249.29295\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 249.29295\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 249.29295\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 249.29295\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 249.29295\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 249.29295\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 249.29295\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 249.29295\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 249.29295\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 249.29295\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 249.29295\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 249.29295\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 249.29295\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 249.29295\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 249.29295\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 249.29295\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 249.29295\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 249.29295\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 249.29295\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 249.29295\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 249.29295\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 249.29295\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 249.29295\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 249.29295\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 249.29295\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 249.29295\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 249.29295\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 249.29295\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 249.29295\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 249.29295\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 249.29295\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 249.29295\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 249.29295\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 249.29295\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 249.29295\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 249.29295\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 249.29295\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 249.29295\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 249.29295\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 249.29295\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 249.29295\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 249.29295\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 249.29295\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.29295\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 249.29295\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.29295\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.29295\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.29295\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.29295\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.29295\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.29295\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.29295\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.29295\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.29295\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.29295\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.29295\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.29295\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.29295\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.29295\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.29295\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.29295\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.29295\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.29295\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.29295\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.29295\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.29295\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.29295\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.29295\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.29295\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.29295\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.29295\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.29295\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.29295\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.29295\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.29295\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.29295\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.29295\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.29295\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.29295\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.29295\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.29295\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.29295\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.29295\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.29295\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.29295\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.29295\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.29295\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.29295\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.29295\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.29295\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.29295\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.29295\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.29295\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.29295\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.29295\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.29295\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.29295\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.29295\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.29295\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.29295\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.29295\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.29295\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.29295\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.29295\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.29295\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.29295\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.29295\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.29295\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.29295\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.29295\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.29295\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.29295\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.29295\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.29295\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.29295\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.29295\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.29295\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.29295\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.29295\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.29295\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.29295\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.29295\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.29295\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.29295\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.29295\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.29295\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 249.29295\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 249.29295\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 249.29295\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 249.29295\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 249.29295\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 249.29295\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 249.29295\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 249.29295\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 249.29295\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 249.29295\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 249.29295\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 249.29295\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 249.29295\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 249.29295\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 249.29295\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 249.29295\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 249.29295\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 249.29295\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 249.29295\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 249.29295\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 249.29295\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 249.29295\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 249.29295\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 249.29295\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 249.29295\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 249.29295\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 249.29295\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 249.29295\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 249.29295\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 249.29295\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 249.29295\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 249.29295\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 249.29295\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 249.29295\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 249.29295\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 249.29295\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 249.29295\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 249.29295\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 249.29295\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 249.29295\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 249.29295\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 249.29295\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 249.29295\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 249.29295\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 249.29295\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 249.29295\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 249.29295\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 249.29295\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 249.29295\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 249.29295\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 249.29295\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 249.29295\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 249.29295\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 249.29295\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 249.29295\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 249.29295\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 249.29295\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 249.29295\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 249.29295\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 249.29295\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 249.29295\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 249.29295\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 249.29295\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 249.29295\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 249.29295\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 249.29295\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 249.29295\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 249.29295\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 249.29295\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 249.29295\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 249.29295\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 249.29295\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 249.29295\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 249.29295\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 249.29295\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 249.29295\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 249.29295\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 249.29295\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 249.29295\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 249.29295\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 249.29295\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 249.29295\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 249.29295\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 249.29295\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 249.29295\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 249.29295\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 249.29295\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 249.29295\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 249.29295\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 249.29295\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 249.29295\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 249.29295\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 249.29295\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 249.29295\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 249.29295\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 249.29295\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 249.29295\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 249.29295\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 249.29295\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 249.29295\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 249.29295\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 249.29295\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 249.29295\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 249.29295\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 249.29295\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 249.29295\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 249.29295\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 249.29295\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 249.29295\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 249.29295\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 249.29295\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 249.29295\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 249.29295\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 249.29295\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 249.29295\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 249.29295\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 249.29295\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 249.29295\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 249.29295\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 249.29295\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 249.29295\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 249.29295\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 249.29295\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 249.29295\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 249.29295\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 249.29295\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 249.29295\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 249.29295\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 249.29295\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 249.29295\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 249.29295\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 249.29295\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 249.29295\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 249.29295\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 249.29295\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 249.29295\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 249.29295\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 249.29295\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 249.29295\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 249.29295\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 249.29295\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 249.29295\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 249.29295\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 249.29295\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 249.29295\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 249.29295\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 249.29295\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 249.29295\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 249.29295\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 249.29295\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 249.29295\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 249.29295\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 249.29295\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 249.29295\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 249.29295\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 249.29295\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 249.29295\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 249.29295\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 249.29295\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 249.29295\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 249.29295\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 249.29295\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 249.29295\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 249.29295\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 249.29295\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 249.29295\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 249.29295\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 249.29295\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 249.29295\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 249.29295\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 249.29295\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 249.29295\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 249.29295\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 249.29295\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 249.29295\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 249.29295\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 249.29295\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 249.29295\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 249.29295\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 249.29295\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 249.29295\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 249.29295\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 249.29295\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 249.29295\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 249.29295\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 249.29295\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 249.29295\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 249.29295\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 249.29295\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 249.29295\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 249.29295\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 249.29295\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 249.29295\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 249.29295\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 249.29295\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 249.29295\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 249.29295\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 249.29295\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 249.29295\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 249.29295\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 249.29295\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 249.29295\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 249.29295\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 249.29295\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 249.29295\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 249.29295\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 249.29295\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 249.29295\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 249.29295\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 249.29295\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 249.29295\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 249.29295\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 249.29295\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 249.29295\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 249.29295\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 249.29295\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 249.29295\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 249.29295\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 249.29295\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 249.29295\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 249.29295\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 249.29295\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 249.29295\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 249.29295\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 249.29295\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 249.29295\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 249.29295\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 249.29295\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 249.29295\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 249.29295\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 249.29295\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 249.29295\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 249.29295\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 249.29295\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 249.29295\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 249.29295\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 249.29295\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 249.29295\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 249.29295\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 249.29295\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 249.29295\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 249.29295\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 249.29295\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 249.29295\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 249.29295\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 249.29295\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 249.29295\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 249.29295\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 249.29295\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 249.29295\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 249.29295\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 249.29295\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 249.29295\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 249.29295\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 249.29295\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 249.29295\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 249.29295\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 249.29295\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 249.29295\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 249.29295\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 249.29295\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 249.29295\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 249.29295\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 249.29295\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 249.29295\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 249.29295\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 249.29295\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 249.29295\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 249.29295\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 249.29295\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 249.29295\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 249.29295\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 249.29295\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 249.29295\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 249.29295\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 249.29295\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 249.29295\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 249.29295\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 249.29295\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 249.29295\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 249.29295\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 249.29295\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 249.29295\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 249.29295\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 249.29295\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 249.29295\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 249.29295\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 249.29295\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 249.29295\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 249.29295\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 249.29295\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 249.29295\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 249.29295\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 249.29295\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 249.29295\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 249.29295\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 249.29295\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 249.29295\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 249.29295\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 249.29295\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 249.29295\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 249.29295\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 249.29295\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 249.29295\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 249.29295\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 249.29295\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 249.29295\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 249.29295\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 249.29295\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 249.29295\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 249.29295\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 249.29295\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 249.29295\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 249.29295\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 249.29295\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 249.29295\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 249.29295\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 249.29295\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 249.29295\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 249.29295\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 249.29295\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 249.29295\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 249.29295\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 249.29295\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 249.29295\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 249.29295\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 249.29295\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 249.29295\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 249.29295\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "dH: r2=0.930, rmsd=6.539, mse=42.759, mae=5.083\n",
      "dS: r2=0.916, rmsd=19.093, mse=364.556, mae=15.125\n",
      "dG: r2=0.952, rmsd=0.962, mse=0.926, mae=0.691\n",
      "Tm: r2=0.901, rmsd=4.263, mse=18.173, mae=3.169\n",
      "input_17\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "conv1d_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_1\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_1\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "conv1d_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_2\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_2\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "flatten\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "node1_1\n",
      "node1_1 <keras.layers.core.dense.Dense object at 0x14a6db957940>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node2_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node3_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node1_2\n",
      "node1_2 <keras.layers.core.dense.Dense object at 0x14a6d80d0340>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node2_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node3_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dH\n",
      "dH <keras.layers.core.dense.Dense object at 0x14a821a4cf40>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "dS\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dG\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Tm\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 249.29295\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 249.29295\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 249.29295\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 249.29295\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 249.29295\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 249.29295\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 249.29295\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 249.29295\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 249.29295\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 249.29295\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 249.29295\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 249.29295\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 249.29295\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 249.29295\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 249.29295\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 249.29295\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 249.29295\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 249.29295\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 249.29295\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 249.29295\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 249.29295\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 249.29295\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 249.29295\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 249.29295\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 249.29295\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 249.29295\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 249.29295\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 249.29295\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 249.29295\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 249.29295\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 249.29295\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 249.29295\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 249.29295\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 249.29295\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 249.29295\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 249.29295\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 249.29295\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 249.29295\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 249.29295\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 249.29295\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 249.29295\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 249.29295\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 249.29295\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 249.29295\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 249.29295\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 249.29295\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 249.29295\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 249.29295\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 249.29295\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 249.29295\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 249.29295\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 249.29295\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 249.29295\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 249.29295\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 249.29295\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 249.29295\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 249.29295\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 249.29295\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 249.29295\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 249.29295\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 249.29295\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 249.29295\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 249.29295\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 249.29295\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 249.29295\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 249.29295\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 249.29295\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 249.29295\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 249.29295\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 249.29295\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 249.29295\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 249.29295\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 249.29295\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 249.29295\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 249.29295\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 249.29295\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 249.29295\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 249.29295\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 249.29295\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 249.29295\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 249.29295\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 249.29295\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 249.29295\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 249.29295\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 249.29295\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 249.29295\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 249.29295\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 249.29295\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 249.29295\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 249.29295\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 249.29295\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 249.29295\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 249.29295\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 249.29295\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 249.29295\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 249.29295\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 249.29295\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 249.29295\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 249.29295\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 249.29295\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 249.29295\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 249.29295\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 249.29295\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 249.29295\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 249.29295\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 249.29295\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 249.29295\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 249.29295\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 249.29295\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 249.29295\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 249.29295\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 249.29295\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 249.29295\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 249.29295\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 249.29295\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 249.29295\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 249.29295\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 249.29295\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 249.29295\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 249.29295\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 249.29295\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 249.29295\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 249.29295\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 249.29295\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 249.29295\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 249.29295\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 249.29295\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 249.29295\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 249.29295\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 249.29295\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 249.29295\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 249.29295\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 249.29295\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 249.29295\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 249.29295\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 249.29295\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 249.29295\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 249.29295\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 249.29295\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 249.29295\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 249.29295\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 249.29295\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 249.29295\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 249.29295\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 249.29295\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 249.29295\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 249.29295\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 249.29295\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 249.29295\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 249.29295\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 249.29295\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 249.29295\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 249.29295\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 249.29295\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 249.29295\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 249.29295\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 249.29295\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 249.29295\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 249.29295\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 249.29295\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 249.29295\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 249.29295\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 249.29295\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 249.29295\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 249.29295\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 249.29295\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 249.29295\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 249.29295\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 249.29295\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 249.29295\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 249.29295\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 249.29295\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 249.29295\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 249.29295\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 249.29295\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 249.29295\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 249.29295\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 249.29295\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 249.29295\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 249.29295\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 249.29295\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 249.29295\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 249.29295\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 249.29295\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 249.29295\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 249.29295\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 249.29295\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 249.29295\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.29295\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 249.29295\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.29295\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.29295\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.29295\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.29295\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.29295\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.29295\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.29295\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.29295\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.29295\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.29295\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.29295\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.29295\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.29295\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.29295\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.29295\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.29295\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.29295\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.29295\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.29295\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.29295\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.29295\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.29295\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.29295\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.29295\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.29295\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.29295\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.29295\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.29295\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.29295\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.29295\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.29295\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.29295\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.29295\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.29295\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.29295\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.29295\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.29295\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.29295\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.29295\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.29295\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.29295\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.29295\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.29295\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.29295\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.29295\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.29295\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.29295\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.29295\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.29295\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.29295\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.29295\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.29295\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.29295\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.29295\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.29295\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.29295\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.29295\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.29295\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.29295\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.29295\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.29295\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.29295\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.29295\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.29295\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.29295\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.29295\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.29295\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.29295\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.29295\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.29295\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.29295\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.29295\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.29295\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.29295\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.29295\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.29295\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.29295\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.29295\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.29295\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.29295\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.29295\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 249.29295\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 249.29295\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 249.29295\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 249.29295\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 249.29295\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 249.29295\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 249.29295\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 249.29295\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 249.29295\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 249.29295\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 249.29295\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 249.29295\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 249.29295\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 249.29295\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 249.29295\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 249.29295\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 249.29295\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 249.29295\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 249.29295\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 249.29295\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 249.29295\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 249.29295\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 249.29295\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 249.29295\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 249.29295\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 249.29295\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 249.29295\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 249.29295\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 249.29295\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 249.29295\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 249.29295\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 249.29295\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 249.29295\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 249.29295\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 249.29295\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 249.29295\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 249.29295\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 249.29295\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 249.29295\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 249.29295\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 249.29295\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 249.29295\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 249.29295\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 249.29295\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 249.29295\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 249.29295\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 249.29295\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 249.29295\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 249.29295\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 249.29295\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 249.29295\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 249.29295\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 249.29295\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 249.29295\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 249.29295\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 249.29295\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 249.29295\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 249.29295\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 249.29295\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 249.29295\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 249.29295\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 249.29295\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 249.29295\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 249.29295\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 249.29295\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 249.29295\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 249.29295\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 249.29295\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 249.29295\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 249.29295\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 249.29295\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 249.29295\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 249.29295\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 249.29295\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 249.29295\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 249.29295\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 249.29295\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 249.29295\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 249.29295\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 249.29295\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 249.29295\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 249.29295\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 249.29295\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 249.29295\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 249.29295\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 249.29295\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 249.29295\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 249.29295\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 249.29295\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 249.29295\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 249.29295\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 249.29295\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 249.29295\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 249.29295\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 249.29295\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 249.29295\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 249.29295\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 249.29295\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 249.29295\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 249.29295\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 249.29295\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 249.29295\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 249.29295\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 249.29295\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 249.29295\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 249.29295\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 249.29295\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 249.29295\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 249.29295\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 249.29295\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 249.29295\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 249.29295\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 249.29295\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 249.29295\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 249.29295\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 249.29295\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 249.29295\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 249.29295\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 249.29295\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 249.29295\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 249.29295\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 249.29295\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 249.29295\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 249.29295\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 249.29295\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 249.29295\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 249.29295\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 249.29295\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 249.29295\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 249.29295\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 249.29295\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 249.29295\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 249.29295\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 249.29295\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 249.29295\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 249.29295\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 249.29295\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 249.29295\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 249.29295\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 249.29295\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 249.29295\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 249.29295\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 249.29295\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 249.29295\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 249.29295\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 249.29295\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 249.29295\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 249.29295\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 249.29295\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 249.29295\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 249.29295\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 249.29295\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 249.29295\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 249.29295\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 249.29295\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 249.29295\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 249.29295\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 249.29295\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 249.29295\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 249.29295\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 249.29295\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 249.29295\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 249.29295\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 249.29295\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 249.29295\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 249.29295\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 249.29295\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 249.29295\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 249.29295\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 249.29295\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 249.29295\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 249.29295\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 249.29295\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 249.29295\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 249.29295\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 249.29295\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 249.29295\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 249.29295\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 249.29295\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 249.29295\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 249.29295\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 249.29295\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 249.29295\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 249.29295\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 249.29295\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 249.29295\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 249.29295\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 249.29295\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 249.29295\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 249.29295\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 249.29295\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 249.29295\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 249.29295\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 249.29295\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 249.29295\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 249.29295\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 249.29295\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 249.29295\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 249.29295\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 249.29295\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 249.29295\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 249.29295\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 249.29295\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 249.29295\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 249.29295\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 249.29295\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 249.29295\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 249.29295\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 249.29295\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 249.29295\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 249.29295\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 249.29295\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 249.29295\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 249.29295\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 249.29295\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 249.29295\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 249.29295\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 249.29295\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 249.29295\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 249.29295\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 249.29295\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 249.29295\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 249.29295\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 249.29295\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 249.29295\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 249.29295\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 249.29295\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 249.29295\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 249.29295\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 249.29295\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 249.29295\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 249.29295\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 249.29295\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 249.29295\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 249.29295\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 249.29295\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 249.29295\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 249.29295\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 249.29295\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 249.29295\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 249.29295\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 249.29295\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 249.29295\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 249.29295\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 249.29295\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 249.29295\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 249.29295\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 249.29295\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 249.29295\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 249.29295\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 249.29295\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 249.29295\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 249.29295\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 249.29295\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 249.29295\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 249.29295\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 249.29295\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 249.29295\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 249.29295\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 249.29295\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 249.29295\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 249.29295\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 249.29295\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 249.29295\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 249.29295\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 249.29295\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 249.29295\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 249.29295\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 249.29295\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 249.29295\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 249.29295\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 249.29295\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 249.29295\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 249.29295\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 249.29295\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 249.29295\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 249.29295\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 249.29295\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 249.29295\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 249.29295\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 249.29295\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 249.29295\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 249.29295\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 249.29295\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 249.29295\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 249.29295\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 249.29295\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 249.29295\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 249.29295\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 249.29295\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 249.29295\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 249.29295\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 249.29295\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 249.29295\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 249.29295\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 249.29295\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 249.29295\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 249.29295\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 249.29295\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 249.29295\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 249.29295\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 249.29295\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 249.29295\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 249.29295\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 249.29295\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 249.29295\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 249.29295\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 249.29295\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 249.29295\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 249.29295\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 249.29295\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 249.29295\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 249.29295\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 249.29295\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 249.29295\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 249.29295\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 249.29295\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 249.29295\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 249.29295\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 249.29295\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 249.29295\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 249.29295\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 249.29295\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 249.29295\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 249.29295\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 249.29295\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 249.29295\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 249.29295\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 249.29295\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "dH: r2=0.929, rmsd=6.552, mse=42.923, mae=5.087\n",
      "dS: r2=0.916, rmsd=19.093, mse=364.556, mae=15.125\n",
      "dG: r2=0.952, rmsd=0.962, mse=0.926, mae=0.691\n",
      "Tm: r2=0.901, rmsd=4.263, mse=18.173, mae=3.169\n",
      "input_17\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "conv1d_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_1\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_1\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "conv1d_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "maxpooling_2\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "batchnorm_2\n",
      "weights: 4\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 4\n",
      "flatten\n",
      "weights: 0\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 0\n",
      "node1_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_1\n",
      "node2_1 <keras.layers.core.dense.Dense object at 0x14a6d924aac0>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node3_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_1\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node1_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node2_2\n",
      "node2_2 <keras.layers.core.dense.Dense object at 0x14a838337880>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "node3_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "node4_2\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dH\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "dS\n",
      "dS <keras.layers.core.dense.Dense object at 0x14a81816df70>\n",
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n",
      "dG\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Tm\n",
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n",
      "Epoch 1/600\n",
      "\n",
      "Epoch 1: val_loss did not improve from 249.29295\n",
      "Epoch 2/600\n",
      "\n",
      "Epoch 2: val_loss did not improve from 249.29295\n",
      "Epoch 3/600\n",
      "\n",
      "Epoch 3: val_loss did not improve from 249.29295\n",
      "Epoch 4/600\n",
      "\n",
      "Epoch 4: val_loss did not improve from 249.29295\n",
      "Epoch 5/600\n",
      "\n",
      "Epoch 5: val_loss did not improve from 249.29295\n",
      "Epoch 6/600\n",
      "\n",
      "Epoch 6: val_loss did not improve from 249.29295\n",
      "Epoch 7/600\n",
      "\n",
      "Epoch 7: val_loss did not improve from 249.29295\n",
      "Epoch 8/600\n",
      "\n",
      "Epoch 8: val_loss did not improve from 249.29295\n",
      "Epoch 9/600\n",
      "\n",
      "Epoch 9: val_loss did not improve from 249.29295\n",
      "Epoch 10/600\n",
      "\n",
      "Epoch 10: val_loss did not improve from 249.29295\n",
      "Epoch 11/600\n",
      "\n",
      "Epoch 11: val_loss did not improve from 249.29295\n",
      "Epoch 12/600\n",
      "\n",
      "Epoch 12: val_loss did not improve from 249.29295\n",
      "Epoch 13/600\n",
      "\n",
      "Epoch 13: val_loss did not improve from 249.29295\n",
      "Epoch 14/600\n",
      "\n",
      "Epoch 14: val_loss did not improve from 249.29295\n",
      "Epoch 15/600\n",
      "\n",
      "Epoch 15: val_loss did not improve from 249.29295\n",
      "Epoch 16/600\n",
      "\n",
      "Epoch 16: val_loss did not improve from 249.29295\n",
      "Epoch 17/600\n",
      "\n",
      "Epoch 17: val_loss did not improve from 249.29295\n",
      "Epoch 18/600\n",
      "\n",
      "Epoch 18: val_loss did not improve from 249.29295\n",
      "Epoch 19/600\n",
      "\n",
      "Epoch 19: val_loss did not improve from 249.29295\n",
      "Epoch 20/600\n",
      "\n",
      "Epoch 20: val_loss did not improve from 249.29295\n",
      "Epoch 21/600\n",
      "\n",
      "Epoch 21: val_loss did not improve from 249.29295\n",
      "Epoch 22/600\n",
      "\n",
      "Epoch 22: val_loss did not improve from 249.29295\n",
      "Epoch 23/600\n",
      "\n",
      "Epoch 23: val_loss did not improve from 249.29295\n",
      "Epoch 24/600\n",
      "\n",
      "Epoch 24: val_loss did not improve from 249.29295\n",
      "Epoch 25/600\n",
      "\n",
      "Epoch 25: val_loss did not improve from 249.29295\n",
      "Epoch 26/600\n",
      "\n",
      "Epoch 26: val_loss did not improve from 249.29295\n",
      "Epoch 27/600\n",
      "\n",
      "Epoch 27: val_loss did not improve from 249.29295\n",
      "Epoch 28/600\n",
      "\n",
      "Epoch 28: val_loss did not improve from 249.29295\n",
      "Epoch 29/600\n",
      "\n",
      "Epoch 29: val_loss did not improve from 249.29295\n",
      "Epoch 30/600\n",
      "\n",
      "Epoch 30: val_loss did not improve from 249.29295\n",
      "Epoch 31/600\n",
      "\n",
      "Epoch 31: val_loss did not improve from 249.29295\n",
      "Epoch 32/600\n",
      "\n",
      "Epoch 32: val_loss did not improve from 249.29295\n",
      "Epoch 33/600\n",
      "\n",
      "Epoch 33: val_loss did not improve from 249.29295\n",
      "Epoch 34/600\n",
      "\n",
      "Epoch 34: val_loss did not improve from 249.29295\n",
      "Epoch 35/600\n",
      "\n",
      "Epoch 35: val_loss did not improve from 249.29295\n",
      "Epoch 36/600\n",
      "\n",
      "Epoch 36: val_loss did not improve from 249.29295\n",
      "Epoch 37/600\n",
      "\n",
      "Epoch 37: val_loss did not improve from 249.29295\n",
      "Epoch 38/600\n",
      "\n",
      "Epoch 38: val_loss did not improve from 249.29295\n",
      "Epoch 39/600\n",
      "\n",
      "Epoch 39: val_loss did not improve from 249.29295\n",
      "Epoch 40/600\n",
      "\n",
      "Epoch 40: val_loss did not improve from 249.29295\n",
      "Epoch 41/600\n",
      "\n",
      "Epoch 41: val_loss did not improve from 249.29295\n",
      "Epoch 42/600\n",
      "\n",
      "Epoch 42: val_loss did not improve from 249.29295\n",
      "Epoch 43/600\n",
      "\n",
      "Epoch 43: val_loss did not improve from 249.29295\n",
      "Epoch 44/600\n",
      "\n",
      "Epoch 44: val_loss did not improve from 249.29295\n",
      "Epoch 45/600\n",
      "\n",
      "Epoch 45: val_loss did not improve from 249.29295\n",
      "Epoch 46/600\n",
      "\n",
      "Epoch 46: val_loss did not improve from 249.29295\n",
      "Epoch 47/600\n",
      "\n",
      "Epoch 47: val_loss did not improve from 249.29295\n",
      "Epoch 48/600\n",
      "\n",
      "Epoch 48: val_loss did not improve from 249.29295\n",
      "Epoch 49/600\n",
      "\n",
      "Epoch 49: val_loss did not improve from 249.29295\n",
      "Epoch 50/600\n",
      "\n",
      "Epoch 50: val_loss did not improve from 249.29295\n",
      "Epoch 51/600\n",
      "\n",
      "Epoch 51: val_loss did not improve from 249.29295\n",
      "Epoch 52/600\n",
      "\n",
      "Epoch 52: val_loss did not improve from 249.29295\n",
      "Epoch 53/600\n",
      "\n",
      "Epoch 53: val_loss did not improve from 249.29295\n",
      "Epoch 54/600\n",
      "\n",
      "Epoch 54: val_loss did not improve from 249.29295\n",
      "Epoch 55/600\n",
      "\n",
      "Epoch 55: val_loss did not improve from 249.29295\n",
      "Epoch 56/600\n",
      "\n",
      "Epoch 56: val_loss did not improve from 249.29295\n",
      "Epoch 57/600\n",
      "\n",
      "Epoch 57: val_loss did not improve from 249.29295\n",
      "Epoch 58/600\n",
      "\n",
      "Epoch 58: val_loss did not improve from 249.29295\n",
      "Epoch 59/600\n",
      "\n",
      "Epoch 59: val_loss did not improve from 249.29295\n",
      "Epoch 60/600\n",
      "\n",
      "Epoch 60: val_loss did not improve from 249.29295\n",
      "Epoch 61/600\n",
      "\n",
      "Epoch 61: val_loss did not improve from 249.29295\n",
      "Epoch 62/600\n",
      "\n",
      "Epoch 62: val_loss did not improve from 249.29295\n",
      "Epoch 63/600\n",
      "\n",
      "Epoch 63: val_loss did not improve from 249.29295\n",
      "Epoch 64/600\n",
      "\n",
      "Epoch 64: val_loss did not improve from 249.29295\n",
      "Epoch 65/600\n",
      "\n",
      "Epoch 65: val_loss did not improve from 249.29295\n",
      "Epoch 66/600\n",
      "\n",
      "Epoch 66: val_loss did not improve from 249.29295\n",
      "Epoch 67/600\n",
      "\n",
      "Epoch 67: val_loss did not improve from 249.29295\n",
      "Epoch 68/600\n",
      "\n",
      "Epoch 68: val_loss did not improve from 249.29295\n",
      "Epoch 69/600\n",
      "\n",
      "Epoch 69: val_loss did not improve from 249.29295\n",
      "Epoch 70/600\n",
      "\n",
      "Epoch 70: val_loss did not improve from 249.29295\n",
      "Epoch 71/600\n",
      "\n",
      "Epoch 71: val_loss did not improve from 249.29295\n",
      "Epoch 72/600\n",
      "\n",
      "Epoch 72: val_loss did not improve from 249.29295\n",
      "Epoch 73/600\n",
      "\n",
      "Epoch 73: val_loss did not improve from 249.29295\n",
      "Epoch 74/600\n",
      "\n",
      "Epoch 74: val_loss did not improve from 249.29295\n",
      "Epoch 75/600\n",
      "\n",
      "Epoch 75: val_loss did not improve from 249.29295\n",
      "Epoch 76/600\n",
      "\n",
      "Epoch 76: val_loss did not improve from 249.29295\n",
      "Epoch 77/600\n",
      "\n",
      "Epoch 77: val_loss did not improve from 249.29295\n",
      "Epoch 78/600\n",
      "\n",
      "Epoch 78: val_loss did not improve from 249.29295\n",
      "Epoch 79/600\n",
      "\n",
      "Epoch 79: val_loss did not improve from 249.29295\n",
      "Epoch 80/600\n",
      "\n",
      "Epoch 80: val_loss did not improve from 249.29295\n",
      "Epoch 81/600\n",
      "\n",
      "Epoch 81: val_loss did not improve from 249.29295\n",
      "Epoch 82/600\n",
      "\n",
      "Epoch 82: val_loss did not improve from 249.29295\n",
      "Epoch 83/600\n",
      "\n",
      "Epoch 83: val_loss did not improve from 249.29295\n",
      "Epoch 84/600\n",
      "\n",
      "Epoch 84: val_loss did not improve from 249.29295\n",
      "Epoch 85/600\n",
      "\n",
      "Epoch 85: val_loss did not improve from 249.29295\n",
      "Epoch 86/600\n",
      "\n",
      "Epoch 86: val_loss did not improve from 249.29295\n",
      "Epoch 87/600\n",
      "\n",
      "Epoch 87: val_loss did not improve from 249.29295\n",
      "Epoch 88/600\n",
      "\n",
      "Epoch 88: val_loss did not improve from 249.29295\n",
      "Epoch 89/600\n",
      "\n",
      "Epoch 89: val_loss did not improve from 249.29295\n",
      "Epoch 90/600\n",
      "\n",
      "Epoch 90: val_loss did not improve from 249.29295\n",
      "Epoch 91/600\n",
      "\n",
      "Epoch 91: val_loss did not improve from 249.29295\n",
      "Epoch 92/600\n",
      "\n",
      "Epoch 92: val_loss did not improve from 249.29295\n",
      "Epoch 93/600\n",
      "\n",
      "Epoch 93: val_loss did not improve from 249.29295\n",
      "Epoch 94/600\n",
      "\n",
      "Epoch 94: val_loss did not improve from 249.29295\n",
      "Epoch 95/600\n",
      "\n",
      "Epoch 95: val_loss did not improve from 249.29295\n",
      "Epoch 96/600\n",
      "\n",
      "Epoch 96: val_loss did not improve from 249.29295\n",
      "Epoch 97/600\n",
      "\n",
      "Epoch 97: val_loss did not improve from 249.29295\n",
      "Epoch 98/600\n",
      "\n",
      "Epoch 98: val_loss did not improve from 249.29295\n",
      "Epoch 99/600\n",
      "\n",
      "Epoch 99: val_loss did not improve from 249.29295\n",
      "Epoch 100/600\n",
      "\n",
      "Epoch 100: val_loss did not improve from 249.29295\n",
      "Epoch 101/600\n",
      "\n",
      "Epoch 101: val_loss did not improve from 249.29295\n",
      "Epoch 102/600\n",
      "\n",
      "Epoch 102: val_loss did not improve from 249.29295\n",
      "Epoch 103/600\n",
      "\n",
      "Epoch 103: val_loss did not improve from 249.29295\n",
      "Epoch 104/600\n",
      "\n",
      "Epoch 104: val_loss did not improve from 249.29295\n",
      "Epoch 105/600\n",
      "\n",
      "Epoch 105: val_loss did not improve from 249.29295\n",
      "Epoch 106/600\n",
      "\n",
      "Epoch 106: val_loss did not improve from 249.29295\n",
      "Epoch 107/600\n",
      "\n",
      "Epoch 107: val_loss did not improve from 249.29295\n",
      "Epoch 108/600\n",
      "\n",
      "Epoch 108: val_loss did not improve from 249.29295\n",
      "Epoch 109/600\n",
      "\n",
      "Epoch 109: val_loss did not improve from 249.29295\n",
      "Epoch 110/600\n",
      "\n",
      "Epoch 110: val_loss did not improve from 249.29295\n",
      "Epoch 111/600\n",
      "\n",
      "Epoch 111: val_loss did not improve from 249.29295\n",
      "Epoch 112/600\n",
      "\n",
      "Epoch 112: val_loss did not improve from 249.29295\n",
      "Epoch 113/600\n",
      "\n",
      "Epoch 113: val_loss did not improve from 249.29295\n",
      "Epoch 114/600\n",
      "\n",
      "Epoch 114: val_loss did not improve from 249.29295\n",
      "Epoch 115/600\n",
      "\n",
      "Epoch 115: val_loss did not improve from 249.29295\n",
      "Epoch 116/600\n",
      "\n",
      "Epoch 116: val_loss did not improve from 249.29295\n",
      "Epoch 117/600\n",
      "\n",
      "Epoch 117: val_loss did not improve from 249.29295\n",
      "Epoch 118/600\n",
      "\n",
      "Epoch 118: val_loss did not improve from 249.29295\n",
      "Epoch 119/600\n",
      "\n",
      "Epoch 119: val_loss did not improve from 249.29295\n",
      "Epoch 120/600\n",
      "\n",
      "Epoch 120: val_loss did not improve from 249.29295\n",
      "Epoch 121/600\n",
      "\n",
      "Epoch 121: val_loss did not improve from 249.29295\n",
      "Epoch 122/600\n",
      "\n",
      "Epoch 122: val_loss did not improve from 249.29295\n",
      "Epoch 123/600\n",
      "\n",
      "Epoch 123: val_loss did not improve from 249.29295\n",
      "Epoch 124/600\n",
      "\n",
      "Epoch 124: val_loss did not improve from 249.29295\n",
      "Epoch 125/600\n",
      "\n",
      "Epoch 125: val_loss did not improve from 249.29295\n",
      "Epoch 126/600\n",
      "\n",
      "Epoch 126: val_loss did not improve from 249.29295\n",
      "Epoch 127/600\n",
      "\n",
      "Epoch 127: val_loss did not improve from 249.29295\n",
      "Epoch 128/600\n",
      "\n",
      "Epoch 128: val_loss did not improve from 249.29295\n",
      "Epoch 129/600\n",
      "\n",
      "Epoch 129: val_loss did not improve from 249.29295\n",
      "Epoch 130/600\n",
      "\n",
      "Epoch 130: val_loss did not improve from 249.29295\n",
      "Epoch 131/600\n",
      "\n",
      "Epoch 131: val_loss did not improve from 249.29295\n",
      "Epoch 132/600\n",
      "\n",
      "Epoch 132: val_loss did not improve from 249.29295\n",
      "Epoch 133/600\n",
      "\n",
      "Epoch 133: val_loss did not improve from 249.29295\n",
      "Epoch 134/600\n",
      "\n",
      "Epoch 134: val_loss did not improve from 249.29295\n",
      "Epoch 135/600\n",
      "\n",
      "Epoch 135: val_loss did not improve from 249.29295\n",
      "Epoch 136/600\n",
      "\n",
      "Epoch 136: val_loss did not improve from 249.29295\n",
      "Epoch 137/600\n",
      "\n",
      "Epoch 137: val_loss did not improve from 249.29295\n",
      "Epoch 138/600\n",
      "\n",
      "Epoch 138: val_loss did not improve from 249.29295\n",
      "Epoch 139/600\n",
      "\n",
      "Epoch 139: val_loss did not improve from 249.29295\n",
      "Epoch 140/600\n",
      "\n",
      "Epoch 140: val_loss did not improve from 249.29295\n",
      "Epoch 141/600\n",
      "\n",
      "Epoch 141: val_loss did not improve from 249.29295\n",
      "Epoch 142/600\n",
      "\n",
      "Epoch 142: val_loss did not improve from 249.29295\n",
      "Epoch 143/600\n",
      "\n",
      "Epoch 143: val_loss did not improve from 249.29295\n",
      "Epoch 144/600\n",
      "\n",
      "Epoch 144: val_loss did not improve from 249.29295\n",
      "Epoch 145/600\n",
      "\n",
      "Epoch 145: val_loss did not improve from 249.29295\n",
      "Epoch 146/600\n",
      "\n",
      "Epoch 146: val_loss did not improve from 249.29295\n",
      "Epoch 147/600\n",
      "\n",
      "Epoch 147: val_loss did not improve from 249.29295\n",
      "Epoch 148/600\n",
      "\n",
      "Epoch 148: val_loss did not improve from 249.29295\n",
      "Epoch 149/600\n",
      "\n",
      "Epoch 149: val_loss did not improve from 249.29295\n",
      "Epoch 150/600\n",
      "\n",
      "Epoch 150: val_loss did not improve from 249.29295\n",
      "Epoch 151/600\n",
      "\n",
      "Epoch 151: val_loss did not improve from 249.29295\n",
      "Epoch 152/600\n",
      "\n",
      "Epoch 152: val_loss did not improve from 249.29295\n",
      "Epoch 153/600\n",
      "\n",
      "Epoch 153: val_loss did not improve from 249.29295\n",
      "Epoch 154/600\n",
      "\n",
      "Epoch 154: val_loss did not improve from 249.29295\n",
      "Epoch 155/600\n",
      "\n",
      "Epoch 155: val_loss did not improve from 249.29295\n",
      "Epoch 156/600\n",
      "\n",
      "Epoch 156: val_loss did not improve from 249.29295\n",
      "Epoch 157/600\n",
      "\n",
      "Epoch 157: val_loss did not improve from 249.29295\n",
      "Epoch 158/600\n",
      "\n",
      "Epoch 158: val_loss did not improve from 249.29295\n",
      "Epoch 159/600\n",
      "\n",
      "Epoch 159: val_loss did not improve from 249.29295\n",
      "Epoch 160/600\n",
      "\n",
      "Epoch 160: val_loss did not improve from 249.29295\n",
      "Epoch 161/600\n",
      "\n",
      "Epoch 161: val_loss did not improve from 249.29295\n",
      "Epoch 162/600\n",
      "\n",
      "Epoch 162: val_loss did not improve from 249.29295\n",
      "Epoch 163/600\n",
      "\n",
      "Epoch 163: val_loss did not improve from 249.29295\n",
      "Epoch 164/600\n",
      "\n",
      "Epoch 164: val_loss did not improve from 249.29295\n",
      "Epoch 165/600\n",
      "\n",
      "Epoch 165: val_loss did not improve from 249.29295\n",
      "Epoch 166/600\n",
      "\n",
      "Epoch 166: val_loss did not improve from 249.29295\n",
      "Epoch 167/600\n",
      "\n",
      "Epoch 167: val_loss did not improve from 249.29295\n",
      "Epoch 168/600\n",
      "\n",
      "Epoch 168: val_loss did not improve from 249.29295\n",
      "Epoch 169/600\n",
      "\n",
      "Epoch 169: val_loss did not improve from 249.29295\n",
      "Epoch 170/600\n",
      "\n",
      "Epoch 170: val_loss did not improve from 249.29295\n",
      "Epoch 171/600\n",
      "\n",
      "Epoch 171: val_loss did not improve from 249.29295\n",
      "Epoch 172/600\n",
      "\n",
      "Epoch 172: val_loss did not improve from 249.29295\n",
      "Epoch 173/600\n",
      "\n",
      "Epoch 173: val_loss did not improve from 249.29295\n",
      "Epoch 174/600\n",
      "\n",
      "Epoch 174: val_loss did not improve from 249.29295\n",
      "Epoch 175/600\n",
      "\n",
      "Epoch 175: val_loss did not improve from 249.29295\n",
      "Epoch 176/600\n",
      "\n",
      "Epoch 176: val_loss did not improve from 249.29295\n",
      "Epoch 177/600\n",
      "\n",
      "Epoch 177: val_loss did not improve from 249.29295\n",
      "Epoch 178/600\n",
      "\n",
      "Epoch 178: val_loss did not improve from 249.29295\n",
      "Epoch 179/600\n",
      "\n",
      "Epoch 179: val_loss did not improve from 249.29295\n",
      "Epoch 180/600\n",
      "\n",
      "Epoch 180: val_loss did not improve from 249.29295\n",
      "Epoch 181/600\n",
      "\n",
      "Epoch 181: val_loss did not improve from 249.29295\n",
      "Epoch 182/600\n",
      "\n",
      "Epoch 182: val_loss did not improve from 249.29295\n",
      "Epoch 183/600\n",
      "\n",
      "Epoch 183: val_loss did not improve from 249.29295\n",
      "Epoch 184/600\n",
      "\n",
      "Epoch 184: val_loss did not improve from 249.29295\n",
      "Epoch 185/600\n",
      "\n",
      "Epoch 185: val_loss did not improve from 249.29295\n",
      "Epoch 186/600\n",
      "\n",
      "Epoch 186: val_loss did not improve from 249.29295\n",
      "Epoch 187/600\n",
      "\n",
      "Epoch 187: val_loss did not improve from 249.29295\n",
      "Epoch 188/600\n",
      "\n",
      "Epoch 188: val_loss did not improve from 249.29295\n",
      "Epoch 189/600\n",
      "\n",
      "Epoch 189: val_loss did not improve from 249.29295\n",
      "Epoch 190/600\n",
      "\n",
      "Epoch 190: val_loss did not improve from 249.29295\n",
      "Epoch 191/600\n",
      "\n",
      "Epoch 191: val_loss did not improve from 249.29295\n",
      "Epoch 192/600\n",
      "\n",
      "Epoch 192: val_loss did not improve from 249.29295\n",
      "Epoch 193/600\n",
      "\n",
      "Epoch 193: val_loss did not improve from 249.29295\n",
      "Epoch 194/600\n",
      "\n",
      "Epoch 194: val_loss did not improve from 249.29295\n",
      "Epoch 195/600\n",
      "\n",
      "Epoch 195: val_loss did not improve from 249.29295\n",
      "Epoch 196/600\n",
      "\n",
      "Epoch 196: val_loss did not improve from 249.29295\n",
      "Epoch 197/600\n",
      "\n",
      "Epoch 197: val_loss did not improve from 249.29295\n",
      "Epoch 198/600\n",
      "\n",
      "Epoch 198: val_loss did not improve from 249.29295\n",
      "Epoch 199/600\n",
      "\n",
      "Epoch 199: val_loss did not improve from 249.29295\n",
      "Epoch 200/600\n",
      "\n",
      "Epoch 200: val_loss did not improve from 249.29295\n",
      "Epoch 201/600\n",
      "\n",
      "Epoch 201: val_loss did not improve from 249.29295\n",
      "Epoch 202/600\n",
      "\n",
      "Epoch 202: val_loss did not improve from 249.29295\n",
      "Epoch 203/600\n",
      "\n",
      "Epoch 203: val_loss did not improve from 249.29295\n",
      "Epoch 204/600\n",
      "\n",
      "Epoch 204: val_loss did not improve from 249.29295\n",
      "Epoch 205/600\n",
      "\n",
      "Epoch 205: val_loss did not improve from 249.29295\n",
      "Epoch 206/600\n",
      "\n",
      "Epoch 206: val_loss did not improve from 249.29295\n",
      "Epoch 207/600\n",
      "\n",
      "Epoch 207: val_loss did not improve from 249.29295\n",
      "Epoch 208/600\n",
      "\n",
      "Epoch 208: val_loss did not improve from 249.29295\n",
      "Epoch 209/600\n",
      "\n",
      "Epoch 209: val_loss did not improve from 249.29295\n",
      "Epoch 210/600\n",
      "\n",
      "Epoch 210: val_loss did not improve from 249.29295\n",
      "Epoch 211/600\n",
      "\n",
      "Epoch 211: val_loss did not improve from 249.29295\n",
      "Epoch 212/600\n",
      "\n",
      "Epoch 212: val_loss did not improve from 249.29295\n",
      "Epoch 213/600\n",
      "\n",
      "Epoch 213: val_loss did not improve from 249.29295\n",
      "Epoch 214/600\n",
      "\n",
      "Epoch 214: val_loss did not improve from 249.29295\n",
      "Epoch 215/600\n",
      "\n",
      "Epoch 215: val_loss did not improve from 249.29295\n",
      "Epoch 216/600\n",
      "\n",
      "Epoch 216: val_loss did not improve from 249.29295\n",
      "Epoch 217/600\n",
      "\n",
      "Epoch 217: val_loss did not improve from 249.29295\n",
      "Epoch 218/600\n",
      "\n",
      "Epoch 218: val_loss did not improve from 249.29295\n",
      "Epoch 219/600\n",
      "\n",
      "Epoch 219: val_loss did not improve from 249.29295\n",
      "Epoch 220/600\n",
      "\n",
      "Epoch 220: val_loss did not improve from 249.29295\n",
      "Epoch 221/600\n",
      "\n",
      "Epoch 221: val_loss did not improve from 249.29295\n",
      "Epoch 222/600\n",
      "\n",
      "Epoch 222: val_loss did not improve from 249.29295\n",
      "Epoch 223/600\n",
      "\n",
      "Epoch 223: val_loss did not improve from 249.29295\n",
      "Epoch 224/600\n",
      "\n",
      "Epoch 224: val_loss did not improve from 249.29295\n",
      "Epoch 225/600\n",
      "\n",
      "Epoch 225: val_loss did not improve from 249.29295\n",
      "Epoch 226/600\n",
      "\n",
      "Epoch 226: val_loss did not improve from 249.29295\n",
      "Epoch 227/600\n",
      "\n",
      "Epoch 227: val_loss did not improve from 249.29295\n",
      "Epoch 228/600\n",
      "\n",
      "Epoch 228: val_loss did not improve from 249.29295\n",
      "Epoch 229/600\n",
      "\n",
      "Epoch 229: val_loss did not improve from 249.29295\n",
      "Epoch 230/600\n",
      "\n",
      "Epoch 230: val_loss did not improve from 249.29295\n",
      "Epoch 231/600\n",
      "\n",
      "Epoch 231: val_loss did not improve from 249.29295\n",
      "Epoch 232/600\n",
      "\n",
      "Epoch 232: val_loss did not improve from 249.29295\n",
      "Epoch 233/600\n",
      "\n",
      "Epoch 233: val_loss did not improve from 249.29295\n",
      "Epoch 234/600\n",
      "\n",
      "Epoch 234: val_loss did not improve from 249.29295\n",
      "Epoch 235/600\n",
      "\n",
      "Epoch 235: val_loss did not improve from 249.29295\n",
      "Epoch 236/600\n",
      "\n",
      "Epoch 236: val_loss did not improve from 249.29295\n",
      "Epoch 237/600\n",
      "\n",
      "Epoch 237: val_loss did not improve from 249.29295\n",
      "Epoch 238/600\n",
      "\n",
      "Epoch 238: val_loss did not improve from 249.29295\n",
      "Epoch 239/600\n",
      "\n",
      "Epoch 239: val_loss did not improve from 249.29295\n",
      "Epoch 240/600\n",
      "\n",
      "Epoch 240: val_loss did not improve from 249.29295\n",
      "Epoch 241/600\n",
      "\n",
      "Epoch 241: val_loss did not improve from 249.29295\n",
      "Epoch 242/600\n",
      "\n",
      "Epoch 242: val_loss did not improve from 249.29295\n",
      "Epoch 243/600\n",
      "\n",
      "Epoch 243: val_loss did not improve from 249.29295\n",
      "Epoch 244/600\n",
      "\n",
      "Epoch 244: val_loss did not improve from 249.29295\n",
      "Epoch 245/600\n",
      "\n",
      "Epoch 245: val_loss did not improve from 249.29295\n",
      "Epoch 246/600\n",
      "\n",
      "Epoch 246: val_loss did not improve from 249.29295\n",
      "Epoch 247/600\n",
      "\n",
      "Epoch 247: val_loss did not improve from 249.29295\n",
      "Epoch 248/600\n",
      "\n",
      "Epoch 248: val_loss did not improve from 249.29295\n",
      "Epoch 249/600\n",
      "\n",
      "Epoch 249: val_loss did not improve from 249.29295\n",
      "Epoch 250/600\n",
      "\n",
      "Epoch 250: val_loss did not improve from 249.29295\n",
      "Epoch 251/600\n",
      "\n",
      "Epoch 251: val_loss did not improve from 249.29295\n",
      "Epoch 252/600\n",
      "\n",
      "Epoch 252: val_loss did not improve from 249.29295\n",
      "Epoch 253/600\n",
      "\n",
      "Epoch 253: val_loss did not improve from 249.29295\n",
      "Epoch 254/600\n",
      "\n",
      "Epoch 254: val_loss did not improve from 249.29295\n",
      "Epoch 255/600\n",
      "\n",
      "Epoch 255: val_loss did not improve from 249.29295\n",
      "Epoch 256/600\n",
      "\n",
      "Epoch 256: val_loss did not improve from 249.29295\n",
      "Epoch 257/600\n",
      "\n",
      "Epoch 257: val_loss did not improve from 249.29295\n",
      "Epoch 258/600\n",
      "\n",
      "Epoch 258: val_loss did not improve from 249.29295\n",
      "Epoch 259/600\n",
      "\n",
      "Epoch 259: val_loss did not improve from 249.29295\n",
      "Epoch 260/600\n",
      "\n",
      "Epoch 260: val_loss did not improve from 249.29295\n",
      "Epoch 261/600\n",
      "\n",
      "Epoch 261: val_loss did not improve from 249.29295\n",
      "Epoch 262/600\n",
      "\n",
      "Epoch 262: val_loss did not improve from 249.29295\n",
      "Epoch 263/600\n",
      "\n",
      "Epoch 263: val_loss did not improve from 249.29295\n",
      "Epoch 264/600\n",
      "\n",
      "Epoch 264: val_loss did not improve from 249.29295\n",
      "Epoch 265/600\n",
      "\n",
      "Epoch 265: val_loss did not improve from 249.29295\n",
      "Epoch 266/600\n",
      "\n",
      "Epoch 266: val_loss did not improve from 249.29295\n",
      "Epoch 267/600\n",
      "\n",
      "Epoch 267: val_loss did not improve from 249.29295\n",
      "Epoch 268/600\n",
      "\n",
      "Epoch 268: val_loss did not improve from 249.29295\n",
      "Epoch 269/600\n",
      "\n",
      "Epoch 269: val_loss did not improve from 249.29295\n",
      "Epoch 270/600\n",
      "\n",
      "Epoch 270: val_loss did not improve from 249.29295\n",
      "Epoch 271/600\n",
      "\n",
      "Epoch 271: val_loss did not improve from 249.29295\n",
      "Epoch 272/600\n",
      "\n",
      "Epoch 272: val_loss did not improve from 249.29295\n",
      "Epoch 273/600\n",
      "\n",
      "Epoch 273: val_loss did not improve from 249.29295\n",
      "Epoch 274/600\n",
      "\n",
      "Epoch 274: val_loss did not improve from 249.29295\n",
      "Epoch 275/600\n",
      "\n",
      "Epoch 275: val_loss did not improve from 249.29295\n",
      "Epoch 276/600\n",
      "\n",
      "Epoch 276: val_loss did not improve from 249.29295\n",
      "Epoch 277/600\n",
      "\n",
      "Epoch 277: val_loss did not improve from 249.29295\n",
      "Epoch 278/600\n",
      "\n",
      "Epoch 278: val_loss did not improve from 249.29295\n",
      "Epoch 279/600\n",
      "\n",
      "Epoch 279: val_loss did not improve from 249.29295\n",
      "Epoch 280/600\n",
      "\n",
      "Epoch 280: val_loss did not improve from 249.29295\n",
      "Epoch 281/600\n",
      "\n",
      "Epoch 281: val_loss did not improve from 249.29295\n",
      "Epoch 282/600\n",
      "\n",
      "Epoch 282: val_loss did not improve from 249.29295\n",
      "Epoch 283/600\n",
      "\n",
      "Epoch 283: val_loss did not improve from 249.29295\n",
      "Epoch 284/600\n",
      "\n",
      "Epoch 284: val_loss did not improve from 249.29295\n",
      "Epoch 285/600\n",
      "\n",
      "Epoch 285: val_loss did not improve from 249.29295\n",
      "Epoch 286/600\n",
      "\n",
      "Epoch 286: val_loss did not improve from 249.29295\n",
      "Epoch 287/600\n",
      "\n",
      "Epoch 287: val_loss did not improve from 249.29295\n",
      "Epoch 288/600\n",
      "\n",
      "Epoch 288: val_loss did not improve from 249.29295\n",
      "Epoch 289/600\n",
      "\n",
      "Epoch 289: val_loss did not improve from 249.29295\n",
      "Epoch 290/600\n",
      "\n",
      "Epoch 290: val_loss did not improve from 249.29295\n",
      "Epoch 291/600\n",
      "\n",
      "Epoch 291: val_loss did not improve from 249.29295\n",
      "Epoch 292/600\n",
      "\n",
      "Epoch 292: val_loss did not improve from 249.29295\n",
      "Epoch 293/600\n",
      "\n",
      "Epoch 293: val_loss did not improve from 249.29295\n",
      "Epoch 294/600\n",
      "\n",
      "Epoch 294: val_loss did not improve from 249.29295\n",
      "Epoch 295/600\n",
      "\n",
      "Epoch 295: val_loss did not improve from 249.29295\n",
      "Epoch 296/600\n",
      "\n",
      "Epoch 296: val_loss did not improve from 249.29295\n",
      "Epoch 297/600\n",
      "\n",
      "Epoch 297: val_loss did not improve from 249.29295\n",
      "Epoch 298/600\n",
      "\n",
      "Epoch 298: val_loss did not improve from 249.29295\n",
      "Epoch 299/600\n",
      "\n",
      "Epoch 299: val_loss did not improve from 249.29295\n",
      "Epoch 300/600\n",
      "\n",
      "Epoch 300: val_loss did not improve from 249.29295\n",
      "Epoch 301/600\n",
      "\n",
      "Epoch 301: val_loss did not improve from 249.29295\n",
      "Epoch 302/600\n",
      "\n",
      "Epoch 302: val_loss did not improve from 249.29295\n",
      "Epoch 303/600\n",
      "\n",
      "Epoch 303: val_loss did not improve from 249.29295\n",
      "Epoch 304/600\n",
      "\n",
      "Epoch 304: val_loss did not improve from 249.29295\n",
      "Epoch 305/600\n",
      "\n",
      "Epoch 305: val_loss did not improve from 249.29295\n",
      "Epoch 306/600\n",
      "\n",
      "Epoch 306: val_loss did not improve from 249.29295\n",
      "Epoch 307/600\n",
      "\n",
      "Epoch 307: val_loss did not improve from 249.29295\n",
      "Epoch 308/600\n",
      "\n",
      "Epoch 308: val_loss did not improve from 249.29295\n",
      "Epoch 309/600\n",
      "\n",
      "Epoch 309: val_loss did not improve from 249.29295\n",
      "Epoch 310/600\n",
      "\n",
      "Epoch 310: val_loss did not improve from 249.29295\n",
      "Epoch 311/600\n",
      "\n",
      "Epoch 311: val_loss did not improve from 249.29295\n",
      "Epoch 312/600\n",
      "\n",
      "Epoch 312: val_loss did not improve from 249.29295\n",
      "Epoch 313/600\n",
      "\n",
      "Epoch 313: val_loss did not improve from 249.29295\n",
      "Epoch 314/600\n",
      "\n",
      "Epoch 314: val_loss did not improve from 249.29295\n",
      "Epoch 315/600\n",
      "\n",
      "Epoch 315: val_loss did not improve from 249.29295\n",
      "Epoch 316/600\n",
      "\n",
      "Epoch 316: val_loss did not improve from 249.29295\n",
      "Epoch 317/600\n",
      "\n",
      "Epoch 317: val_loss did not improve from 249.29295\n",
      "Epoch 318/600\n",
      "\n",
      "Epoch 318: val_loss did not improve from 249.29295\n",
      "Epoch 319/600\n",
      "\n",
      "Epoch 319: val_loss did not improve from 249.29295\n",
      "Epoch 320/600\n",
      "\n",
      "Epoch 320: val_loss did not improve from 249.29295\n",
      "Epoch 321/600\n",
      "\n",
      "Epoch 321: val_loss did not improve from 249.29295\n",
      "Epoch 322/600\n",
      "\n",
      "Epoch 322: val_loss did not improve from 249.29295\n",
      "Epoch 323/600\n",
      "\n",
      "Epoch 323: val_loss did not improve from 249.29295\n",
      "Epoch 324/600\n",
      "\n",
      "Epoch 324: val_loss did not improve from 249.29295\n",
      "Epoch 325/600\n",
      "\n",
      "Epoch 325: val_loss did not improve from 249.29295\n",
      "Epoch 326/600\n",
      "\n",
      "Epoch 326: val_loss did not improve from 249.29295\n",
      "Epoch 327/600\n",
      "\n",
      "Epoch 327: val_loss did not improve from 249.29295\n",
      "Epoch 328/600\n",
      "\n",
      "Epoch 328: val_loss did not improve from 249.29295\n",
      "Epoch 329/600\n",
      "\n",
      "Epoch 329: val_loss did not improve from 249.29295\n",
      "Epoch 330/600\n",
      "\n",
      "Epoch 330: val_loss did not improve from 249.29295\n",
      "Epoch 331/600\n",
      "\n",
      "Epoch 331: val_loss did not improve from 249.29295\n",
      "Epoch 332/600\n",
      "\n",
      "Epoch 332: val_loss did not improve from 249.29295\n",
      "Epoch 333/600\n",
      "\n",
      "Epoch 333: val_loss did not improve from 249.29295\n",
      "Epoch 334/600\n",
      "\n",
      "Epoch 334: val_loss did not improve from 249.29295\n",
      "Epoch 335/600\n",
      "\n",
      "Epoch 335: val_loss did not improve from 249.29295\n",
      "Epoch 336/600\n",
      "\n",
      "Epoch 336: val_loss did not improve from 249.29295\n",
      "Epoch 337/600\n",
      "\n",
      "Epoch 337: val_loss did not improve from 249.29295\n",
      "Epoch 338/600\n",
      "\n",
      "Epoch 338: val_loss did not improve from 249.29295\n",
      "Epoch 339/600\n",
      "\n",
      "Epoch 339: val_loss did not improve from 249.29295\n",
      "Epoch 340/600\n",
      "\n",
      "Epoch 340: val_loss did not improve from 249.29295\n",
      "Epoch 341/600\n",
      "\n",
      "Epoch 341: val_loss did not improve from 249.29295\n",
      "Epoch 342/600\n",
      "\n",
      "Epoch 342: val_loss did not improve from 249.29295\n",
      "Epoch 343/600\n",
      "\n",
      "Epoch 343: val_loss did not improve from 249.29295\n",
      "Epoch 344/600\n",
      "\n",
      "Epoch 344: val_loss did not improve from 249.29295\n",
      "Epoch 345/600\n",
      "\n",
      "Epoch 345: val_loss did not improve from 249.29295\n",
      "Epoch 346/600\n",
      "\n",
      "Epoch 346: val_loss did not improve from 249.29295\n",
      "Epoch 347/600\n",
      "\n",
      "Epoch 347: val_loss did not improve from 249.29295\n",
      "Epoch 348/600\n",
      "\n",
      "Epoch 348: val_loss did not improve from 249.29295\n",
      "Epoch 349/600\n",
      "\n",
      "Epoch 349: val_loss did not improve from 249.29295\n",
      "Epoch 350/600\n",
      "\n",
      "Epoch 350: val_loss did not improve from 249.29295\n",
      "Epoch 351/600\n",
      "\n",
      "Epoch 351: val_loss did not improve from 249.29295\n",
      "Epoch 352/600\n",
      "\n",
      "Epoch 352: val_loss did not improve from 249.29295\n",
      "Epoch 353/600\n",
      "\n",
      "Epoch 353: val_loss did not improve from 249.29295\n",
      "Epoch 354/600\n",
      "\n",
      "Epoch 354: val_loss did not improve from 249.29295\n",
      "Epoch 355/600\n",
      "\n",
      "Epoch 355: val_loss did not improve from 249.29295\n",
      "Epoch 356/600\n",
      "\n",
      "Epoch 356: val_loss did not improve from 249.29295\n",
      "Epoch 357/600\n",
      "\n",
      "Epoch 357: val_loss did not improve from 249.29295\n",
      "Epoch 358/600\n",
      "\n",
      "Epoch 358: val_loss did not improve from 249.29295\n",
      "Epoch 359/600\n",
      "\n",
      "Epoch 359: val_loss did not improve from 249.29295\n",
      "Epoch 360/600\n",
      "\n",
      "Epoch 360: val_loss did not improve from 249.29295\n",
      "Epoch 361/600\n",
      "\n",
      "Epoch 361: val_loss did not improve from 249.29295\n",
      "Epoch 362/600\n",
      "\n",
      "Epoch 362: val_loss did not improve from 249.29295\n",
      "Epoch 363/600\n",
      "\n",
      "Epoch 363: val_loss did not improve from 249.29295\n",
      "Epoch 364/600\n",
      "\n",
      "Epoch 364: val_loss did not improve from 249.29295\n",
      "Epoch 365/600\n",
      "\n",
      "Epoch 365: val_loss did not improve from 249.29295\n",
      "Epoch 366/600\n",
      "\n",
      "Epoch 366: val_loss did not improve from 249.29295\n",
      "Epoch 367/600\n",
      "\n",
      "Epoch 367: val_loss did not improve from 249.29295\n",
      "Epoch 368/600\n",
      "\n",
      "Epoch 368: val_loss did not improve from 249.29295\n",
      "Epoch 369/600\n",
      "\n",
      "Epoch 369: val_loss did not improve from 249.29295\n",
      "Epoch 370/600\n",
      "\n",
      "Epoch 370: val_loss did not improve from 249.29295\n",
      "Epoch 371/600\n",
      "\n",
      "Epoch 371: val_loss did not improve from 249.29295\n",
      "Epoch 372/600\n",
      "\n",
      "Epoch 372: val_loss did not improve from 249.29295\n",
      "Epoch 373/600\n",
      "\n",
      "Epoch 373: val_loss did not improve from 249.29295\n",
      "Epoch 374/600\n",
      "\n",
      "Epoch 374: val_loss did not improve from 249.29295\n",
      "Epoch 375/600\n",
      "\n",
      "Epoch 375: val_loss did not improve from 249.29295\n",
      "Epoch 376/600\n",
      "\n",
      "Epoch 376: val_loss did not improve from 249.29295\n",
      "Epoch 377/600\n",
      "\n",
      "Epoch 377: val_loss did not improve from 249.29295\n",
      "Epoch 378/600\n",
      "\n",
      "Epoch 378: val_loss did not improve from 249.29295\n",
      "Epoch 379/600\n",
      "\n",
      "Epoch 379: val_loss did not improve from 249.29295\n",
      "Epoch 380/600\n",
      "\n",
      "Epoch 380: val_loss did not improve from 249.29295\n",
      "Epoch 381/600\n",
      "\n",
      "Epoch 381: val_loss did not improve from 249.29295\n",
      "Epoch 382/600\n",
      "\n",
      "Epoch 382: val_loss did not improve from 249.29295\n",
      "Epoch 383/600\n",
      "\n",
      "Epoch 383: val_loss did not improve from 249.29295\n",
      "Epoch 384/600\n",
      "\n",
      "Epoch 384: val_loss did not improve from 249.29295\n",
      "Epoch 385/600\n",
      "\n",
      "Epoch 385: val_loss did not improve from 249.29295\n",
      "Epoch 386/600\n",
      "\n",
      "Epoch 386: val_loss did not improve from 249.29295\n",
      "Epoch 387/600\n",
      "\n",
      "Epoch 387: val_loss did not improve from 249.29295\n",
      "Epoch 388/600\n",
      "\n",
      "Epoch 388: val_loss did not improve from 249.29295\n",
      "Epoch 389/600\n",
      "\n",
      "Epoch 389: val_loss did not improve from 249.29295\n",
      "Epoch 390/600\n",
      "\n",
      "Epoch 390: val_loss did not improve from 249.29295\n",
      "Epoch 391/600\n",
      "\n",
      "Epoch 391: val_loss did not improve from 249.29295\n",
      "Epoch 392/600\n",
      "\n",
      "Epoch 392: val_loss did not improve from 249.29295\n",
      "Epoch 393/600\n",
      "\n",
      "Epoch 393: val_loss did not improve from 249.29295\n",
      "Epoch 394/600\n",
      "\n",
      "Epoch 394: val_loss did not improve from 249.29295\n",
      "Epoch 395/600\n",
      "\n",
      "Epoch 395: val_loss did not improve from 249.29295\n",
      "Epoch 396/600\n",
      "\n",
      "Epoch 396: val_loss did not improve from 249.29295\n",
      "Epoch 397/600\n",
      "\n",
      "Epoch 397: val_loss did not improve from 249.29295\n",
      "Epoch 398/600\n",
      "\n",
      "Epoch 398: val_loss did not improve from 249.29295\n",
      "Epoch 399/600\n",
      "\n",
      "Epoch 399: val_loss did not improve from 249.29295\n",
      "Epoch 400/600\n",
      "\n",
      "Epoch 400: val_loss did not improve from 249.29295\n",
      "Epoch 401/600\n",
      "\n",
      "Epoch 401: val_loss did not improve from 249.29295\n",
      "Epoch 402/600\n",
      "\n",
      "Epoch 402: val_loss did not improve from 249.29295\n",
      "Epoch 403/600\n",
      "\n",
      "Epoch 403: val_loss did not improve from 249.29295\n",
      "Epoch 404/600\n",
      "\n",
      "Epoch 404: val_loss did not improve from 249.29295\n",
      "Epoch 405/600\n",
      "\n",
      "Epoch 405: val_loss did not improve from 249.29295\n",
      "Epoch 406/600\n",
      "\n",
      "Epoch 406: val_loss did not improve from 249.29295\n",
      "Epoch 407/600\n",
      "\n",
      "Epoch 407: val_loss did not improve from 249.29295\n",
      "Epoch 408/600\n",
      "\n",
      "Epoch 408: val_loss did not improve from 249.29295\n",
      "Epoch 409/600\n",
      "\n",
      "Epoch 409: val_loss did not improve from 249.29295\n",
      "Epoch 410/600\n",
      "\n",
      "Epoch 410: val_loss did not improve from 249.29295\n",
      "Epoch 411/600\n",
      "\n",
      "Epoch 411: val_loss did not improve from 249.29295\n",
      "Epoch 412/600\n",
      "\n",
      "Epoch 412: val_loss did not improve from 249.29295\n",
      "Epoch 413/600\n",
      "\n",
      "Epoch 413: val_loss did not improve from 249.29295\n",
      "Epoch 414/600\n",
      "\n",
      "Epoch 414: val_loss did not improve from 249.29295\n",
      "Epoch 415/600\n",
      "\n",
      "Epoch 415: val_loss did not improve from 249.29295\n",
      "Epoch 416/600\n",
      "\n",
      "Epoch 416: val_loss did not improve from 249.29295\n",
      "Epoch 417/600\n",
      "\n",
      "Epoch 417: val_loss did not improve from 249.29295\n",
      "Epoch 418/600\n",
      "\n",
      "Epoch 418: val_loss did not improve from 249.29295\n",
      "Epoch 419/600\n",
      "\n",
      "Epoch 419: val_loss did not improve from 249.29295\n",
      "Epoch 420/600\n",
      "\n",
      "Epoch 420: val_loss did not improve from 249.29295\n",
      "Epoch 421/600\n",
      "\n",
      "Epoch 421: val_loss did not improve from 249.29295\n",
      "Epoch 422/600\n",
      "\n",
      "Epoch 422: val_loss did not improve from 249.29295\n",
      "Epoch 423/600\n",
      "\n",
      "Epoch 423: val_loss did not improve from 249.29295\n",
      "Epoch 424/600\n",
      "\n",
      "Epoch 424: val_loss did not improve from 249.29295\n",
      "Epoch 425/600\n",
      "\n",
      "Epoch 425: val_loss did not improve from 249.29295\n",
      "Epoch 426/600\n",
      "\n",
      "Epoch 426: val_loss did not improve from 249.29295\n",
      "Epoch 427/600\n",
      "\n",
      "Epoch 427: val_loss did not improve from 249.29295\n",
      "Epoch 428/600\n",
      "\n",
      "Epoch 428: val_loss did not improve from 249.29295\n",
      "Epoch 429/600\n",
      "\n",
      "Epoch 429: val_loss did not improve from 249.29295\n",
      "Epoch 430/600\n",
      "\n",
      "Epoch 430: val_loss did not improve from 249.29295\n",
      "Epoch 431/600\n",
      "\n",
      "Epoch 431: val_loss did not improve from 249.29295\n",
      "Epoch 432/600\n",
      "\n",
      "Epoch 432: val_loss did not improve from 249.29295\n",
      "Epoch 433/600\n",
      "\n",
      "Epoch 433: val_loss did not improve from 249.29295\n",
      "Epoch 434/600\n",
      "\n",
      "Epoch 434: val_loss did not improve from 249.29295\n",
      "Epoch 435/600\n",
      "\n",
      "Epoch 435: val_loss did not improve from 249.29295\n",
      "Epoch 436/600\n",
      "\n",
      "Epoch 436: val_loss did not improve from 249.29295\n",
      "Epoch 437/600\n",
      "\n",
      "Epoch 437: val_loss did not improve from 249.29295\n",
      "Epoch 438/600\n",
      "\n",
      "Epoch 438: val_loss did not improve from 249.29295\n",
      "Epoch 439/600\n",
      "\n",
      "Epoch 439: val_loss did not improve from 249.29295\n",
      "Epoch 440/600\n",
      "\n",
      "Epoch 440: val_loss did not improve from 249.29295\n",
      "Epoch 441/600\n",
      "\n",
      "Epoch 441: val_loss did not improve from 249.29295\n",
      "Epoch 442/600\n",
      "\n",
      "Epoch 442: val_loss did not improve from 249.29295\n",
      "Epoch 443/600\n",
      "\n",
      "Epoch 443: val_loss did not improve from 249.29295\n",
      "Epoch 444/600\n",
      "\n",
      "Epoch 444: val_loss did not improve from 249.29295\n",
      "Epoch 445/600\n",
      "\n",
      "Epoch 445: val_loss did not improve from 249.29295\n",
      "Epoch 446/600\n",
      "\n",
      "Epoch 446: val_loss did not improve from 249.29295\n",
      "Epoch 447/600\n",
      "\n",
      "Epoch 447: val_loss did not improve from 249.29295\n",
      "Epoch 448/600\n",
      "\n",
      "Epoch 448: val_loss did not improve from 249.29295\n",
      "Epoch 449/600\n",
      "\n",
      "Epoch 449: val_loss did not improve from 249.29295\n",
      "Epoch 450/600\n",
      "\n",
      "Epoch 450: val_loss did not improve from 249.29295\n",
      "Epoch 451/600\n",
      "\n",
      "Epoch 451: val_loss did not improve from 249.29295\n",
      "Epoch 452/600\n",
      "\n",
      "Epoch 452: val_loss did not improve from 249.29295\n",
      "Epoch 453/600\n",
      "\n",
      "Epoch 453: val_loss did not improve from 249.29295\n",
      "Epoch 454/600\n",
      "\n",
      "Epoch 454: val_loss did not improve from 249.29295\n",
      "Epoch 455/600\n",
      "\n",
      "Epoch 455: val_loss did not improve from 249.29295\n",
      "Epoch 456/600\n",
      "\n",
      "Epoch 456: val_loss did not improve from 249.29295\n",
      "Epoch 457/600\n",
      "\n",
      "Epoch 457: val_loss did not improve from 249.29295\n",
      "Epoch 458/600\n",
      "\n",
      "Epoch 458: val_loss did not improve from 249.29295\n",
      "Epoch 459/600\n",
      "\n",
      "Epoch 459: val_loss did not improve from 249.29295\n",
      "Epoch 460/600\n",
      "\n",
      "Epoch 460: val_loss did not improve from 249.29295\n",
      "Epoch 461/600\n",
      "\n",
      "Epoch 461: val_loss did not improve from 249.29295\n",
      "Epoch 462/600\n",
      "\n",
      "Epoch 462: val_loss did not improve from 249.29295\n",
      "Epoch 463/600\n",
      "\n",
      "Epoch 463: val_loss did not improve from 249.29295\n",
      "Epoch 464/600\n",
      "\n",
      "Epoch 464: val_loss did not improve from 249.29295\n",
      "Epoch 465/600\n",
      "\n",
      "Epoch 465: val_loss did not improve from 249.29295\n",
      "Epoch 466/600\n",
      "\n",
      "Epoch 466: val_loss did not improve from 249.29295\n",
      "Epoch 467/600\n",
      "\n",
      "Epoch 467: val_loss did not improve from 249.29295\n",
      "Epoch 468/600\n",
      "\n",
      "Epoch 468: val_loss did not improve from 249.29295\n",
      "Epoch 469/600\n",
      "\n",
      "Epoch 469: val_loss did not improve from 249.29295\n",
      "Epoch 470/600\n",
      "\n",
      "Epoch 470: val_loss did not improve from 249.29295\n",
      "Epoch 471/600\n",
      "\n",
      "Epoch 471: val_loss did not improve from 249.29295\n",
      "Epoch 472/600\n",
      "\n",
      "Epoch 472: val_loss did not improve from 249.29295\n",
      "Epoch 473/600\n",
      "\n",
      "Epoch 473: val_loss did not improve from 249.29295\n",
      "Epoch 474/600\n",
      "\n",
      "Epoch 474: val_loss did not improve from 249.29295\n",
      "Epoch 475/600\n",
      "\n",
      "Epoch 475: val_loss did not improve from 249.29295\n",
      "Epoch 476/600\n",
      "\n",
      "Epoch 476: val_loss did not improve from 249.29295\n",
      "Epoch 477/600\n",
      "\n",
      "Epoch 477: val_loss did not improve from 249.29295\n",
      "Epoch 478/600\n",
      "\n",
      "Epoch 478: val_loss did not improve from 249.29295\n",
      "Epoch 479/600\n",
      "\n",
      "Epoch 479: val_loss did not improve from 249.29295\n",
      "Epoch 480/600\n",
      "\n",
      "Epoch 480: val_loss did not improve from 249.29295\n",
      "Epoch 481/600\n",
      "\n",
      "Epoch 481: val_loss did not improve from 249.29295\n",
      "Epoch 482/600\n",
      "\n",
      "Epoch 482: val_loss did not improve from 249.29295\n",
      "Epoch 483/600\n",
      "\n",
      "Epoch 483: val_loss did not improve from 249.29295\n",
      "Epoch 484/600\n",
      "\n",
      "Epoch 484: val_loss did not improve from 249.29295\n",
      "Epoch 485/600\n",
      "\n",
      "Epoch 485: val_loss did not improve from 249.29295\n",
      "Epoch 486/600\n",
      "\n",
      "Epoch 486: val_loss did not improve from 249.29295\n",
      "Epoch 487/600\n",
      "\n",
      "Epoch 487: val_loss did not improve from 249.29295\n",
      "Epoch 488/600\n",
      "\n",
      "Epoch 488: val_loss did not improve from 249.29295\n",
      "Epoch 489/600\n",
      "\n",
      "Epoch 489: val_loss did not improve from 249.29295\n",
      "Epoch 490/600\n",
      "\n",
      "Epoch 490: val_loss did not improve from 249.29295\n",
      "Epoch 491/600\n",
      "\n",
      "Epoch 491: val_loss did not improve from 249.29295\n",
      "Epoch 492/600\n",
      "\n",
      "Epoch 492: val_loss did not improve from 249.29295\n",
      "Epoch 493/600\n",
      "\n",
      "Epoch 493: val_loss did not improve from 249.29295\n",
      "Epoch 494/600\n",
      "\n",
      "Epoch 494: val_loss did not improve from 249.29295\n",
      "Epoch 495/600\n",
      "\n",
      "Epoch 495: val_loss did not improve from 249.29295\n",
      "Epoch 496/600\n",
      "\n",
      "Epoch 496: val_loss did not improve from 249.29295\n",
      "Epoch 497/600\n",
      "\n",
      "Epoch 497: val_loss did not improve from 249.29295\n",
      "Epoch 498/600\n",
      "\n",
      "Epoch 498: val_loss did not improve from 249.29295\n",
      "Epoch 499/600\n",
      "\n",
      "Epoch 499: val_loss did not improve from 249.29295\n",
      "Epoch 500/600\n",
      "\n",
      "Epoch 500: val_loss did not improve from 249.29295\n",
      "Epoch 501/600\n",
      "\n",
      "Epoch 501: val_loss did not improve from 249.29295\n",
      "Epoch 502/600\n",
      "\n",
      "Epoch 502: val_loss did not improve from 249.29295\n",
      "Epoch 503/600\n",
      "\n",
      "Epoch 503: val_loss did not improve from 249.29295\n",
      "Epoch 504/600\n",
      "\n",
      "Epoch 504: val_loss did not improve from 249.29295\n",
      "Epoch 505/600\n",
      "\n",
      "Epoch 505: val_loss did not improve from 249.29295\n",
      "Epoch 506/600\n",
      "\n",
      "Epoch 506: val_loss did not improve from 249.29295\n",
      "Epoch 507/600\n",
      "\n",
      "Epoch 507: val_loss did not improve from 249.29295\n",
      "Epoch 508/600\n",
      "\n",
      "Epoch 508: val_loss did not improve from 249.29295\n",
      "Epoch 509/600\n",
      "\n",
      "Epoch 509: val_loss did not improve from 249.29295\n",
      "Epoch 510/600\n",
      "\n",
      "Epoch 510: val_loss did not improve from 249.29295\n",
      "Epoch 511/600\n",
      "\n",
      "Epoch 511: val_loss did not improve from 249.29295\n",
      "Epoch 512/600\n",
      "\n",
      "Epoch 512: val_loss did not improve from 249.29295\n",
      "Epoch 513/600\n",
      "\n",
      "Epoch 513: val_loss did not improve from 249.29295\n",
      "Epoch 514/600\n",
      "\n",
      "Epoch 514: val_loss did not improve from 249.29295\n",
      "Epoch 515/600\n",
      "\n",
      "Epoch 515: val_loss did not improve from 249.29295\n",
      "Epoch 516/600\n",
      "\n",
      "Epoch 516: val_loss did not improve from 249.29295\n",
      "Epoch 517/600\n",
      "\n",
      "Epoch 517: val_loss did not improve from 249.29295\n",
      "Epoch 518/600\n",
      "\n",
      "Epoch 518: val_loss did not improve from 249.29295\n",
      "Epoch 519/600\n",
      "\n",
      "Epoch 519: val_loss did not improve from 249.29295\n",
      "Epoch 520/600\n",
      "\n",
      "Epoch 520: val_loss did not improve from 249.29295\n",
      "Epoch 521/600\n",
      "\n",
      "Epoch 521: val_loss did not improve from 249.29295\n",
      "Epoch 522/600\n",
      "\n",
      "Epoch 522: val_loss did not improve from 249.29295\n",
      "Epoch 523/600\n",
      "\n",
      "Epoch 523: val_loss did not improve from 249.29295\n",
      "Epoch 524/600\n",
      "\n",
      "Epoch 524: val_loss did not improve from 249.29295\n",
      "Epoch 525/600\n",
      "\n",
      "Epoch 525: val_loss did not improve from 249.29295\n",
      "Epoch 526/600\n",
      "\n",
      "Epoch 526: val_loss did not improve from 249.29295\n",
      "Epoch 527/600\n",
      "\n",
      "Epoch 527: val_loss did not improve from 249.29295\n",
      "Epoch 528/600\n",
      "\n",
      "Epoch 528: val_loss did not improve from 249.29295\n",
      "Epoch 529/600\n",
      "\n",
      "Epoch 529: val_loss did not improve from 249.29295\n",
      "Epoch 530/600\n",
      "\n",
      "Epoch 530: val_loss did not improve from 249.29295\n",
      "Epoch 531/600\n",
      "\n",
      "Epoch 531: val_loss did not improve from 249.29295\n",
      "Epoch 532/600\n",
      "\n",
      "Epoch 532: val_loss did not improve from 249.29295\n",
      "Epoch 533/600\n",
      "\n",
      "Epoch 533: val_loss did not improve from 249.29295\n",
      "Epoch 534/600\n",
      "\n",
      "Epoch 534: val_loss did not improve from 249.29295\n",
      "Epoch 535/600\n",
      "\n",
      "Epoch 535: val_loss did not improve from 249.29295\n",
      "Epoch 536/600\n",
      "\n",
      "Epoch 536: val_loss did not improve from 249.29295\n",
      "Epoch 537/600\n",
      "\n",
      "Epoch 537: val_loss did not improve from 249.29295\n",
      "Epoch 538/600\n",
      "\n",
      "Epoch 538: val_loss did not improve from 249.29295\n",
      "Epoch 539/600\n",
      "\n",
      "Epoch 539: val_loss did not improve from 249.29295\n",
      "Epoch 540/600\n",
      "\n",
      "Epoch 540: val_loss did not improve from 249.29295\n",
      "Epoch 541/600\n",
      "\n",
      "Epoch 541: val_loss did not improve from 249.29295\n",
      "Epoch 542/600\n",
      "\n",
      "Epoch 542: val_loss did not improve from 249.29295\n",
      "Epoch 543/600\n",
      "\n",
      "Epoch 543: val_loss did not improve from 249.29295\n",
      "Epoch 544/600\n",
      "\n",
      "Epoch 544: val_loss did not improve from 249.29295\n",
      "Epoch 545/600\n",
      "\n",
      "Epoch 545: val_loss did not improve from 249.29295\n",
      "Epoch 546/600\n",
      "\n",
      "Epoch 546: val_loss did not improve from 249.29295\n",
      "Epoch 547/600\n",
      "\n",
      "Epoch 547: val_loss did not improve from 249.29295\n",
      "Epoch 548/600\n",
      "\n",
      "Epoch 548: val_loss did not improve from 249.29295\n",
      "Epoch 549/600\n",
      "\n",
      "Epoch 549: val_loss did not improve from 249.29295\n",
      "Epoch 550/600\n",
      "\n",
      "Epoch 550: val_loss did not improve from 249.29295\n",
      "Epoch 551/600\n",
      "\n",
      "Epoch 551: val_loss did not improve from 249.29295\n",
      "Epoch 552/600\n",
      "\n",
      "Epoch 552: val_loss did not improve from 249.29295\n",
      "Epoch 553/600\n",
      "\n",
      "Epoch 553: val_loss did not improve from 249.29295\n",
      "Epoch 554/600\n",
      "\n",
      "Epoch 554: val_loss did not improve from 249.29295\n",
      "Epoch 555/600\n",
      "\n",
      "Epoch 555: val_loss did not improve from 249.29295\n",
      "Epoch 556/600\n",
      "\n",
      "Epoch 556: val_loss did not improve from 249.29295\n",
      "Epoch 557/600\n",
      "\n",
      "Epoch 557: val_loss did not improve from 249.29295\n",
      "Epoch 558/600\n",
      "\n",
      "Epoch 558: val_loss did not improve from 249.29295\n",
      "Epoch 559/600\n",
      "\n",
      "Epoch 559: val_loss did not improve from 249.29295\n",
      "Epoch 560/600\n",
      "\n",
      "Epoch 560: val_loss did not improve from 249.29295\n",
      "Epoch 561/600\n",
      "\n",
      "Epoch 561: val_loss did not improve from 249.29295\n",
      "Epoch 562/600\n",
      "\n",
      "Epoch 562: val_loss did not improve from 249.29295\n",
      "Epoch 563/600\n",
      "\n",
      "Epoch 563: val_loss did not improve from 249.29295\n",
      "Epoch 564/600\n",
      "\n",
      "Epoch 564: val_loss did not improve from 249.29295\n",
      "Epoch 565/600\n",
      "\n",
      "Epoch 565: val_loss did not improve from 249.29295\n",
      "Epoch 566/600\n",
      "\n",
      "Epoch 566: val_loss did not improve from 249.29295\n",
      "Epoch 567/600\n",
      "\n",
      "Epoch 567: val_loss did not improve from 249.29295\n",
      "Epoch 568/600\n",
      "\n",
      "Epoch 568: val_loss did not improve from 249.29295\n",
      "Epoch 569/600\n",
      "\n",
      "Epoch 569: val_loss did not improve from 249.29295\n",
      "Epoch 570/600\n",
      "\n",
      "Epoch 570: val_loss did not improve from 249.29295\n",
      "Epoch 571/600\n",
      "\n",
      "Epoch 571: val_loss did not improve from 249.29295\n",
      "Epoch 572/600\n",
      "\n",
      "Epoch 572: val_loss did not improve from 249.29295\n",
      "Epoch 573/600\n",
      "\n",
      "Epoch 573: val_loss did not improve from 249.29295\n",
      "Epoch 574/600\n",
      "\n",
      "Epoch 574: val_loss did not improve from 249.29295\n",
      "Epoch 575/600\n",
      "\n",
      "Epoch 575: val_loss did not improve from 249.29295\n",
      "Epoch 576/600\n",
      "\n",
      "Epoch 576: val_loss did not improve from 249.29295\n",
      "Epoch 577/600\n",
      "\n",
      "Epoch 577: val_loss did not improve from 249.29295\n",
      "Epoch 578/600\n",
      "\n",
      "Epoch 578: val_loss did not improve from 249.29295\n",
      "Epoch 579/600\n",
      "\n",
      "Epoch 579: val_loss did not improve from 249.29295\n",
      "Epoch 580/600\n",
      "\n",
      "Epoch 580: val_loss did not improve from 249.29295\n",
      "Epoch 581/600\n",
      "\n",
      "Epoch 581: val_loss did not improve from 249.29295\n",
      "Epoch 582/600\n",
      "\n",
      "Epoch 582: val_loss did not improve from 249.29295\n",
      "Epoch 583/600\n",
      "\n",
      "Epoch 583: val_loss did not improve from 249.29295\n",
      "Epoch 584/600\n",
      "\n",
      "Epoch 584: val_loss did not improve from 249.29295\n",
      "Epoch 585/600\n",
      "\n",
      "Epoch 585: val_loss did not improve from 249.29295\n",
      "Epoch 586/600\n",
      "\n",
      "Epoch 586: val_loss did not improve from 249.29295\n",
      "Epoch 587/600\n",
      "\n",
      "Epoch 587: val_loss did not improve from 249.29295\n",
      "Epoch 588/600\n",
      "\n",
      "Epoch 588: val_loss did not improve from 249.29295\n",
      "Epoch 589/600\n",
      "\n",
      "Epoch 589: val_loss did not improve from 249.29295\n",
      "Epoch 590/600\n",
      "\n",
      "Epoch 590: val_loss did not improve from 249.29295\n",
      "Epoch 591/600\n",
      "\n",
      "Epoch 591: val_loss did not improve from 249.29295\n",
      "Epoch 592/600\n",
      "\n",
      "Epoch 592: val_loss did not improve from 249.29295\n",
      "Epoch 593/600\n",
      "\n",
      "Epoch 593: val_loss did not improve from 249.29295\n",
      "Epoch 594/600\n",
      "\n",
      "Epoch 594: val_loss did not improve from 249.29295\n",
      "Epoch 595/600\n",
      "\n",
      "Epoch 595: val_loss did not improve from 249.29295\n",
      "Epoch 596/600\n",
      "\n",
      "Epoch 596: val_loss did not improve from 249.29295\n",
      "Epoch 597/600\n",
      "\n",
      "Epoch 597: val_loss did not improve from 249.29295\n",
      "Epoch 598/600\n",
      "\n",
      "Epoch 598: val_loss did not improve from 249.29295\n",
      "Epoch 599/600\n",
      "\n",
      "Epoch 599: val_loss did not improve from 249.29295\n",
      "Epoch 600/600\n",
      "\n",
      "Epoch 600: val_loss did not improve from 249.29295\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.929, rmsd=6.552, mse=42.923, mae=5.087\n",
      "dS: r2=0.916, rmsd=19.122, mse=365.668, mae=15.141\n",
      "dG: r2=0.952, rmsd=0.962, mse=0.926, mae=0.691\n",
      "Tm: r2=0.901, rmsd=4.263, mse=18.173, mae=3.169\n"
     ]
    }
   ],
   "source": [
    "finger = 2\n",
    "for finger in [4,3,1,2]:\n",
    "    for layer in model2.layers:\n",
    "        layer.trainable=False\n",
    "        print(layer.name)\n",
    "        if layer.name in [f'node{finger}_1',f'node{finger}_2',f'node{finger}_3',prop[finger-1]]:#,prop[finger-1]]:#\n",
    "            layer.trainable=True\n",
    "            print(layer.name, layer)\n",
    "        print(\"weights:\", len(layer.weights))\n",
    "        print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "        print(\"non_trainable_weights:\", len(layer.non_trainable_weights))\n",
    "        # print(layer.trainable_weights)\n",
    "\n",
    "    #     COMPILE MODEl\n",
    "    model2.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "      \n",
    "    history = model2.fit(X_padded_train, [y_1_train, y_2_train, y_3_train, y_4_train],\n",
    "                            epochs=600,\n",
    "                            batch_size=16, \n",
    "                            validation_data=(X_padded_val, [y_1_val,   y_2_val,   y_3_val,   y_4_val]),\n",
    "                            verbose = 3,\n",
    "                            callbacks=keras_callbacks)\n",
    "    \n",
    "    y_test_pred = model2.predict(X_padded_test)\n",
    "    # y_test_pred\n",
    "    with open(\"Results_output_weights_freeze_compile.txt\", \"a\") as myfile:\n",
    "        myfile.write(f\"appended text finger: {finger}\\n\")\n",
    "        for n in range(4):\n",
    "            r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "            print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "            myfile.write(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}\\n')\n",
    "\n",
    "    if finger == 1:\n",
    "        post_1 = model2.weights\n",
    "    elif finger == 2:\n",
    "        post_2 = model2.weights\n",
    "    elif finger == 3:\n",
    "        post_3 = model2.weights\n",
    "    else:\n",
    "        post_4 = model2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6497b15-898b-482e-9dfc-10d8b80ba6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.896, rmsd=7.945, mse=63.126, mae=6.136\n",
      "dS: r2=0.880, rmsd=22.845, mse=521.878, mae=17.711\n",
      "dG: r2=0.919, rmsd=1.254, mse=1.572, mae=0.900\n",
      "Tm: r2=0.885, rmsd=4.581, mse=20.987, mae=3.574\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fbfb3609-c613-4892-88d4-f77b6b93fcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for layer in model2.layers:\n",
    "    print(layer.trainable)    \n",
    "    # layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0e28a48-42be-41c8-a70e-7383ab680394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       "array([[ 2.38277435e-01, -1.34514257e-01,  5.89380302e-02,\n",
       "        -1.66284069e-01, -1.59468248e-01, -1.15931988e-01,\n",
       "         2.90592134e-01,  1.59710835e-04, -2.14988720e-02,\n",
       "        -1.78132251e-01, -1.21865205e-01,  1.03081159e-01,\n",
       "        -1.85814090e-02,  2.90744249e-02,  5.21656796e-02,\n",
       "        -1.40717059e-01,  1.44536540e-01,  2.21545741e-01,\n",
       "         1.65588632e-01, -9.81609896e-03, -2.37246081e-01,\n",
       "         1.34280017e-02,  2.63009191e-01,  7.97763094e-03,\n",
       "        -5.77026643e-02,  2.17137203e-01, -1.62514634e-02,\n",
       "        -1.06312990e-01, -1.71949372e-01, -1.35731816e-01,\n",
       "         7.32536465e-02, -8.63921493e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [prior, post_1, post_2, post_3, post_4]\n",
    "\n",
    "\n",
    "# prior[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bf85b546-6a4a-419f-b1e0-9a7174795f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "post1= tf.Tensor(\n",
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]], shape=(3, 1, 32), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]], shape=(3, 1, 32), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]], shape=(3, 1, 32), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True  True  True  True  True  True]]], shape=(3, 1, 32), dtype=bool)\n",
      "1 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "2 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "3 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "4 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "5 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "6 \n",
      "post1= tf.Tensor(\n",
      "[[[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]], shape=(3, 32, 32), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]], shape=(3, 32, 32), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]], shape=(3, 32, 32), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]\n",
      "\n",
      " [[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]], shape=(3, 32, 32), dtype=bool)\n",
      "7 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "8 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "9 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "10 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "11 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "12 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool)\n",
      "13 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool)\n",
      "14 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool)\n",
      "15 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool)\n",
      "16 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool)\n",
      "17 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool)\n",
      "18 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(480, 16), dtype=bool)\n",
      "19 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool)\n",
      "20 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool)\n",
      "21 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "22 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool)\n",
      "23 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "24 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool)\n",
      "25 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "26 \n",
      "post1= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool)\n",
      "27 \n",
      "post1= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n",
      "28 \n",
      "post1= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool)\n",
      "29 \n",
      "post1= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post2= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post3= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post4= tf.Tensor([ True], shape=(1,), dtype=bool)\n",
      "30 \n",
      "post1= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool)\n",
      "31 \n",
      "post1= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post2= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post3= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post4= tf.Tensor([ True], shape=(1,), dtype=bool)\n",
      "32 \n",
      "post1= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool)\n",
      "33 \n",
      "post1= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post2= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post3= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post4= tf.Tensor([ True], shape=(1,), dtype=bool)\n",
      "34 \n",
      "post1= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post2= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post3= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool) \n",
      "post4= tf.Tensor(\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]], shape=(32, 1), dtype=bool)\n",
      "35 \n",
      "post1= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post2= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post3= tf.Tensor([ True], shape=(1,), dtype=bool) \n",
      "post4= tf.Tensor([ True], shape=(1,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "len(prior)\n",
    "prior[0][1]\n",
    "for i in range(len(prior)):\n",
    "    # for j in range(len(prior[i])):\n",
    "        # print(i,j,\n",
    "        #       '\\npost1=',prior[i][j]==post_1[i][j],\n",
    "        #       '\\npost2=',prior[i][j]==post_2[i][j],\n",
    "        #       '\\npost3=',prior[i][j]==post_3[i][j],\n",
    "        #       '\\npost4=',prior[i][j]==post_4[i][j])\n",
    "    print(i,\n",
    "              '\\npost1=',prior[i]==post_1[i],\n",
    "              '\\npost2=',prior[i]==post_2[i],\n",
    "              '\\npost3=',prior[i]==post_3[i],\n",
    "              '\\npost4=',prior[i]==post_4[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af283f11-dc27-48a9-843e-925fa3648a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "import graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff2894be-11a1-40d7-928b-12fa1f96deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "613dcf1e-6418-4fd0-b7ca-f160064740a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model2, to_file = 'model3.png',\n",
    "          show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=200,\n",
    "    show_layer_activations=True,\n",
    "    show_trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b956e13a-d1a2-43b9-aa54-77a4f08a15c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x14a818072910>,\n",
       " <keras.layers.convolutional.conv1d.Conv1D at 0x14a6d819b550>,\n",
       " <keras.layers.pooling.max_pooling1d.MaxPooling1D at 0x14aae01ab280>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14aae01ab6a0>,\n",
       " <keras.layers.convolutional.conv1d.Conv1D at 0x14a82318d310>,\n",
       " <keras.layers.pooling.max_pooling1d.MaxPooling1D at 0x14a823185b50>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14a823193eb0>,\n",
       " <keras.layers.convolutional.conv1d.Conv1D at 0x14a823185520>,\n",
       " <keras.layers.pooling.max_pooling1d.MaxPooling1D at 0x14a823193700>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14a8231a4fd0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x14a8231abeb0>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8231a4340>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8231b4850>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fcd460>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fda400>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8231ab790>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fbcfa0>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8231a47c0>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fc1e50>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8231abbb0>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fbc5e0>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fcd040>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fc1e80>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fe3280>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fe3a30>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0fd3f70>,\n",
       " <keras.layers.core.dense.Dense at 0x14a8b0ff1220>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fc1654f-e798-488a-b829-d5dcf8f2165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /users/qdb16186/.conda/envs/tf/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /users/qdb16186/.conda/envs/tf/lib/python3.9/site-packages (from pydot) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b7dc3bc-9059-4d12-9722-aa0d2bcaa518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /users/qdb16186/.conda/envs/tf/lib/python3.9/site-packages (0.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f017cee-604b-494a-a7fd-94833773cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install graphviz\n",
    "!y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7fa51a32-005d-4f1c-a8f4-377ca8382ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /opt/software/anaconda/python-3.9.7/2021.11\n",
      "test                     /opt/software/anaconda/python-3.9.7/2021.11/envs/test\n",
      "tf                    *  /users/qdb16186/.conda/envs/tf\n",
      "tf2                      /users/qdb16186/.conda/envs/tf2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee554a7a-b532-4240-8dfa-f680c2739492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydotplus\n",
      "  Downloading pydotplus-2.0.2.tar.gz (278 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m\u001b[0m \u001b[32m278.7/278.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /users/qdb16186/.conda/envs/tf/lib/python3.9/site-packages (from pydotplus) (3.0.9)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Building wheel for pydotplus (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pydotplus: filename=pydotplus-2.0.2-py3-none-any.whl size=24552 sha256=7bfd0ff34937f06ba99d4703ef56e8dd160f8f02b6d3b1d0cabeabe04c3914e7\n",
      "  Stored in directory: /users/qdb16186/.cache/pip/wheels/89/e5/de/6966007cf223872eedfbebbe0e074534e72e9128c8fd4b55eb\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e57f927b-5d19-4383-9ad1-37950f095aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /bin/sudo: Permission denied\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2e56450e-b2f4-4e57-aa1d-8e5898eeb246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.get_weights()\n",
    "model2.save_weights(f\"{os.getcwd()}/test/test.cpk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "39b9e450-8eba-41ff-8031-f0a23219c70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x14abc5bcd1c0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=build_model()\n",
    "model3.load_weights(f\"{os.getcwd()}/test/test.cpk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8f3b010c-60eb-41c1-95dd-e24c7d98bb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "dH: r2=0.928, rmsd=6.600, mse=43.563, mae=5.132\n",
      "dS: r2=0.913, rmsd=19.455, mse=378.502, mae=15.228\n",
      "dG: r2=0.950, rmsd=0.988, mse=0.975, mae=0.742\n",
      "Tm: r2=0.870, rmsd=4.874, mse=23.758, mae=3.674\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "28a3c03d-22ee-4c9d-a523-f55b661c5d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x14ac3170d370>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9e887b68-85a0-4aa3-bbb9-b21d7763c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "dH: r2=0.928, rmsd=6.600, mse=43.563, mae=5.132\n",
      "dS: r2=0.913, rmsd=19.455, mse=378.502, mae=15.228\n",
      "dG: r2=0.950, rmsd=0.988, mse=0.975, mae=0.742\n",
      "Tm: r2=0.870, rmsd=4.874, mse=23.758, mae=3.674\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model3.predict(X_padded_test)\n",
    "# y_test_pred\n",
    "prop = ['dH','dS','dG','Tm']\n",
    "for n in range(4):\n",
    "    \n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats_hp(y_test_pred[n], Y_test, prop[n])\n",
    "    print(f'{prop[n]}: r2={r2}, rmsd={rmsd}, mse={mse}, mae={mae}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dd38d5f2-5753-410c-abc2-a953e70140a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "50e2491c-6526-4ba0-a726-09b3fc63701a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tm'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b6ef9774-3cc8-4c0a-92c6-03efc48c92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(4):\n",
    "    store_df[f'true_{prop[n]}'] = Y_test[f'{prop[n]}']\n",
    "\n",
    "for n in range(4):\n",
    "    store_df[f'pred_{prop[n]}'] = y_test_pred[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "961ff9bf-e019-4ec9-8dd1-d0be2ea1f282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_dH</th>\n",
       "      <th>true_dS</th>\n",
       "      <th>true_dG</th>\n",
       "      <th>true_Tm</th>\n",
       "      <th>pred_dH</th>\n",
       "      <th>pred_dS</th>\n",
       "      <th>pred_dG</th>\n",
       "      <th>pred_Tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-52.4</td>\n",
       "      <td>-146.0</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>32.2</td>\n",
       "      <td>-58.497154</td>\n",
       "      <td>-163.608185</td>\n",
       "      <td>-7.701569</td>\n",
       "      <td>37.141209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-50.2</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-55.096191</td>\n",
       "      <td>-156.436844</td>\n",
       "      <td>-6.426764</td>\n",
       "      <td>32.363731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-56.2</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-58.043678</td>\n",
       "      <td>-162.015137</td>\n",
       "      <td>-7.913347</td>\n",
       "      <td>37.387241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-47.2</td>\n",
       "      <td>-128.0</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>34.1</td>\n",
       "      <td>-55.865150</td>\n",
       "      <td>-158.537445</td>\n",
       "      <td>-6.663093</td>\n",
       "      <td>32.498135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-49.8</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-59.314575</td>\n",
       "      <td>-165.573303</td>\n",
       "      <td>-8.165701</td>\n",
       "      <td>38.538620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-123.9</td>\n",
       "      <td>-335.0</td>\n",
       "      <td>-19.9</td>\n",
       "      <td>70.4</td>\n",
       "      <td>-111.313019</td>\n",
       "      <td>-301.635040</td>\n",
       "      <td>-17.727169</td>\n",
       "      <td>67.300087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-62.0</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-60.921051</td>\n",
       "      <td>-168.633911</td>\n",
       "      <td>-8.763830</td>\n",
       "      <td>40.619907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-68.0</td>\n",
       "      <td>-196.0</td>\n",
       "      <td>-7.2</td>\n",
       "      <td>33.7</td>\n",
       "      <td>-64.010612</td>\n",
       "      <td>-186.320221</td>\n",
       "      <td>-6.683361</td>\n",
       "      <td>31.028843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-59.0</td>\n",
       "      <td>-172.0</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>25.4</td>\n",
       "      <td>-57.103489</td>\n",
       "      <td>-165.194427</td>\n",
       "      <td>-6.081664</td>\n",
       "      <td>29.231810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-68.4</td>\n",
       "      <td>-193.0</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>39.7</td>\n",
       "      <td>-71.801071</td>\n",
       "      <td>-206.676895</td>\n",
       "      <td>-8.364224</td>\n",
       "      <td>36.988953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-64.5</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-64.564392</td>\n",
       "      <td>-185.724854</td>\n",
       "      <td>-7.489816</td>\n",
       "      <td>34.081341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-70.1</td>\n",
       "      <td>-199.0</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>38.9</td>\n",
       "      <td>-70.913269</td>\n",
       "      <td>-204.703354</td>\n",
       "      <td>-7.955063</td>\n",
       "      <td>35.573608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-62.8</td>\n",
       "      <td>-179.0</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>33.7</td>\n",
       "      <td>-63.440876</td>\n",
       "      <td>-182.972015</td>\n",
       "      <td>-7.080705</td>\n",
       "      <td>32.899208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-75.1</td>\n",
       "      <td>-213.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>41.6</td>\n",
       "      <td>-70.801872</td>\n",
       "      <td>-201.651672</td>\n",
       "      <td>-8.558112</td>\n",
       "      <td>38.473019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-54.2</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>36.1</td>\n",
       "      <td>-58.532021</td>\n",
       "      <td>-163.563736</td>\n",
       "      <td>-7.906748</td>\n",
       "      <td>37.568996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-63.6</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-6.8</td>\n",
       "      <td>31.7</td>\n",
       "      <td>-65.827400</td>\n",
       "      <td>-187.463593</td>\n",
       "      <td>-7.240196</td>\n",
       "      <td>35.598583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-122.9</td>\n",
       "      <td>-336.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>66.7</td>\n",
       "      <td>-111.151344</td>\n",
       "      <td>-301.266235</td>\n",
       "      <td>-17.677299</td>\n",
       "      <td>67.220947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-123.4</td>\n",
       "      <td>-337.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>67.1</td>\n",
       "      <td>-113.750359</td>\n",
       "      <td>-308.746368</td>\n",
       "      <td>-17.930794</td>\n",
       "      <td>67.767136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-63.5</td>\n",
       "      <td>-178.0</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>38.7</td>\n",
       "      <td>-60.622601</td>\n",
       "      <td>-168.034058</td>\n",
       "      <td>-8.623343</td>\n",
       "      <td>40.044483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-116.1</td>\n",
       "      <td>-313.0</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>69.7</td>\n",
       "      <td>-104.185699</td>\n",
       "      <td>-282.121613</td>\n",
       "      <td>-16.470081</td>\n",
       "      <td>63.366470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-66.1</td>\n",
       "      <td>-191.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>33.9</td>\n",
       "      <td>-57.384438</td>\n",
       "      <td>-160.946686</td>\n",
       "      <td>-7.436615</td>\n",
       "      <td>35.926952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-82.5</td>\n",
       "      <td>-227.0</td>\n",
       "      <td>-12.2</td>\n",
       "      <td>53.4</td>\n",
       "      <td>-80.625710</td>\n",
       "      <td>-221.675262</td>\n",
       "      <td>-11.731495</td>\n",
       "      <td>49.845016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-124.8</td>\n",
       "      <td>-341.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>67.2</td>\n",
       "      <td>-109.806770</td>\n",
       "      <td>-298.363983</td>\n",
       "      <td>-17.067305</td>\n",
       "      <td>65.359238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-58.2</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-57.450275</td>\n",
       "      <td>-161.186676</td>\n",
       "      <td>-7.386703</td>\n",
       "      <td>35.746113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-116.3</td>\n",
       "      <td>-317.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>66.3</td>\n",
       "      <td>-113.241165</td>\n",
       "      <td>-307.082062</td>\n",
       "      <td>-18.168957</td>\n",
       "      <td>68.545631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-118.9</td>\n",
       "      <td>-317.0</td>\n",
       "      <td>-20.5</td>\n",
       "      <td>73.9</td>\n",
       "      <td>-114.733192</td>\n",
       "      <td>-309.099182</td>\n",
       "      <td>-18.880049</td>\n",
       "      <td>70.703720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-62.7</td>\n",
       "      <td>-171.0</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>45.7</td>\n",
       "      <td>-62.606537</td>\n",
       "      <td>-172.162796</td>\n",
       "      <td>-9.435804</td>\n",
       "      <td>42.534676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-56.7</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>32.3</td>\n",
       "      <td>-47.613434</td>\n",
       "      <td>-131.538544</td>\n",
       "      <td>-6.880133</td>\n",
       "      <td>25.964348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-45.7</td>\n",
       "      <td>-129.0</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-44.994301</td>\n",
       "      <td>-126.074615</td>\n",
       "      <td>-5.909400</td>\n",
       "      <td>21.509151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-59.7</td>\n",
       "      <td>-165.0</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-61.634613</td>\n",
       "      <td>-170.814163</td>\n",
       "      <td>-8.868538</td>\n",
       "      <td>39.992115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-61.6</td>\n",
       "      <td>-167.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>46.6</td>\n",
       "      <td>-60.907104</td>\n",
       "      <td>-167.437134</td>\n",
       "      <td>-9.171438</td>\n",
       "      <td>42.072407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-67.7</td>\n",
       "      <td>-187.0</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>45.2</td>\n",
       "      <td>-61.127167</td>\n",
       "      <td>-168.069427</td>\n",
       "      <td>-9.197507</td>\n",
       "      <td>42.090233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-64.7</td>\n",
       "      <td>-182.0</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>38.5</td>\n",
       "      <td>-61.072769</td>\n",
       "      <td>-168.881744</td>\n",
       "      <td>-8.819872</td>\n",
       "      <td>40.002148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-57.4</td>\n",
       "      <td>-162.0</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-58.316048</td>\n",
       "      <td>-163.288391</td>\n",
       "      <td>-7.818919</td>\n",
       "      <td>37.162674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>-325.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>67.9</td>\n",
       "      <td>-111.757805</td>\n",
       "      <td>-302.080170</td>\n",
       "      <td>-17.943846</td>\n",
       "      <td>67.801407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-64.9</td>\n",
       "      <td>-184.0</td>\n",
       "      <td>-7.8</td>\n",
       "      <td>36.4</td>\n",
       "      <td>-56.941467</td>\n",
       "      <td>-161.184174</td>\n",
       "      <td>-7.126483</td>\n",
       "      <td>33.353165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-66.2</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-58.821754</td>\n",
       "      <td>-164.440033</td>\n",
       "      <td>-8.073500</td>\n",
       "      <td>37.369305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-70.3</td>\n",
       "      <td>-192.0</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>49.9</td>\n",
       "      <td>-60.219875</td>\n",
       "      <td>-166.721344</td>\n",
       "      <td>-8.777602</td>\n",
       "      <td>40.443493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-42.2</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>-45.248188</td>\n",
       "      <td>-126.586639</td>\n",
       "      <td>-6.023583</td>\n",
       "      <td>21.978268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-58.9</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>-59.191048</td>\n",
       "      <td>-164.863281</td>\n",
       "      <td>-8.164048</td>\n",
       "      <td>38.461189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-45.2</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>-8.9</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-48.518860</td>\n",
       "      <td>-133.079681</td>\n",
       "      <td>-7.261748</td>\n",
       "      <td>29.492060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-57.5</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>-7.8</td>\n",
       "      <td>36.6</td>\n",
       "      <td>-60.565807</td>\n",
       "      <td>-167.775894</td>\n",
       "      <td>-8.635214</td>\n",
       "      <td>40.224133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-76.8</td>\n",
       "      <td>-214.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>-62.550472</td>\n",
       "      <td>-171.833527</td>\n",
       "      <td>-9.446918</td>\n",
       "      <td>42.795464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-37.5</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>25.3</td>\n",
       "      <td>-48.187538</td>\n",
       "      <td>-133.222504</td>\n",
       "      <td>-7.059307</td>\n",
       "      <td>27.468390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-74.1</td>\n",
       "      <td>-201.0</td>\n",
       "      <td>-11.8</td>\n",
       "      <td>53.8</td>\n",
       "      <td>-70.806229</td>\n",
       "      <td>-199.454987</td>\n",
       "      <td>-9.270905</td>\n",
       "      <td>41.081924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     true_dH  true_dS  true_dG  true_Tm     pred_dH     pred_dS    pred_dG  \\\n",
       "7      -52.4   -146.0     -7.1     32.2  -58.497154 -163.608185  -7.701569   \n",
       "9      -50.2   -143.0     -6.0     27.0  -55.096191 -156.436844  -6.426764   \n",
       "17     -56.2   -151.0     -9.3     45.0  -58.043678 -162.015137  -7.913347   \n",
       "21     -47.2   -128.0     -7.5     34.1  -55.865150 -158.537445  -6.663093   \n",
       "22     -49.8   -134.0     -8.2     41.5  -59.314575 -165.573303  -8.165701   \n",
       "24    -123.9   -335.0    -19.9     70.4 -111.313019 -301.635040 -17.727169   \n",
       "42     -62.0   -170.0     -9.4     46.0  -60.921051 -168.633911  -8.763830   \n",
       "49     -68.0   -196.0     -7.2     33.7  -64.010612 -186.320221  -6.683361   \n",
       "50     -59.0   -172.0     -5.7     25.4  -57.103489 -165.194427  -6.081664   \n",
       "52     -68.4   -193.0     -8.6     39.7  -71.801071 -206.676895  -8.364224   \n",
       "56     -64.5   -183.0     -7.7     36.0  -64.564392 -185.724854  -7.489816   \n",
       "60     -70.1   -199.0     -8.4     38.9  -70.913269 -204.703354  -7.955063   \n",
       "61     -62.8   -179.0     -7.3     33.7  -63.440876 -182.972015  -7.080705   \n",
       "68     -75.1   -213.0     -9.0     41.6  -70.801872 -201.651672  -8.558112   \n",
       "74     -54.2   -151.0     -7.4     36.1  -58.532021 -163.563736  -7.906748   \n",
       "76     -63.6   -183.0     -6.8     31.7  -65.827400 -187.463593  -7.240196   \n",
       "79    -122.9   -336.0    -18.7     66.7 -111.151344 -301.266235 -17.677299   \n",
       "81    -123.4   -337.0    -18.8     67.1 -113.750359 -308.746368 -17.930794   \n",
       "85     -63.5   -178.0     -8.3     38.7  -60.622601 -168.034058  -8.623343   \n",
       "86    -116.1   -313.0    -19.1     69.7 -104.185699 -282.121613 -16.470081   \n",
       "92     -66.1   -191.0     -7.0     33.9  -57.384438 -160.946686  -7.436615   \n",
       "95     -82.5   -227.0    -12.2     53.4  -80.625710 -221.675262 -11.731495   \n",
       "101   -124.8   -341.0    -19.0     67.2 -109.806770 -298.363983 -17.067305   \n",
       "102    -58.2   -164.0     -7.5     36.0  -57.450275 -161.186676  -7.386703   \n",
       "111   -116.3   -317.0    -18.0     66.3 -113.241165 -307.082062 -18.168957   \n",
       "113   -118.9   -317.0    -20.5     73.9 -114.733192 -309.099182 -18.880049   \n",
       "115    -62.7   -171.0     -9.6     45.7  -62.606537 -172.162796  -9.435804   \n",
       "121    -56.7   -160.0     -7.1     32.3  -47.613434 -131.538544  -6.880133   \n",
       "123    -45.7   -129.0     -5.8     25.0  -44.994301 -126.074615  -5.909400   \n",
       "129    -59.7   -165.0     -8.4     40.0  -61.634613 -170.814163  -8.868538   \n",
       "137    -61.6   -167.0     -9.8     46.6  -60.907104 -167.437134  -9.171438   \n",
       "138    -67.7   -187.0     -9.7     45.2  -61.127167 -168.069427  -9.197507   \n",
       "141    -64.7   -182.0     -8.3     38.5  -61.072769 -168.881744  -8.819872   \n",
       "146    -57.4   -162.0     -7.3     35.0  -58.316048 -163.288391  -7.818919   \n",
       "150   -119.6   -325.0    -18.7     67.9 -111.757805 -302.080170 -17.943846   \n",
       "163    -64.9   -184.0     -7.8     36.4  -56.941467 -161.184174  -7.126483   \n",
       "164    -66.2   -183.0     -9.3     44.2  -58.821754 -164.440033  -8.073500   \n",
       "172    -70.3   -192.0    -10.7     49.9  -60.219875 -166.721344  -8.777602   \n",
       "179    -42.2   -117.0     -6.0     25.6  -45.248188 -126.586639  -6.023583   \n",
       "186    -58.9   -164.0     -8.0     37.5  -59.191048 -164.863281  -8.164048   \n",
       "195    -45.2   -117.0     -8.9     46.8  -48.518860 -133.079681  -7.261748   \n",
       "198    -57.5   -160.0     -7.8     36.6  -60.565807 -167.775894  -8.635214   \n",
       "199    -76.8   -214.0    -10.3     47.3  -62.550472 -171.833527  -9.446918   \n",
       "200    -37.5   -100.0     -6.5     25.3  -48.187538 -133.222504  -7.059307   \n",
       "203    -74.1   -201.0    -11.8     53.8  -70.806229 -199.454987  -9.270905   \n",
       "\n",
       "       pred_Tm  \n",
       "7    37.141209  \n",
       "9    32.363731  \n",
       "17   37.387241  \n",
       "21   32.498135  \n",
       "22   38.538620  \n",
       "24   67.300087  \n",
       "42   40.619907  \n",
       "49   31.028843  \n",
       "50   29.231810  \n",
       "52   36.988953  \n",
       "56   34.081341  \n",
       "60   35.573608  \n",
       "61   32.899208  \n",
       "68   38.473019  \n",
       "74   37.568996  \n",
       "76   35.598583  \n",
       "79   67.220947  \n",
       "81   67.767136  \n",
       "85   40.044483  \n",
       "86   63.366470  \n",
       "92   35.926952  \n",
       "95   49.845016  \n",
       "101  65.359238  \n",
       "102  35.746113  \n",
       "111  68.545631  \n",
       "113  70.703720  \n",
       "115  42.534676  \n",
       "121  25.964348  \n",
       "123  21.509151  \n",
       "129  39.992115  \n",
       "137  42.072407  \n",
       "138  42.090233  \n",
       "141  40.002148  \n",
       "146  37.162674  \n",
       "150  67.801407  \n",
       "163  33.353165  \n",
       "164  37.369305  \n",
       "172  40.443493  \n",
       "179  21.978268  \n",
       "186  38.461189  \n",
       "195  29.492060  \n",
       "198  40.224133  \n",
       "199  42.795464  \n",
       "200  27.468390  \n",
       "203  41.081924  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ceadb-dea4-4c84-968a-069f6be68836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
