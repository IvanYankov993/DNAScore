{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7a0c4191-29a5-489c-b5b5-04a4c362a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8610b8-1ac7-4447-a75f-e46ee7fafa33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2890200c-61ee-4242-bb7d-425a020d6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir='CNN_multitask_grid'\n",
    "os.chdir('/users/qdb16186')\n",
    "path=os.getcwd()\n",
    "abs_dir=path+\"/\"+workdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4ca3a-4720-4cf6-9249-a2cfe078557f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bash_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf25f0f-7096-4366-89f2-f690d6d8c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_script_gen(abs_dir):\n",
    "    dir=abs_dir\n",
    "    cores=15\n",
    "    HH=48\n",
    "    job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/sbatch_.sh','w') as f:\n",
    "        f.write(f\"\"\"#!/bin/bash\n",
    "\n",
    "#======================================================\n",
    "#\n",
    "# Job script for running on a single node\n",
    "#\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# Propogate environment variables to the compute node\n",
    "#SBATCH --export=ALL\n",
    "#\n",
    "# Run in the standard partition (queue)\n",
    "#SBATCH --partition=standard\n",
    "#\n",
    "# Specify project account\n",
    "#SBATCH --account=palmer-addmd\n",
    "#\n",
    "# No. of tasks required\n",
    "#SBATCH --ntasks=1 --cpus-per-task={cores}\n",
    "#\n",
    "# Distribute processes in round-robin fashion for load balancing\n",
    "#SBATCH --distribution=cyclic\n",
    "#\n",
    "#\n",
    "# Specify (hard) runtime (HH:MM:SS)\n",
    "#SBATCH --time={HH}:00:00\n",
    "#\n",
    "# Job name\n",
    "#SBATCH --job-name={job_name}\n",
    "#\n",
    "# Output file\n",
    "#SBATCH --output={job_name}_slurm-%j.out\n",
    "#======================================================\n",
    "\n",
    "module purge\n",
    "module load anaconda/python-3.9.7/2021.11\n",
    "source activate tf\n",
    "module purge\n",
    "\n",
    "#======================================================\n",
    "# Prologue script to record job details\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_prologue.sh\n",
    "#------------------------------------------------------\n",
    "\n",
    "python {python_script}\n",
    "#mpirun -np $SLURM_NTASKS namd2 HEWL_002M_D2.inp > HEWL_002M_D2.out.$SLURM_JOB_ID \n",
    "\n",
    "#======================================================\n",
    "# Epilogue script to record job endtime and runtime\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_epilogue.sh\n",
    "#------------------------------------------------------\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d8718d-a054-410e-a4f3-b314cff72172",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script_gen(abs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d233b-bfea-4510-aff6-5135859f64d5",
   "metadata": {},
   "source": [
    "# Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e720b1-dc4b-49ae-94b5-d11b9b0440c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d14fe077-98fe-4f83-acab-8a956e9226f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_script(abs_dir):\n",
    "    dir=abs_dir\n",
    "    cores=15\n",
    "    HH=48\n",
    "    job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/python_script.py','w') as f:\n",
    "        f.write(\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# sklearn inports\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "# import pickle as pk\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, X = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep\n",
    "\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    y_outputs['y_true'] = true\n",
    "    y_outputs['y_pred'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(dense1,dense2,dense3,convn,**kwargs):\n",
    "    \n",
    "    print(\"dense1: \", dense1)\n",
    "    \n",
    "    print(\"dense2: \", dense2)\n",
    "    \n",
    "    print(\"dense3: \", dense3)\n",
    "\n",
    "    print(\"convn: \", convn)\n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    if convn > 0:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            input_shape=(250,1),\n",
    "                            name = 'conv1d_1'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x)\n",
    "        x = keras.layers.BatchNormalization(name = 'batchnorm_1')(x)\n",
    "    \n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    if convn > 1:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_2'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_2')(x)   \n",
    "    if convn > 2:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_3'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_3')(x)            \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    if convn > -1:\n",
    "        x = keras.layers.Flatten(name = 'flatten')(x)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    if dense1 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                    dense1,\n",
    "                    activation='relu',\n",
    "                    use_bias=True,\n",
    "                    # name='layer_1',\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=None,\n",
    "                    bias_regularizer=None,\n",
    "                    activity_regularizer=None,\n",
    "                    kernel_constraint=None,\n",
    "                    bias_constraint=None\n",
    "                )(x)\n",
    "    if dense2 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "    if dense3 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "#     OUTPUT LAYERS\n",
    "    output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x)\n",
    "\n",
    "    # model = Model(inputs=inputs, outputs=[output_1, output_2, output_3])\n",
    "    model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS ADAPTIVE LEARNING RATE   \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "### MODEL END Start Workflow\n",
    "\n",
    "### Keras Model REgressor Ready for Grid Search\n",
    "def model_for_grid(resultdir,resample,epochs,**kwargs):\n",
    "#Needs variable epochs \n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min', \n",
    "                       verbose=1, \n",
    "                       patience=2000, \n",
    "                       restore_best_weights=True)\n",
    "    # CSV Logger\n",
    "    csv_logger = CSVLogger(f\"{resultdir}/csv_logger/model_history_log_resample_{resample}.csv\", \n",
    "                           append=True)\n",
    "    # # CP_callbacks\n",
    "    # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "    #                                                  save_weights_only=True,\n",
    "    #                                                  verbose=1)\n",
    "    \n",
    "    # TensorBoard\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{resultdir}/tensorboard_logs/{resample}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                          update_freq = 1,\n",
    "                                                          # histogram_freq=1, \n",
    "                                                          write_graph=False, \n",
    "                                                          write_images=False)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    \n",
    "    # Covert to list and provide to Keras Regressor\n",
    "    keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "        \n",
    "    ###initialise model\n",
    "    model = KerasRegressor(model=build_model, \n",
    "                           batch=32,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_split=0.2,\n",
    "                           callbacks=keras_callbacks,\n",
    "                           dense1=1,\n",
    "                            dense2=1,\n",
    "                            dense3=1,\n",
    "                           convn=1\n",
    "                          )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "### Input Parameters and Workflow\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\n",
    "prop='Granulated'\n",
    "# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=50\n",
    "n_folds=5\n",
    "n_jobs=1\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(n_folds, test_size=0.3, random_state=1)\n",
    "\n",
    "###define scoring dict for cv\n",
    "scorers = {\n",
    "    'r2':make_scorer(r2_func), \n",
    "    'rmse':make_scorer(rmse_func, greater_is_better=False), \n",
    "    'bias':make_scorer(bias_func, greater_is_better=False), \n",
    "    'sdep':make_scorer(sdep_func, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1   \n",
    "\n",
    "    \n",
    "    for prop in desc_type:\n",
    "    #     adjust y[2] to * Temperature /1000 dS*T kcal/mol\n",
    "        #for CNN with padding\n",
    "        x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "\n",
    "\n",
    "        i=-1\n",
    "        for GSHT in GSHT_list:\n",
    "            i+=1\n",
    "\n",
    "            ### DEFINE MODEL\n",
    "            model_name = f'CNN_single_task_grid_{grid_number}'\n",
    "            \n",
    "            path=\"{}/CV/{}/{}/{}/{}\".format(os.getcwd(),resample,model_name,prop,GSHT)\n",
    "            \n",
    "            model = model_for_grid(path,resample,epochs) #resultdir\n",
    "            \n",
    "            create_dir(home,resample,model_name,prop,GSHT)\n",
    "            \n",
    "            ### Grid            \n",
    "            pipe_cond='No_scalling'\n",
    "            for pipe_cond in ['No_scalling']:#,'Scalling']: \n",
    "                # Until we find a way to normalise the data in this pipeline. \n",
    "                # We can do it with hyper_tunner \n",
    "                # NB! Do not forget to adjust the keras callbacks becasue they will overwrite No_scalling outputs\n",
    "                if pipe_cond==\"Scalling\":        \n",
    "                    ### PIPE\n",
    "                    # Define inputs for pipe\n",
    "                    scaler = StandardScaler()\n",
    "                    pipe = Pipeline(steps=[(\"scaler\", scaler), (f\"{model_name}\",model)])\n",
    "            \n",
    "                     ###parameter grid\n",
    "                    param_grid_model = {\n",
    "                                     f\"{model_name}__dense1\":[8,16,32],\n",
    "                                     f\"{model_name}__dense2\":[8,16,32],\n",
    "                                     f\"{model_name}_dense3\":[8,16,32],\n",
    "                                        f\"{model_name}__convn\":[0]}\n",
    "                    \n",
    "                    ###create CV using sklearn.GridSearchCV\n",
    "                    grid = GridSearchCV(\n",
    "                        estimator=pipe, \n",
    "                        param_grid=param_grid_model,\n",
    "                        n_jobs=n_jobs, \n",
    "                        cv=n_folds, \n",
    "                        refit='rmse', \n",
    "                        scoring=scorers, \n",
    "                        return_train_score=True,\n",
    "                        )\n",
    "        \n",
    "                else:\n",
    "                    ###parameter grid\n",
    "                    param_grid_model = {                     \n",
    "                                 \"dense1\":[32],\n",
    "                                 \"dense2\":[32],\n",
    "                                 \"dense3\":[8,32],\n",
    "                                 'convn':[3]}\n",
    "                    \n",
    "                    ###create CV using sklearn.GridSearchCV\n",
    "                    grid = GridSearchCV(\n",
    "                        estimator=model,\n",
    "                        param_grid=param_grid_model,\n",
    "                        n_jobs=n_jobs, \n",
    "                        cv=n_folds, \n",
    "                        refit='rmse', \n",
    "                        scoring=scorers, \n",
    "                        return_train_score=True,\n",
    "                        )\n",
    "    \n",
    "        ########### Fid Model with Scikeras and scikit learn.\n",
    "                history = grid.fit(x_train, y_train[i])\n",
    "       ############ Store results\n",
    "    \n",
    "                results=pd.DataFrame(history.cv_results_)\n",
    "                results.to_csv(path+f\"/gridsearch_resample_{resample}_pipe_cond_{pipe_cond}.csv\")\n",
    "    \n",
    "                y_pred_test=history.predict(x_test)\n",
    "                y_pred_train=history.predict(x_train)\n",
    "    \n",
    "                save_splits(train_idx,y_train[i],y_pred_train,resample,path,f\"train_pipe_cond_{pipe_cond}\")\n",
    "                save_splits(test_idx,y_test[i],y_pred_test,resample,path,f\"test_pipe_cond_{pipe_cond}\")\n",
    "        \n",
    "# total_time=time.time()-time_start\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9750898-9227-45f7-bc02-102ec9b3d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29304407-9f2e-47e6-88ed-6e51a314cbfb",
   "metadata": {},
   "source": [
    "# Python Multitask 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a9fa8-de6a-451e-ba69-70a182ae7d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00962dc-482c-4c25-bf88-fd64e419daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_script(abs_dir,prop,GSHT,cores,job_resample,job_i_fold):\n",
    "    dir=abs_dir\n",
    "    cores=1\n",
    "    job_name=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}\"\n",
    "    python_script=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}.py\"\n",
    "    with open(f'{dir}/{python_script}','w') as f:\n",
    "        f.write(\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow import tensorflow.keras.losses.Reduction\n",
    "# from tensorflow.keras.losses import Loss\n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    x_layer = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, x_layer = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "def wrapped_train_test_split_no_padding(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, x_layer = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return x_train, x_test, df_np(y_train), df_np(y_test)\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT,i_fold):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"fold_{}\".format(i_fold))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"fold_{}\".format(i_fold))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep\n",
    "\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    if len(true)==4:\n",
    "        for i in len(true):\n",
    "            y_outputs[f'y_true_{i}'] = true\n",
    "            y_outputs[f'y_pred_{i}'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def save_splits_hp(idx,true_y,true_x,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    if len(true_y)==4:\n",
    "        for i in range(len(true_y)):\n",
    "            y_outputs[f'y_true_{i}'] = true_y[i]\n",
    "    y_outputs.to_csv(f'{path}/Split_y_{resample}_type_{split_type}.csv', index=False)\n",
    "\n",
    "    x_inputs=pd.DataFrame(true_x)\n",
    "    x_inputs.to_csv(f'{path}/Split_x_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "#     Hyper parameters         \n",
    "    model_type1 = hp.Choice(\"model_type1\", [\"CNN3\",\"CNN2\",\"CNN1\"])\n",
    "    model_type = hp.Choice(\"model_type\", [\"Dense3\"])\n",
    " \n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x_layer=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    # with hp.conditional_scope(\"model_type1\", [\"CNN0\"]):\n",
    "    #         if model_type1 == \"CNN0\":\n",
    "    #             pass\n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    with hp.conditional_scope(\"model_type1\", [\"CNN1\",\"CNN2\"\"CNN3\"]):\n",
    "            if model_type1 != \"CNN0\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        input_shape=(250,1),\n",
    "                        name = 'conv1d_1'\n",
    "                        )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = 'batchnorm_1')(x_layer)\n",
    "                pass\n",
    "                \n",
    "            if model_type1 != \"CNN1\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_2'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_2')(x_layer)\n",
    "\n",
    "            if model_type1 != \"CNN1\" or \"CNN2\":               \n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_3'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_3')(x_layer)\n",
    "                \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    x_layer = keras.layers.Flatten(name = 'flatten')(x_layer)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    # with hp.conditional_scope(\"model_type\", [\"Dense0\"]):\n",
    "    #     if model_type == \"Dense0\":\n",
    "    #         pass\n",
    "            \n",
    "    with hp.conditional_scope(\"model_type\", [\"Dense3\"]): #[\"Dense1\",\"Dense2\",\"Dense3\"]\n",
    "        if model_type != \"Dense0\":\n",
    "            hp_layer_1= hp.Choice(f'layer_1', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_1,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        # name='layer_1',\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "        if model_type != \"Dense1\":\n",
    "            hp_layer_2_2= hp.Choice(f'layer_2_2', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_2_2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "\n",
    "        if model_type != \"Dense1\" or \"Dense2\":\n",
    "            hp_layer_3_3= hp.Choice(f'layer_3_3',  values=[16,32,64])\n",
    "            \n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_3_3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "#     OUTPUT LAYERS\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='melting_temperature')(x_layer)\n",
    "\n",
    "    output_1 = keras.layers.Dense(1, name='dH')(x_layer)\n",
    "    output_2 = keras.layers.Dense(1, name='dS')(x_layer)\n",
    "    output_3 = keras.layers.Dense(1, name='dG')(x_layer)\n",
    "    output_4 = keras.layers.Dense(1, name='Tm')(x_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[output_1, output_2, output_3, output_4])\n",
    "    # model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS\n",
    "#     SETTINGS\n",
    "\n",
    "#     ADAPTIVE LEARNING RATE   \n",
    "    \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "### MODEL END Start Workflow\n",
    "\n",
    "\n",
    "### 2 more FUNCTIONS\n",
    "\n",
    "\n",
    "# def model_for_grid(resultdir,resample,epochs,**kwargs):\n",
    "# #Needs variable epochs \n",
    "#     es = EarlyStopping(monitor='val_loss', \n",
    "#                        mode='min', \n",
    "#                        verbose=1, \n",
    "#                        patience=2000, \n",
    "#                        restore_best_weights=True)\n",
    "#     # CSV Logger\n",
    "#     csv_logger = CSVLogger(f\"{resultdir}/csv_logger/model_history_log_resample_{resample}.csv\", \n",
    "#                            append=True)\n",
    "#     # CP_callbacks\n",
    "#     # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "#     #                                                  save_weights_only=True,\n",
    "#     #                                                  verbose=1)\n",
    "    \n",
    "#     # TensorBoard\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{resultdir}/tensorboard_logs/{resample}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "#                                                           update_freq = 1,\n",
    "#                                                           # histogram_freq=1, \n",
    "#                                                           write_graph=False, \n",
    "#                                                           write_images=False)\n",
    "#     # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    \n",
    "#     # Covert to list and provide to Keras Regressor\n",
    "   \n",
    "#     keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "        \n",
    "#     ###initialise model\n",
    "#     model = KerasRegressor(model=build_model, \n",
    "#                            batch=32,\n",
    "#                             epochs=epochs,\n",
    "#                             verbose=1,\n",
    "#                             validation_split=0.2,\n",
    "#                            callbacks=keras_callbacks,\n",
    "#                            dense1=1,\n",
    "#                             dense2=1,\n",
    "#                             dense3=1,\n",
    "#                            convn=1\n",
    "#                           )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wrapped_train_val_split(hp_train_idx,hp_val_idx,x_train,y_train):\n",
    "    x_hp_train = x_train.iloc[hp_train_idx].to_numpy()\n",
    "    x_hp_val = x_train.iloc[hp_val_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "\n",
    "    y_train_df=[]\n",
    "    for y_i in y_train:\n",
    "       y_train_df.append(pd.DataFrame(y_i))\n",
    "\n",
    "    y_train=y_train_df\n",
    "    y_hp_train = [y_train[0].iloc[hp_train_idx],y_train[1].iloc[hp_train_idx],y_train[2].iloc[hp_train_idx],y_train[3].iloc[hp_train_idx]]\n",
    "    y_hp_val  = [y_train[0].iloc[hp_val_idx] ,y_train[1].iloc[hp_val_idx] ,y_train[2].iloc[hp_val_idx] ,y_train[3].iloc[hp_val_idx]]\n",
    "    \n",
    "    # return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "    return x_hp_train, x_hp_val, df_np(y_hp_train), df_np(y_hp_val)\n",
    "\n",
    "\n",
    "### Input Parameters and Workflow\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"/users/qdb16186/CNN_multitask_grid/Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\"\"\")\n",
    "\n",
    "        f.write(f\"\"\"\n",
    "prop='{prop}'\n",
    "GSHT='dH_dS_dG_Tm' \n",
    "# GSHT={GSHT}\n",
    "\"\"\")\n",
    "\n",
    "        f.write(\"\"\"# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=50\n",
    "n_folds=5\n",
    "n_jobs=1\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(1, test_size=test_frac, random_state=1)\n",
    "\n",
    "###define scoring dict for cv\n",
    "scorers = {\n",
    "    'r2':make_scorer(r2_func), \n",
    "    'rmse':make_scorer(rmse_func, greater_is_better=False), \n",
    "    'bias':make_scorer(bias_func, greater_is_better=False), \n",
    "    'sdep':make_scorer(sdep_func, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1\n",
    "    # add a resample skipper\n",
    "    print(f'resample: {resample}')\n",
    "    \"\"\")\n",
    "        f.write(f\"\"\"\n",
    "    if resample=={job_resample}:\n",
    "\"\"\")\n",
    "        f.write(\"\"\"\n",
    "        print(f'RUN resample: {resample}')\n",
    "    \n",
    "        # ### DEFINE MODEL\n",
    "        model_name = f'{prop}_multi_task_CNN'\n",
    "        \n",
    "        ### CV Train test split\n",
    "        #     adjust y[2] to * Temperature /1000 dS*T kcal/mol\n",
    "        #for CNN with padding\n",
    "        x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "        \n",
    "        # fold splits\n",
    "        kf=KFold(n_splits=n_folds)\n",
    "        for i_fold, (fold_train_index, fold_test_index) in enumerate(kf.split(x_train)):\n",
    "            print('fold: ', i_fold)\n",
    "\n",
    "    \"\"\")\n",
    "        f.write(f\"\"\"\n",
    "            if i_fold=={job_i_fold}:\n",
    "\"\"\")\n",
    "        f.write(\"\"\"\n",
    "                print('RUN fold',i_fold)\n",
    "                batch=16\n",
    "                # add a fold skipper\n",
    "                x_fold_train, x_fold_test, y_fold_train, y_fold_test = wrapped_train_val_split(fold_train_index,fold_test_index,pd.DataFrame(x_train),y_train)\n",
    "               \n",
    "                for hp_train_idx, hp_val_idx in train_test_split_hp.split(pd.DataFrame(x_fold_train)):\n",
    "                    print('hp')\n",
    "                    x_hp_train, x_hp_val, y_hp_train, y_hp_val = wrapped_train_val_split(hp_train_idx,hp_val_idx,pd.DataFrame(x_train),y_train)\n",
    "                    \n",
    "                    \n",
    "                    ### FIT MODEL AND EVALUTATE IT\n",
    "                    ###fit the model\n",
    "            \n",
    "                    path=\"{}/CV/{}/{}/{}/{}/fold_{}\".format(os.getcwd(),resample,model_name,prop,GSHT,i_fold)\n",
    "        \n",
    "                    create_dir(home,resample,model_name,prop,GSHT,i_fold)\n",
    "        \n",
    "                    ### DEFINE MODEL\n",
    "                    # model_name = 'CNN_single_task'\n",
    "                    # model = model_for_grid(path,resample,epochs) #resultdir\n",
    "                    \n",
    "                    pipe_cond='No_scalling'\n",
    "        \n",
    "                    tuner = kt.GridSearch(build_model,\n",
    "                                       objective=kt.Objective('val_loss', 'min'),\n",
    "                                        # loss = 'val_loss',\n",
    "                                       # objective = ['val_mse','epoch_entropy_pred_mse','val_free_energy_pred_mse'],\n",
    "                                      directory=f'{path}/hyper_param_tunning_tunner_fold_{i_fold}',\n",
    "                                      overwrite=False,\n",
    "                                      project_name=f'{batch}')\n",
    "                    with open(f'{path}/tuner_path.txt', 'w') as f:\n",
    "                        f.write(tuner.project_dir)\n",
    "                    f.close\n",
    "        \n",
    "                    #### CALL BACKS!\n",
    "                    es1 = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min', \n",
    "                       verbose=1, \n",
    "                       patience=2000, \n",
    "                       restore_best_weights=True)\n",
    "                    # CSV Logger\n",
    "                    csv_logger1 = CSVLogger(f\"{path}/csv_logger/model_history_log_resample_{resample}_fold_{i_fold}.csv\", \n",
    "                                           append=True)\n",
    "                    # CP_callbacks            \n",
    "                    # cp_callback1 = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "                    #                                                  save_weights_only=True,\n",
    "                    #                                                  verbose=1)\n",
    "                    # TensorBoard\n",
    "                    tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=f'{path}/tensorboard_logs/{resample}_fold_{i_fold}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                                          update_freq = 1,\n",
    "                                                                          # histogram_freq=1, \n",
    "                                                                          write_graph=False, \n",
    "                                                                          write_images=False)\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "                    \n",
    "                    # Covert to list and provide to Keras Regressor\n",
    "                    keras_callbacks1 = [es1, csv_logger1, tensorboard_callback1]\n",
    "                    #Save data\n",
    "                    save_splits_hp(hp_train_idx,y_hp_train,x_hp_train,resample,path,f\"train_hp_fold_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "                    save_splits_hp(hp_val_idx,y_hp_val,x_hp_val,resample,path,f\"val_hp_fold_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "        \n",
    "                    \n",
    "                    history=tuner.search(x_hp_train, y_hp_train[:],\n",
    "                                epochs = epochs,\n",
    "                                batch_size=batch,\n",
    "                                verbose = 2,\n",
    "                                validation_data =(x_hp_val, y_hp_val[:]),\n",
    "                                 # validation_split = 0.2,\n",
    "                                callbacks=[es1, csv_logger1, tensorboard_callback1])\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966455a-832e-440b-9bbc-41b442acc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_resample\n",
    "job_i_fold\n",
    "python_script(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "dir=abs_dir\n",
    "cores=1\n",
    "job_name=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}\"\n",
    "python_script=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}.py\"\n",
    "with open(f'{dir}/{python_script}','w') as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a82fe-e0ae-48aa-8f04-71e540b05301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3950c7-2691-4683-933f-99f0b0c12ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "008a476b-8117-4f4c-b63d-2128dfa70bd8",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "7f7fde44-b498-4c94-b318-d923996073cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resample: 1\n",
      "Reloading Tuner from /users/qdb16186/CNN_multitask_grid/CV/1/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_0/hyper_param_tunning_tunner_fold_0/16/tuner0.json\n",
      "Reloading Tuner from /users/qdb16186/CNN_multitask_grid/CV/1/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_1/hyper_param_tunning_tunner_fold_1/16/tuner0.json\n",
      "Reloading Tuner from /users/qdb16186/CNN_multitask_grid/CV/1/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_2/hyper_param_tunning_tunner_fold_2/16/tuner0.json\n",
      "Reloading Tuner from /users/qdb16186/CNN_multitask_grid/CV/1/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_3/hyper_param_tunning_tunner_fold_3/16/tuner0.json\n",
      "Reloading Tuner from /users/qdb16186/CNN_multitask_grid/CV/1/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_4/hyper_param_tunning_tunner_fold_4/16/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow import tensorflow.keras.losses.Reduction\n",
    "# from tensorflow.keras.losses import Loss\n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    x_layer = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, x_layer = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "def wrapped_train_test_split_no_padding(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, x_layer = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return x_train, x_test, df_np(y_train), df_np(y_test)\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT,i_fold):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"fold_{}\".format(i_fold))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"fold_{}\".format(i_fold))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep\n",
    "\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    if len(true)==4:\n",
    "        for i in len(true):\n",
    "            y_outputs[f'y_true_{i}'] = true\n",
    "            y_outputs[f'y_pred_{i}'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def save_splits_hp(idx,true_y,true_x,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    if len(true_y)==4:\n",
    "        for i in range(len(true_y)):\n",
    "            y_outputs[f'y_true_{i}'] = true_y[i]\n",
    "    y_outputs.to_csv(f'{path}/Split_y_{resample}_type_{split_type}.csv', index=False)\n",
    "\n",
    "    x_inputs=pd.DataFrame(true_x)\n",
    "    x_inputs.to_csv(f'{path}/Split_x_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "#     Hyper parameters         \n",
    "    model_type1 = hp.Choice(\"model_type1\", [\"CNN3\",\"CNN2\",\"CNN1\"])\n",
    "    model_type = hp.Choice(\"model_type\", [\"Dense3\"])\n",
    " \n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x_layer=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    # with hp.conditional_scope(\"model_type1\", [\"CNN0\"]):\n",
    "    #         if model_type1 == \"CNN0\":\n",
    "    #             pass\n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    with hp.conditional_scope(\"model_type1\", [\"CNN1\",\"CNN2\"\"CNN3\"]):\n",
    "            if model_type1 != \"CNN0\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        input_shape=(250,1),\n",
    "                        name = 'conv1d_1'\n",
    "                        )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = 'batchnorm_1')(x_layer)\n",
    "                pass\n",
    "                \n",
    "            if model_type1 != \"CNN1\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_2'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_2')(x_layer)\n",
    "\n",
    "            if model_type1 != \"CNN1\" or \"CNN2\":               \n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_3'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_3')(x_layer)\n",
    "                \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    x_layer = keras.layers.Flatten(name = 'flatten')(x_layer)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    # with hp.conditional_scope(\"model_type\", [\"Dense0\"]):\n",
    "    #     if model_type == \"Dense0\":\n",
    "    #         pass\n",
    "            \n",
    "    with hp.conditional_scope(\"model_type\", [\"Dense3\"]): #[\"Dense1\",\"Dense2\",\"Dense3\"]\n",
    "        if model_type != \"Dense0\":\n",
    "            hp_layer_1= hp.Choice(f'layer_1', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_1,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        # name='layer_1',\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "        if model_type != \"Dense1\":\n",
    "            hp_layer_2_2= hp.Choice(f'layer_2_2', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_2_2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "\n",
    "        if model_type != \"Dense1\" or \"Dense2\":\n",
    "            hp_layer_3_3= hp.Choice(f'layer_3_3',  values=[16,32,64])\n",
    "            \n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_3_3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "#     OUTPUT LAYERS\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='melting_temperature')(x_layer)\n",
    "\n",
    "    output_1 = keras.layers.Dense(1, name='dH')(x_layer)\n",
    "    output_2 = keras.layers.Dense(1, name='dS')(x_layer)\n",
    "    output_3 = keras.layers.Dense(1, name='dG')(x_layer)\n",
    "    output_4 = keras.layers.Dense(1, name='Tm')(x_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[output_1, output_2, output_3, output_4])\n",
    "    # model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS\n",
    "#     SETTINGS\n",
    "\n",
    "#     ADAPTIVE LEARNING RATE   \n",
    "    \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    learning_rate_fn=0.00001\n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "### MODEL END Start Workflow\n",
    "\n",
    "\n",
    "### 2 more FUNCTIONS\n",
    "\n",
    "\n",
    "# def model_for_grid(resultdir,resample,epochs,**kwargs):\n",
    "# #Needs variable epochs \n",
    "#     es = EarlyStopping(monitor='val_loss', \n",
    "#                        mode='min', \n",
    "#                        verbose=1, \n",
    "#                        patience=2000, \n",
    "#                        restore_best_weights=True)\n",
    "#     # CSV Logger\n",
    "#     csv_logger = CSVLogger(f\"{resultdir}/csv_logger/model_history_log_resample_{resample}.csv\", \n",
    "#                            append=True)\n",
    "#     # CP_callbacks\n",
    "#     # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "#     #                                                  save_weights_only=True,\n",
    "#     #                                                  verbose=1)\n",
    "    \n",
    "#     # TensorBoard\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{resultdir}/tensorboard_logs/{resample}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "#                                                           update_freq = 1,\n",
    "#                                                           # histogram_freq=1, \n",
    "#                                                           write_graph=False, \n",
    "#                                                           write_images=False)\n",
    "#     # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    \n",
    "#     # Covert to list and provide to Keras Regressor\n",
    "   \n",
    "#     keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "        \n",
    "#     ###initialise model\n",
    "#     model = KerasRegressor(model=build_model, \n",
    "#                            batch=32,\n",
    "#                             epochs=epochs,\n",
    "#                             verbose=1,\n",
    "#                             validation_split=0.2,\n",
    "#                            callbacks=keras_callbacks,\n",
    "#                            dense1=1,\n",
    "#                             dense2=1,\n",
    "#                             dense3=1,\n",
    "#                            convn=1\n",
    "#                           )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wrapped_train_val_split(hp_train_idx,hp_val_idx,x_train,y_train):\n",
    "    x_hp_train = x_train.iloc[hp_train_idx].to_numpy()\n",
    "    x_hp_val = x_train.iloc[hp_val_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "\n",
    "    y_train_df=[]\n",
    "    for y_i in y_train:\n",
    "       y_train_df.append(pd.DataFrame(y_i))\n",
    "\n",
    "    y_train=y_train_df\n",
    "    y_hp_train = [y_train[0].iloc[hp_train_idx],y_train[1].iloc[hp_train_idx],y_train[2].iloc[hp_train_idx],y_train[3].iloc[hp_train_idx]]\n",
    "    y_hp_val  = [y_train[0].iloc[hp_val_idx] ,y_train[1].iloc[hp_val_idx] ,y_train[2].iloc[hp_val_idx] ,y_train[3].iloc[hp_val_idx]]\n",
    "    \n",
    "    # return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "    return x_hp_train, x_hp_val, df_np(y_hp_train), df_np(y_hp_val)\n",
    "\n",
    "\n",
    "### Input Parameters and Workflow\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"/users/qdb16186/CNN_multitask_grid/Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\n",
    "prop='Granulated'\n",
    "GSHT='dH_dS_dG_Tm'\n",
    "# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=5\n",
    "n_folds=5\n",
    "n_jobs=1\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(1, test_size=test_frac, random_state=1)\n",
    "\n",
    "###define scoring dict for cv\n",
    "scorers = {\n",
    "    'r2':make_scorer(r2_func), \n",
    "    'rmse':make_scorer(rmse_func, greater_is_better=False), \n",
    "    'bias':make_scorer(bias_func, greater_is_better=False), \n",
    "    'sdep':make_scorer(sdep_func, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1\n",
    "    if resample >1:\n",
    "        break\n",
    "    # add a resample skipper\n",
    "    print(f'resample: {resample}')\n",
    "    \n",
    "    # ### DEFINE MODEL\n",
    "    model_name = f'{prop}_multi_task_CNN'\n",
    "    \n",
    "    ### CV Train test split\n",
    "    #     adjust y[2] to * Temperature /1000 dS*T kcal/mol\n",
    "    #for CNN with padding\n",
    "    x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "    \n",
    "    # fold splits\n",
    "    kf=KFold(n_splits=n_folds)\n",
    "    for i_fold, (fold_train_index, fold_test_index) in enumerate(kf.split(x_train)):\n",
    "        batch=16\n",
    "        # add a fold skipper\n",
    "        x_fold_train, x_fold_test, y_fold_train, y_fold_test = wrapped_train_val_split(fold_train_index,fold_test_index,pd.DataFrame(x_train),y_train)\n",
    "       \n",
    "        for hp_train_idx, hp_val_idx in train_test_split_hp.split(pd.DataFrame(x_fold_train)):\n",
    "         \n",
    "            x_hp_train, x_hp_val, y_hp_train, y_hp_val = wrapped_train_val_split(hp_train_idx,hp_val_idx,pd.DataFrame(x_train),y_train)\n",
    "            \n",
    "            \n",
    "            ### FIT MODEL AND EVALUTATE IT\n",
    "            ###fit the model\n",
    "    \n",
    "            path=\"{}/CV/{}/{}/{}/{}/fold_{}\".format(os.getcwd(),resample,model_name,prop,GSHT,i_fold)\n",
    "\n",
    "            create_dir(home,resample,model_name,prop,GSHT,i_fold)\n",
    "\n",
    "            ### DEFINE MODEL\n",
    "            # model_name = 'CNN_single_task'\n",
    "            # model = model_for_grid(path,resample,epochs) #resultdir\n",
    "            \n",
    "            pipe_cond='No_scalling'\n",
    "\n",
    "            tuner = kt.GridSearch(build_model,\n",
    "                               objective=kt.Objective('val_loss', 'min'),\n",
    "                                # loss = 'val_loss',\n",
    "                               # objective = ['val_mse','epoch_entropy_pred_mse','val_free_energy_pred_mse'],\n",
    "                              directory=f'{path}/hyper_param_tunning_tunner_fold_{i_fold}',\n",
    "                              overwrite=False,\n",
    "                              project_name=f'{batch}')\n",
    "            with open(f'{path}/tuner_path.txt', 'w') as f:\n",
    "                f.write(tuner.project_dir)\n",
    "            f.close\n",
    "\n",
    "            #### CALL BACKS!\n",
    "            es1 = EarlyStopping(monitor='val_loss', \n",
    "               mode='min', \n",
    "               verbose=1, \n",
    "               patience=2000, \n",
    "               restore_best_weights=True)\n",
    "            # CSV Logger\n",
    "            csv_logger1 = CSVLogger(f\"{path}/csv_logger/model_history_log_resample_{resample}_fold_{i_fold}.csv\", \n",
    "                                   append=True)\n",
    "            # CP_callbacks            \n",
    "            # cp_callback1 = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "            #                                                  save_weights_only=True,\n",
    "            #                                                  verbose=1)\n",
    "            # TensorBoard\n",
    "            tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=f'{path}/tensorboard_logs/{resample}_fold_{i_fold}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                                  update_freq = 1,\n",
    "                                                                  # histogram_freq=1, \n",
    "                                                                  write_graph=False, \n",
    "                                                                  write_images=False)\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "            \n",
    "            # Covert to list and provide to Keras Regressor\n",
    "            keras_callbacks1 = [es1, csv_logger1, tensorboard_callback1]\n",
    "            #Save data\n",
    "            save_splits_hp(hp_train_idx,y_hp_train,x_hp_train,resample,path,f\"train_hp_fold_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "            save_splits_hp(hp_val_idx,y_hp_val,x_hp_val,resample,path,f\"val_hp_fold_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "\n",
    "            \n",
    "            history=tuner.search(x_hp_train, y_hp_train[:],\n",
    "                        epochs = epochs,\n",
    "                        batch_size=batch,\n",
    "                        verbose = 2,\n",
    "                        validation_data =(x_hp_val, y_hp_val[:]),\n",
    "                         # validation_split = 0.2,\n",
    "                        callbacks=[es1, csv_logger1, tensorboard_callback1])\n",
    "            break\n",
    "\n",
    "    #    ############ Store results\n",
    "    \n",
    "    #             # results=pd.DataFrame(history.cv_results_)\n",
    "    #             # results.to_csv(path+f\"/gridsearch_resample_{resample}_pipe_cond_{pipe_cond}.csv\")\n",
    "    \n",
    "    #             # best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    #             # model_hp = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "    #             # model_fitted=model2.fit(x_train,y_train[0])\n",
    "    #             # y_pred_test=history.predict(x_test)\n",
    "    #             # y_pred_train=history.predict(x_train)\n",
    "    #             # y_pred_test=model_fitted.predict(x_test)\n",
    "    #             # y_pred_train=model_fitted.predict(x_train)\n",
    "    \n",
    "    #             # save_splits(train_idx,y_train[:],y_pred_train,resample,path,f\"train_pipe_cond_{pipe_cond}\")\n",
    "    #             # save_splits(test_idx,y_test[:],y_pred_test,resample,path,f\"test_pipe_cond_{pipe_cond}\")\n",
    "    #     save_splits_hp(fold_test_index,y_fold_test,x_fold_test,resample,path,f\"test_hp_fold_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "        \n",
    "    # path=\"{}/CV/{}/{}/{}/{}/\".format(os.getcwd(),resample,model_name,prop,GSHT,i_fold)\n",
    "    # save_splits_hp(train_idx,y_train,x_train,resample,path,f\"train_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "    # save_splits_hp(test_idx,y_test,x_test,resample,path,f\"test_{i_fold}_pipe_cond_{pipe_cond}\")\n",
    "# total_time=time.time()-time_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "566f96e3-6310-4cf8-90d7-85cee0da03c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CV/5/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_4/hyper_param_tunning_tunner_fold_4/32'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = kt.GridSearch(build_model,\n",
    "                               objective=kt.Objective('val_loss', 'min'),\n",
    "                                # loss = 'val_loss',\n",
    "                               # objective = ['val_mse','epoch_entropy_pred_mse','val_free_energy_pred_mse'],\n",
    "                              directory=f'{path}/hyper_param_tunning_tunner_fold_{i_fold}',\n",
    "                              overwrite=False,\n",
    "                              project_name='32')\n",
    "i_fold\n",
    "tuner.project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eac90b-193c-4d77-825a-25d06b10bc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03fb60-bdb4-43e5-8277-f80a3621adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc3780e0-8928-4ee0-b6b6-f2abfa078779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "model_type1 (Choice)\n",
      "{'default': 'CNN3', 'conditions': [], 'values': ['CNN3', 'CNN2', 'CNN1'], 'ordered': False}\n",
      "model_type (Choice)\n",
      "{'default': 'Dense3', 'conditions': [], 'values': ['Dense3'], 'ordered': False}\n",
      "layer_1 (Choice)\n",
      "{'default': 16, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['Dense3']}}], 'values': [16, 32, 64, 128], 'ordered': True}\n",
      "layer_2_2 (Choice)\n",
      "{'default': 16, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['Dense3']}}], 'values': [16, 32, 64, 128], 'ordered': True}\n",
      "layer_3_3 (Choice)\n",
      "{'default': 16, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['Dense3']}}], 'values': [16, 32, 64], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa8baa88-2a80-4f5a-876a-e853f2774954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CV/5/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_4/hyper_param_tunning_tunner_fold_4/16'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2c9feb9-2b2f-4ff3-b169-49258c049965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22e613-0051-40cb-b0f7-20e956c0b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()\n",
    "tuner.search_space_summary()\n",
    "tuner.get_best_hyperparameters()\n",
    "tuner.metrics\n",
    "tuner.project_dir\n",
    "# tuner.project_name\n",
    "tuner.get_best_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb66582f-b6c6-4c67-bd23-d129e9b9db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 144 Complete [00h 00m 05s]\n",
      "val_loss: 54375.52734375\n",
      "\n",
      "Best val_loss So Far: 247.62664794921875\n",
      "Total elapsed time: 00h 25m 58s\n"
     ]
    }
   ],
   "source": [
    "history=tuner.search(x_hp_train, y_hp_train[:],\n",
    "                        epochs = 2,\n",
    "                        batch_size=16,\n",
    "                        verbose = 2,\n",
    "                        validation_data =(x_hp_val, y_hp_val[:]),\n",
    "                         # validation_split = 0.2,\n",
    "                        callbacks=[es1, csv_logger1, tensorboard_callback1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4df2abe4-ba62-46b9-978a-1edc4a4e698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fafda0-bb96-412a-81d0-06cfde34e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.results_summary()\n",
    "tuner.get_best_hyperparameters()[0]\n",
    "# pd.DataFrame(tuner.get_best_models()[0].predict(x_hp_val)[0]).to_csv(\"val.csv\")\n",
    "plt.scatter(y_test[3],tuner.get_best_models()[0].predict(x_test)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006b34e0-1053-4176-8f43-cf3132fa3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395103c4-4ed1-48c6-b3b0-3e738e83f234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 144)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"/users/qdb16186/CNN_multitask_grid/Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\n",
    "prop='Granulated'\n",
    "# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6c3ee-6123-44b5-9ae3-b85a047c0efb",
   "metadata": {},
   "source": [
    "# Bash Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86de57bb-33c7-419c-8466-d5bd295dee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_script_gen(abs_dir,prop,GSHT,cores,job_resample,job_i_fold):\n",
    "    dir=abs_dir\n",
    "    cores=1\n",
    "    HH=48\n",
    "    job_name=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}\"\n",
    "    python_script=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}.py\"\n",
    "    \n",
    "    # job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    # python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/sbatch_{job_name}.sh','w') as f:\n",
    "        f.write(f\"\"\"#!/bin/bash\n",
    "\n",
    "#======================================================\n",
    "#\n",
    "# Job script for running on a single node\n",
    "#\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# Propogate environment variables to the compute node\n",
    "#SBATCH --export=ALL\n",
    "#\n",
    "# Run in the standard partition (queue)\n",
    "#SBATCH --partition=standard\n",
    "#\n",
    "# Specify project account\n",
    "#SBATCH --account=palmer-addmd\n",
    "#\n",
    "# No. of tasks required\n",
    "#SBATCH --ntasks=1 --cpus-per-task={cores}\n",
    "#\n",
    "# Distribute processes in round-robin fashion for load balancing\n",
    "#SBATCH --distribution=cyclic\n",
    "#\n",
    "#\n",
    "# Specify (hard) runtime (HH:MM:SS)\n",
    "#SBATCH --time={HH}:00:00\n",
    "#\n",
    "# Job name\n",
    "#SBATCH --job-name={job_name}\n",
    "#\n",
    "# Output file\n",
    "#SBATCH --output={job_name}_slurm-%j.out\n",
    "#======================================================\n",
    "\n",
    "module purge\n",
    "module load anaconda/python-3.9.7/2021.11\n",
    "source activate tf\n",
    "module purge\n",
    "\n",
    "#======================================================\n",
    "# Prologue script to record job details\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_prologue.sh\n",
    "#------------------------------------------------------\n",
    "\n",
    "python {python_script}\n",
    "#mpirun -np $SLURM_NTASKS namd2 HEWL_002M_D2.inp > HEWL_002M_D2.out.$SLURM_JOB_ID \n",
    "\n",
    "#======================================================\n",
    "# Epilogue script to record job endtime and runtime\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_epilogue.sh\n",
    "#------------------------------------------------------\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e227de7-0aa6-4e20-970d-83f5852153a0",
   "metadata": {},
   "source": [
    "# Run Jupyter book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1310c50c-3b55-421b-923b-456ecef768f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir='CNN_multitask_grid'\n",
    "os.chdir('/users/qdb16186')\n",
    "path=os.getcwd()\n",
    "abs_dir=path+\"/\"+workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b2cc7d-298b-45d9-bf0f-fed2c745802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/qdb16186/CNN_multitask_grid/sbatch_1_0_Granulated_CNN_multi_task_dH_dS_dG_Tm_1.sh\n",
      "Submitted batch job 10654882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# desc_type = ['DNA-Groups','Granulated','OHEP','LP_dec2']\n",
    "# GSHT_list=['dH','dS','dG','Tm']\n",
    "GSHT='dH_dS_dG_Tm' \n",
    "cores=1\n",
    "prop = 'LP_dec2'\n",
    "os.chdir('/users/qdb16186')\n",
    "os.chdir(abs_dir)\n",
    "\n",
    "job_resample=1\n",
    "job_i_fold=0\n",
    "python_script(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "bash_script_gen(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "\n",
    "job_name=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}\"\n",
    "print(f'{abs_dir}/sbatch_{job_name}.sh')\n",
    "bashCommand = f\"sbatch {abs_dir}/sbatch_{job_name}.sh\"\n",
    "os.system(bashCommand)\n",
    "# for job_resample in range(1,51):\n",
    "#     for job_i_fold in range(5):\n",
    "        \n",
    "#         python_script(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "#         bash_script_gen(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "\n",
    "\n",
    "# for prop in desc_type:\n",
    "        # python_script(abs_dir,prop,GSHT,cores)\n",
    "        # bash_script_gen(abs_dir,prop,GSHT,cores)\n",
    "\n",
    "\n",
    "# def python_script(abs_dir,prop,GSHT,cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcdf7a8c-5e91-4000-8e50-9728346f31ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10800"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45*5*48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037d3c85-cc90-4a3a-a80e-0ba14148de97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('monitor.sh','w') as f:\n",
    "#     f.write('#!/bin/bash \\n')\n",
    "#     f.write('sqme | wc -l')\n",
    "#     f.close\n",
    "# os.system('chmod u+x monitor.sh')\n",
    "# os.system('./monitor.sh')\n",
    "# temp_count=os.system('squeue -u qdb16186 | wc -l')\n",
    "import subprocess\n",
    "command = 'squeue -u qdb16186 | wc -l'\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "result\n",
    "output = result.stdout.strip()\n",
    "int(output)==251\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c71dbcc0-8e38-4604-a65e-e7692abde3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State has changed!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to check the state (replace this with your own logic)\n",
    "def check_state():\n",
    "    # Replace this with your actual logic to check the state\n",
    "    # For example, you might check the value of a variable, read from a sensor, etc.\n",
    "    command = 'squeue -u qdb16186 | wc -l'\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    output = result.stdout.strip()\n",
    "    # Condition\n",
    "    if int(output)>260:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Time to sleep between state checks\n",
    "sleep_interval = 1 #300  # in seconds\n",
    "\n",
    "# Keep sleeping until the state changes\n",
    "while check_state():\n",
    "    time.sleep(sleep_interval)\n",
    "\n",
    "# Once the state changes, continue with the rest of your code\n",
    "print(\"State has changed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde21bfa-e128-4b88-a098-2cd67144eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Number of iterations in the for loop\n",
    "num_iterations = 10\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Your loop code goes here\n",
    "    \n",
    "    # Example: Print iteration number\n",
    "    print(f\"Iteration: {i}\")\n",
    "    \n",
    "    # Wait for a short duration (optional)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Clear the previous cell output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Display the latest output\n",
    "    # display(f\"Iteration: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efbe696-c58b-4466-b949-6c1849d9b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/qdb16186/CNN_multitask_grid/sbatch_5_4_LP_dec2_CNN_multi_task_dH_dS_dG_Tm_1.sh\n",
      "Submitted batch job 10669330\n"
     ]
    }
   ],
   "source": [
    "GSHT='dH_dS_dG_Tm' \n",
    "cores=1\n",
    "prop = 'LP_dec2'\n",
    "os.chdir('/users/qdb16186')\n",
    "os.chdir(abs_dir)\n",
    "# for prop in ['DNA-groups','LP_dec2','OHEP']\n",
    "for job_resample in range(1,6):\n",
    "    for job_i_fold in range(5):\n",
    "        # if job_resample == 1 and job_i_fold == 0:\n",
    "        #     continue\n",
    "        python_script(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "        bash_script_gen(abs_dir,prop,GSHT,cores,job_resample,job_i_fold)\n",
    "        \n",
    "        job_name=f\"{job_resample}_{job_i_fold}_{prop}_CNN_multi_task_{GSHT}_{cores}\"\n",
    "        print(f'{abs_dir}/sbatch_{job_name}.sh')\n",
    "        bashCommand = f\"sbatch {abs_dir}/sbatch_{job_name}.sh\"\n",
    "        # os.system(bashCommand)\n",
    "        clear_output(wait=True)\n",
    "        # os.system(f'{bashCommand} > /dev/null 2>&1')\n",
    "        print(f'{abs_dir}/sbatch_{job_name}.sh')\n",
    "        os.system(bashCommand)\n",
    "        # while check_state():\n",
    "        #     time.sleep(sleep_interval)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fe6bee-e536-46e6-b550-602de3434db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sbatch /users/qdb16186/CNN_multitask_grid/sbatch_1_0_Granulated_CNN_multi_task_dH_dS_dG_Tm_1.sh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bashCommand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa681b67-ea19-46ee-b849-c199d457db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628061\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628062\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628063\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628064\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628065\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628066\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628067\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628068\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628069\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628070\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628071\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628072\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628073\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628074\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628075\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628076\n"
     ]
    }
   ],
   "source": [
    "desc_type = ['DNA-Groups','Granulated','OHEP','LP_dec2']\n",
    "GSHT_list=['dH','dS','dG','Tm']\n",
    "cores=10\n",
    "os.chdir('/users/qdb16186')\n",
    "os.chdir(abs_dir)\n",
    "for prop in desc_type:\n",
    "    for GSHT in GSHT_list:\n",
    "        job_name=f\"{prop}_{GSHT}_{cores}_CNN_single_task\"\n",
    "        print(f'{abs_dir}/sbatch_{job_name}.sh')\n",
    "        bashCommand = f\"sbatch {abs_dir}/sbatch_{job_name}.sh\"\n",
    "        os.system(bashCommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1776cd4-8c0f-4ea6-825a-4e2346435391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSHT = 'dH'\n",
    "# GSHT in \n",
    "GSHT_list=['dH','dS','dG','Tm']\n",
    "GSHT_list.index(GSHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adf9a010-a6ad-46e0-8f38-c15a91c62281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CNN_single_task'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47539a44-8f91-41ff-84b2-51f805d9d800",
   "metadata": {},
   "source": [
    "# Load and Run oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f1b5b3ad-b82e-40eb-90e7-0c0ea4a6cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow import tensorflow.keras.losses.Reduction\n",
    "# from tensorflow.keras.losses import Loss\n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    x_layer = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, x_layer = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "def wrapped_train_test_split_no_padding(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, x_layer = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return x_train, x_test, df_np(y_train), df_np(y_test)\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT,i_fold):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"fold_{}\".format(i_fold))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"fold_{}\".format(i_fold))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    if len(true)==4:\n",
    "        for i in len(true):\n",
    "            y_outputs[f'y_true_{i}'] = true\n",
    "            y_outputs[f'y_pred_{i}'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "def save_splits_hp(idx,true_y,true_x,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    if len(true_y)==4:\n",
    "        for i in range(len(true_y)):\n",
    "            y_outputs[f'y_true_{i}'] = true_y[i]\n",
    "    y_outputs.to_csv(f'{path}/Split_y_{resample}_type_{split_type}.csv', index=False)\n",
    "\n",
    "    x_inputs=pd.DataFrame(true_x)\n",
    "    x_inputs.to_csv(f'{path}/Split_x_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "dbaa5d7a-143f-4dc0-ae52-81b466d7dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \n",
    "#     Hyper parameters         \n",
    "    model_type1 = hp.Choice(\"model_type1\", [\"CNN3\",\"CNN2\",\"CNN1\"])\n",
    "    model_type = hp.Choice(\"model_type\", [\"Dense3\"])\n",
    " \n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x_layer=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    # with hp.conditional_scope(\"model_type1\", [\"CNN0\"]):\n",
    "    #         if model_type1 == \"CNN0\":\n",
    "    #             pass\n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    with hp.conditional_scope(\"model_type1\", [\"CNN1\",\"CNN2\"\"CNN3\"]):\n",
    "            if model_type1 != \"CNN0\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                        kernel_size=(3), \n",
    "                        strides=(2), \n",
    "                        padding='valid', \n",
    "                        activation='relu', \n",
    "                        input_shape=(250,1),\n",
    "                        name = 'conv1d_1'\n",
    "                        )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = 'batchnorm_1')(x_layer)\n",
    "                pass\n",
    "                \n",
    "            if model_type1 != \"CNN1\":\n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_2'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_2')(x_layer)\n",
    "\n",
    "            if model_type1 != \"CNN1\" or \"CNN2\":               \n",
    "                x_layer = keras.layers.Conv1D(32, \n",
    "                                    kernel_size=(3), \n",
    "                                    strides=(2), \n",
    "                                    padding='valid', \n",
    "                                    activation='relu', \n",
    "                                    name = f'conv1d_3'\n",
    "                                    )(x_layer)\n",
    "                x_layer = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x_layer)\n",
    "                x_layer = keras.layers.BatchNormalization(name = f'batchnorm_3')(x_layer)\n",
    "                \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    x_layer = keras.layers.Flatten(name = 'flatten')(x_layer)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    # with hp.conditional_scope(\"model_type\", [\"Dense0\"]):\n",
    "    #     if model_type == \"Dense0\":\n",
    "    #         pass\n",
    "            \n",
    "    with hp.conditional_scope(\"model_type\", [\"Dense3\"]): #[\"Dense1\",\"Dense2\",\"Dense3\"]\n",
    "        if model_type != \"Dense0\":\n",
    "            hp_layer_1= hp.Choice(f'layer_1', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_1,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        # name='layer_1',\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "        if model_type != \"Dense1\":\n",
    "            hp_layer_2_2= hp.Choice(f'layer_2_2', values=[16,32,64,128])\n",
    "\n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_2_2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "\n",
    "        if model_type != \"Dense1\" or \"Dense2\":\n",
    "            hp_layer_3_3= hp.Choice(f'layer_3_3',  values=[16,32,64])\n",
    "            \n",
    "            x_layer = keras.layers.Dense(\n",
    "                        hp_layer_3_3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x_layer)\n",
    "#     OUTPUT LAYERS\n",
    "\n",
    "    # output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x_layer)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x_layer)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x_layer)\n",
    "    # output_4 = keras.layers.Dense(1, name='melting_temperature')(x_layer)\n",
    "\n",
    "    output_1 = keras.layers.Dense(1, name='dH')(x_layer)\n",
    "    output_2 = keras.layers.Dense(1, name='dS')(x_layer)\n",
    "    output_3 = keras.layers.Dense(1, name='dG')(x_layer)\n",
    "    output_4 = keras.layers.Dense(1, name='Tm')(x_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[output_1, output_2, output_3, output_4])\n",
    "    # model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS\n",
    "#     SETTINGS\n",
    "\n",
    "#     ADAPTIVE LEARNING RATE   \n",
    "    \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    learning_rate_fn = 0.00001\n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "af936fcf-63e5-4d7a-aec3-9fdf97ae1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=pd.read_csv(\"/users/qdb16186/CNN_multitask_grid/Lomzov_dataset_IY.csv\")\n",
    "prop='Granulated'\n",
    "model_name = f'{prop}_multi_task_CNN'\n",
    "GSHT='dH_dS_dG_Tm' \n",
    "resample=1\n",
    "batch=16\n",
    "i_fold=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04529a8-38b0-4ad5-b978-02187f5a1142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "31a8f90c-9300-4667-a15a-138167e50e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from /users/qdb16186/CNN_multitask_grid/CV/2/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_4/hyper_param_tunning_tunner_fold_4/16/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "path=\"{}/CV/{}/{}/{}/{}/fold_{}\".format('/users/qdb16186/CNN_multitask_grid',resample,model_name,prop,GSHT,i_fold)\n",
    "tuner = kt.GridSearch(build_model,\n",
    "                           objective=kt.Objective('val_loss', 'min'),\n",
    "                            # loss = 'val_loss',\n",
    "                           # objective = ['val_mse','epoch_entropy_pred_mse','val_free_energy_pred_mse'],\n",
    "                          directory=f'{path}/hyper_param_tunning_tunner_fold_{i_fold}',\n",
    "                          overwrite=False,\n",
    "                          project_name=f'{batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "01a43896-e471-4a33-9479-2cfce57b82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_train_val_split(hp_train_idx,hp_val_idx,x_train,y_train):\n",
    "    x_hp_train = x_train.iloc[hp_train_idx].to_numpy()\n",
    "    x_hp_val = x_train.iloc[hp_val_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "\n",
    "    y_train_df=[]\n",
    "    for y_i in y_train:\n",
    "       y_train_df.append(pd.DataFrame(y_i))\n",
    "\n",
    "    y_train=y_train_df\n",
    "    y_hp_train = [y_train[0].iloc[hp_train_idx],y_train[1].iloc[hp_train_idx],y_train[2].iloc[hp_train_idx],y_train[3].iloc[hp_train_idx]]\n",
    "    y_hp_val  = [y_train[0].iloc[hp_val_idx] ,y_train[1].iloc[hp_val_idx] ,y_train[2].iloc[hp_val_idx] ,y_train[3].iloc[hp_val_idx]]\n",
    "    \n",
    "    # return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "    return x_hp_train, x_hp_val, df_np(y_hp_train), df_np(y_hp_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "297c9d3a-a1ea-4f7e-bc75-af3ba18e9bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "model_type1 (Choice)\n",
      "{'default': 'CNN3', 'conditions': [], 'values': ['CNN3', 'CNN2', 'CNN1'], 'ordered': False}\n",
      "model_type (Choice)\n",
      "{'default': 'Dense3', 'conditions': [], 'values': ['Dense3'], 'ordered': False}\n",
      "layer_1 (Choice)\n",
      "{'default': 16, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['Dense3']}}], 'values': [16, 32, 64, 128], 'ordered': True}\n",
      "layer_2_2 (Choice)\n",
      "{'default': 16, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['Dense3']}}], 'values': [16, 32, 64, 128], 'ordered': True}\n",
      "layer_3_3 (Choice)\n",
      "{'default': 16, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['Dense3']}}], 'values': [16, 32, 64], 'ordered': True}\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 250, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 124, 32)      128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " maxpooling_1 (MaxPooling1D)    (None, 62, 32)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_1 (BatchNormalizatio  (None, 62, 32)      128         ['maxpooling_1[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 30, 32)       3104        ['batchnorm_1[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_3 (MaxPooling1D)    (None, 15, 32)       0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_3 (BatchNormalizatio  (None, 15, 32)      128         ['maxpooling_3[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 480)          0           ['batchnorm_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           15392       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           1056        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           528         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dH (Dense)                     (None, 1)            17          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dS (Dense)                     (None, 1)            17          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dG (Dense)                     (None, 1)            17          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " Tm (Dense)                     (None, 1)            17          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,532\n",
      "Trainable params: 20,404\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps\n",
    "tuner.get_best_hyperparameters(num_trials=2)[:]\n",
    "model_hyper = tuner.hypermodel.build(best_hps)\n",
    "model_hyper.count_params()\n",
    "model_hyper.layers\n",
    "model_hyper.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de0e34-00ce-4215-af3f-d73985fbafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_number=str(dir_.replace(\"trial_\",\"\"))\n",
    "grid_trial_number=df_temp.index[df_temp['selection'] == df_temp['selection'].min()][0]\n",
    "trial_grid=tuner.oracle.get_trial(grid_trial_number)\n",
    "model_grid=tuner.load_model(trial_grid)\n",
    "# best_hps = tuner.get_best_hyperparameters(5)\n",
    "# model_grid.\n",
    "model_grid.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "8d421015-847f-4733-b197-9d187dc5de5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters at 0x1541992c66a0>"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_hps=trial_grid.display_hyperparameters()\n",
    "# best_hps\n",
    "trial_grid.hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "432c7ca1-3eb5-46f3-a3a1-622f0b2c55f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 250, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 124, 32)      128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " maxpooling_1 (MaxPooling1D)    (None, 62, 32)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_1 (BatchNormalizatio  (None, 62, 32)      128         ['maxpooling_1[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 30, 32)       3104        ['batchnorm_1[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_2 (MaxPooling1D)    (None, 15, 32)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_2 (BatchNormalizatio  (None, 15, 32)      128         ['maxpooling_2[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 7, 32)        3104        ['batchnorm_2[0][0]']            \n",
      "                                                                                                  \n",
      " maxpooling_3 (MaxPooling1D)    (None, 3, 32)        0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batchnorm_3 (BatchNormalizatio  (None, 3, 32)       128         ['maxpooling_3[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['batchnorm_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           6208        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           2080        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           1056        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dH (Dense)                     (None, 1)            33          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dS (Dense)                     (None, 1)            33          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dG (Dense)                     (None, 1)            33          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,163\n",
      "Trainable params: 15,971\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_grid.\n",
    "model_lr = build_model(trial_grid.hyperparameters)\n",
    "model_lr.layers\n",
    "model_lr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec9034-69b6-4b7c-ae7a-f9e1e1ebeb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "f328c7c7-7327-4117-8713-74a5534569fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_grid.get_metrics_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "b9864640-e999-441b-be70-efcd6184b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Example: df_train is your training DataFrame with target variables\n",
    "scaler_dH = StandardScaler().fit(pd.DataFrame(y_fold_train[0]))\n",
    "scaler_dS = StandardScaler().fit(pd.DataFrame(y_fold_train[1]))\n",
    "scaler_dG = StandardScaler().fit(pd.DataFrame(y_fold_train[2]))\n",
    "scaler_Tm = StandardScaler().fit(pd.DataFrame(y_fold_train[3]))\n",
    "\n",
    "\n",
    "# Fit on training data and transform the target variables\n",
    "# df_train[['task1', 'task2', 'task3', 'task4']] = \n",
    "# fit_dH=scaler.fit(y_fold_train)|\n",
    "# fit_dH = scaler_dH.fit(y_fold_train[0])\n",
    "# fit_dS = scaler_dS.fit(y_fold_train[1])\n",
    "# fit_dG = scaler_dG.fit(y_fold_train[2])\n",
    "# fit_Tm = scaler_Tm.fit(y_fold_train[3])\n",
    "\n",
    "# fit_scaler.transform(y_fold_train)\n",
    "# y_fold_train\n",
    "[scaler_dH.transform(pd.DataFrame(y_fold_train[0])),\n",
    "scaler_dS.transform(pd.DataFrame(y_fold_train[1])),\n",
    "scaler_dG.transform(pd.DataFrame(y_fold_train[2])),\n",
    "scaler_Tm.transform(pd.DataFrame(y_fold_train[3]))]\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# Example 1D array (replace this with your actual data)\n",
    "# y_fold_test = np.array([-60.7, -50.5, -59.2, -61.2, -58.8, -62.6, -62.8, -104.4, -38.6])\n",
    "\n",
    "# # Reshape the 1D array to 2D\n",
    "# y_fold_test_reshaped = y_fold_test.reshape(-1, 1)\n",
    "\n",
    "# # Fit on training data and transform the test data\n",
    "# y_fold_test_scaled = scaler.fit_transform(y_fold_train.reshape(-1, 1))\n",
    "# y_fold_train\n",
    "# y_fold_train[0]\n",
    "# scaler_dH.transform(pd.DataFrame(y_fold_test[0]))\n",
    "scaller_x = StandardScaler().fit(pd.DataFrame(x_fold_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37047ad3-79c6-4df6-80dd-b929d425c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f64d36-a530-47da-98f6-f6b232f55152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "c26995da-0d8c-4875-8f8a-4374b46deef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[661], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model_grid.summary()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# grid_path=\"{}/CV/{}/{}/{}/{}\".format('/users/qdb16186/CNN_multitask_grid',resample,model_name,prop,GSHT)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#                                                       write_images=False)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x_fold_train, x_fold_test, y_fold_train, y_fold_test \u001b[38;5;241m=\u001b[39m wrapped_train_val_split(fold_train_index,fold_test_index,pd\u001b[38;5;241m.\u001b[39mDataFrame(x_train),y_train)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel_lr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_fold_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# model_grid.summary()\n",
    "\n",
    "# grid_path=\"{}/CV/{}/{}/{}/{}\".format('/users/qdb16186/CNN_multitask_grid',resample,model_name,prop,GSHT)\n",
    "#### CALL BACKS!\n",
    "# es_grid = EarlyStopping(monitor='val_loss', \n",
    "#    mode='min', \n",
    "#    verbose=1, \n",
    "#    patience=2000, \n",
    "#    restore_best_weights=True)\n",
    "# CSV Logger\n",
    "# csv_logger_grid = CSVLogger(f\"{grid_path}/model_history_log_resample_{resample}_grid.csv\", \n",
    "#                        append=True)\n",
    "# CP_callbacks            \n",
    "# cp_callback1 = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1)\n",
    "# TensorBoard\n",
    "# tensorboard_callback_grid = tf.keras.callbacks.TensorBoard(log_dir=f'{grid_path}/tensorboard_logs/{resample}_grid', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "#                                                       update_freq = 1,\n",
    "#                                                       # histogram_freq=1, \n",
    "#                                                       write_graph=False, \n",
    "#                                                       write_images=False)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "\n",
    "\n",
    "x_fold_train, x_fold_test, y_fold_train, y_fold_test = wrapped_train_val_split(fold_train_index,fold_test_index,pd.DataFrame(x_train),y_train)\n",
    "model_lr.fit(x_fold_train, y_fold_train[:],\n",
    "                        epochs = 2,\n",
    "                        batch_size=16,\n",
    "                        verbose = 1,\n",
    "                        validation_data =(x_fold_test, y_fold_test[:]))\n",
    "#                callbacks=[es_grid, csv_logger_grid, tensorboard_callback_grid])\n",
    "# model_grid\n",
    "\n",
    "# model_lr.fit(x_fold_train, [scaler_dH.transform(pd.DataFrame(y_fold_train[0])),\n",
    "# scaler_dS.transform(pd.DataFrame(y_fold_train[1])),\n",
    "# scaler_dG.transform(pd.DataFrame(y_fold_train[2])),\n",
    "# scaler_Tm.transform(pd.DataFrame(y_fold_train[3]))],\n",
    "#                         epochs = 2,\n",
    "#                          batch_size=16,\n",
    "#                         verbose = 2,\n",
    "#                         validation_data =(x_fold_test, [scaler_dH.transform(pd.DataFrame(y_fold_test[0])),\n",
    "# scaler_dS.transform(pd.DataFrame(y_fold_test[1])),\n",
    "# scaler_dG.transform(pd.DataFrame(y_fold_test[2])),\n",
    "# scaler_Tm.transform(pd.DataFrame(y_fold_test[3]))]))\n",
    "#               callbacks=[es_grid])#, csv_logger_grid, tensorboard_callback_grid])\n",
    "\n",
    "\n",
    "# model_grid.fit([scaller_x.transform(pd.DataFrame(x_fold_train))], [scaler_dH.transform(pd.DataFrame(y_fold_train[0])),\n",
    "# scaler_dS.transform(pd.DataFrame(y_fold_train[1])),\n",
    "# scaler_dG.transform(pd.DataFrame(y_fold_train[2])),\n",
    "# scaler_Tm.transform(pd.DataFrame(y_fold_train[3]))],\n",
    "#                         epochs = 200,\n",
    "#                         batch_size=16,\n",
    "#                         verbose = 3,\n",
    "#                         validation_data =([scaller_x.transform(pd.DataFrame(x_fold_test))], [scaler_dH.transform(pd.DataFrame(y_fold_test[0])),\n",
    "# scaler_dS.transform(pd.DataFrame(y_fold_test[1])),\n",
    "# scaler_dG.transform(pd.DataFrame(y_fold_test[2])),\n",
    "# scaler_Tm.transform(pd.DataFrame(y_fold_test[3]))]),\n",
    "#                callbacks=[es_grid, csv_logger_grid, tensorboard_callback_grid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "8aaede43-ab22-4793-90cd-6bb4ae6de9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mse'"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_grid.predict(x_fold_train)\n",
    "# x_fold_train\n",
    "model_grid.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "ac58f9d3-5fb9-4b62-be13-09b03907588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "grid_eval_df =  grid_pred(grid_path,grid_trial_number,model_lr,x_fold_test,y_fold_test)\n",
    "# grid_eval_df =  grid_pred(grid_path,grid_trial_number,model_grid,scaller_x.transform((x_fold_test)),y_fold_test)\n",
    "# grid_eval_df =  grid_pred(grid_path,grid_trial_number,model_grid,x_fold_train,y_fold_train)\n",
    "# scallers_grids=[scaler_dH,scaler_dS,scaler_dG,\n",
    "#                 scaler_Tm]\n",
    "i=-1\n",
    "for string in GSHT_list:\n",
    "    i+=1\n",
    "    grid_eval_df[f'y_{string}_pred']=scallers_grids[i].inverse_transform(pd.DataFrame(grid_eval_df[f'y_{string}_pred']))\n",
    "# model_grid(x_fold_train)\n",
    "# grid_eval_df\n",
    "# grid_y_pred_train_cv = model_grid.predict(x_fold_test)\n",
    "# grid_y_pred_train_cv_enthalpy = grid_y_pred_train_cv[0].squeeze()\n",
    "# grid_y_pred_train_cv_entropy = grid_y_pred_train_cv[1].squeeze()\n",
    "# grid_y_pred_train_cv_free_energy = grid_y_pred_train_cv[2].squeeze()\n",
    "# grid_y_pred_train_cv_Tm = grid_y_pred_train_cv[3].squeeze()\n",
    "\n",
    "# grid_y_train_resample_output = pd.DataFrame()\n",
    "# # y_train_resample_output['ID'] = train_idx[fold_test_index]\n",
    "# # y_train_resample_output['Temp'] = train_resample['Temp']\n",
    "\n",
    "# grid_y_train_resample_output['y_dH'] = y_fold_test[0]\n",
    "# grid_y_train_resample_output['y_dS'] = y_fold_test[1]\n",
    "# grid_y_train_resample_output['y_dG'] = y_fold_test[2]\n",
    "# grid_y_train_resample_output['y_Tm'] = y_fold_test[3]\n",
    "\n",
    "# grid_y_train_resample_output[\"y_dH_pred\"]=grid_y_pred_train_cv_enthalpy\n",
    "# grid_y_train_resample_output[\"y_dS_pred\"]=grid_y_pred_train_cv_entropy\n",
    "# grid_y_train_resample_output[\"y_dG_pred\"]=grid_y_pred_train_cv_free_energy\n",
    "# grid_y_train_resample_output[\"y_Tm_pred\"]=grid_y_pred_train_cv_Tm\n",
    "\n",
    "# grid_y_train_resample_output.to_csv(f'{grid_path}/true_pred_grid_trial_{grid_trial_number}.csv')\n",
    "# grid_y_train_resample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdb133-f82a-4005-b8cb-5df0731c02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_eval_df.columns\n",
    "grid_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bdbc7-d018-40f7-a2bf-53c95381300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[scaler_dH.transform(pd.DataFrame(y_fold_test[0])),\n",
    "scaler_dS.transform(pd.DataFrame(y_fold_test[1])),\n",
    "scaler_dG.transform(pd.DataFrame(y_fold_test[2])),\n",
    "scaler_Tm.transform(pd.DataFrame(y_fold_test[3]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "5c5a2715-0787-4b71-a02a-d8fbb2fd126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH:  0.727 9.367 -2.031 9.144 0.734 -19.227 87.737 6.427\n",
      "dS:  0.614 29.792 -4.632 29.430 0.742 -50.819 887.557 20.804\n",
      "dG:  0.738 1.794 -0.242 1.778 0.713 -2.894 3.220 1.361\n",
      "Tm:  0.493 8.840 0.675 8.815 0.453 23.094 78.154 7.111\n",
      "  :  r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(grid_eval_df['y_Tm'],grid_eval_df['y_Tm_pred'])\n",
    "string='Tm'\n",
    "scallers_grids=[scaler_dH,scaler_dS,scaler_dG,\n",
    "                scaler_Tm]\n",
    "\n",
    "for string in GSHT_list:\n",
    "    r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats(grid_eval_df,string)\n",
    "    print(f'{string}: ', r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae)\n",
    "print(f'  : ','r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc346e5a-1215-45d9-b031-c2a0b818a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "b7a4bc61-a51c-4e38-b07e-ac7eeb899e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_pred(grid_path,grid_trial_number,model,x_fold_test,y_fold_test): \n",
    "    y_pred_train_cv = model.predict(x_fold_test)\n",
    "    y_pred_train_cv_enthalpy = y_pred_train_cv[0].squeeze()\n",
    "    y_pred_train_cv_entropy = y_pred_train_cv[1].squeeze()\n",
    "    y_pred_train_cv_free_energy = y_pred_train_cv[2].squeeze()\n",
    "    y_pred_train_cv_Tm = y_pred_train_cv[3].squeeze()\n",
    "    \n",
    "    y_train_resample_output = pd.DataFrame()\n",
    "    # y_train_resample_output['ID'] = train_idx[fold_test_index]\n",
    "    # y_train_resample_output['Temp'] = train_resample['Temp']\n",
    "    \n",
    "    y_train_resample_output['y_dH'] = y_fold_test[0]\n",
    "    y_train_resample_output['y_dS'] = y_fold_test[1]\n",
    "    y_train_resample_output['y_dG'] = y_fold_test[2]\n",
    "    y_train_resample_output['y_Tm'] = y_fold_test[3]\n",
    "    \n",
    "    y_train_resample_output[\"y_dH_pred\"]=y_pred_train_cv_enthalpy\n",
    "    y_train_resample_output[\"y_dS_pred\"]=y_pred_train_cv_entropy\n",
    "    y_train_resample_output[\"y_dG_pred\"]=y_pred_train_cv_free_energy\n",
    "    y_train_resample_output[\"y_Tm_pred\"]=y_pred_train_cv_Tm\n",
    "    \n",
    "    y_train_resample_output.to_csv(f'{grid_path}/true_pred_grid_trial_{grid_trial_number}.csv')\n",
    "    return y_train_resample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18594e4-ee98-4e2d-a148-616b6e65dcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11341ec9-f0b2-4b30-a2f0-e26a204fae1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c83461-31b0-43ab-b5ea-12e7f11016aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91a75f-d759-46bb-b092-490304050eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "# desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=50\n",
    "n_folds=5\n",
    "# n_jobs=1\n",
    "# epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(1, test_size=test_frac, random_state=1)\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1\n",
    "    # add a resample skipper\n",
    "    # print(f'resample: {resample}')\n",
    "    if resample!=1:\n",
    "        break\n",
    "\n",
    "    print(f'RUN resample: {resample}')\n",
    "\n",
    "    # ### DEFINE MODEL\n",
    "    model_name = f'{prop}_multi_task_CNN'\n",
    "    x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "    \n",
    "    # fold splits\n",
    "    kf=KFold(n_splits=n_folds)\n",
    "    keyList = ['trial',\n",
    "                 'model_type1',\n",
    "                 'model_type',\n",
    "                     'layer_3_3',\n",
    "                     'layer_2_2',\n",
    "                     'layer_1',\n",
    "               'r2_dH_0','rmsd_dH_0','bias_dH_0','SDEP_dH_0','gradient_dH_0','b_dH_0','mse_dH_0','mae_dH_0',\n",
    "               'r2_dH_1','rmsd_dH_1','bias_dH_1','SDEP_dH_1','gradient_dH_1','b_dH_1','mse_dH_1','mae_dH_1',\n",
    "               'r2_dH_2','rmsd_dH_2','bias_dH_2','SDEP_dH_2','gradient_dH_2','b_dH_2','mse_dH_2','mae_dH_2',\n",
    "               'r2_dH_3','rmsd_dH_3','bias_dH_3','SDEP_dH_3','gradient_dH_3','b_dH_3','mse_dH_3','mae_dH_3',\n",
    "               'r2_dH_4','rmsd_dH_4','bias_dH_4','SDEP_dH_4','gradient_dH_4','b_dH_4','mse_dH_4','mae_dH_4',\n",
    "               'r2_dH_mean','rmsd_dH_mean','bias_dH_mean','SDEP_dH_mean','gradient_dH_mean','b_dH_mean','mse_dH_mean','mae_dH_mean',\n",
    "               'r2_dH_std','rmsd_dH_std','bias_dH_std','SDEP_dH_std','gradient_dH_std','b_dH_std','mse_dH_std','mae_dH_std',\n",
    "               'r2_dS_0','rmsd_dS_0','bias_dS_0','SDEP_dS_0','gradient_dS_0','b_dS_0','mse_dS_0','mae_dS_0',\n",
    "               'r2_dS_1','rmsd_dS_1','bias_dS_1','SDEP_dS_1','gradient_dS_1','b_dS_1','mse_dS_1','mae_dS_1',\n",
    "               'r2_dS_2','rmsd_dS_2','bias_dS_2','SDEP_dS_2','gradient_dS_2','b_dS_2','mse_dS_2','mae_dS_2',\n",
    "               'r2_dS_3','rmsd_dS_3','bias_dS_3','SDEP_dS_3','gradient_dS_3','b_dS_3','mse_dS_3','mae_dS_3',\n",
    "               'r2_dS_4','rmsd_dS_4','bias_dS_4','SDEP_dS_4','gradient_dS_4','b_dS_4','mse_dS_4','mae_dS_4',\n",
    "               'r2_dS_mean','rmsd_dS_mean','bias_dS_mean','SDEP_dS_mean','gradient_dS_mean','b_dS_mean','mse_dS_mean','mae_dS_mean',\n",
    "               'r2_dS_std','rmsd_dS_std','bias_dS_std','SDEP_dS_std','gradient_dS_std','b_dS_std','mse_dS_std','mae_dS_std',\n",
    "               'r2_dG_0','rmsd_dG_0','bias_dG_0','SDEP_dG_0','gradient_dG_0','b_dG_0','mse_dG_0','mae_dG_0',\n",
    "               'r2_dG_1','rmsd_dG_1','bias_dG_1','SDEP_dG_1','gradient_dG_1','b_dG_1','mse_dG_1','mae_dG_1',\n",
    "               'r2_dG_2','rmsd_dG_2','bias_dG_2','SDEP_dG_2','gradient_dG_2','b_dG_2','mse_dG_2','mae_dG_2',\n",
    "               'r2_dG_3','rmsd_dG_3','bias_dG_3','SDEP_dG_3','gradient_dG_3','b_dG_3','mse_dG_3','mae_dG_3',\n",
    "               'r2_dG_4','rmsd_dG_4','bias_dG_4','SDEP_dG_4','gradient_dG_4','b_dG_4','mse_dG_4','mae_dG_4',\n",
    "               'r2_dG_mean','rmsd_dG_mean','bias_dG_mean','SDEP_dG_mean','gradient_dG_mean','b_dG_mean','mse_dG_mean','mae_dG_mean',\n",
    "               'r2_dG_std','rmsd_dG_std','bias_dG_std','SDEP_dG_std','gradient_dG_std','b_dG_std','mse_dG_std','mae_dG_std',\n",
    "\n",
    "               'r2_Tm_0','rmsd_Tm_0','bias_Tm_0','SDEP_Tm_0','gradient_Tm_0','b_Tm_0','mse_Tm_0','mae_Tm_0',\n",
    "               'r2_Tm_1','rmsd_Tm_1','bias_Tm_1','SDEP_Tm_1','gradient_Tm_1','b_Tm_1','mse_Tm_1','mae_Tm_1',\n",
    "               'r2_Tm_2','rmsd_Tm_2','bias_Tm_2','SDEP_Tm_2','gradient_Tm_2','b_Tm_2','mse_Tm_2','mae_Tm_2',\n",
    "               'r2_Tm_3','rmsd_Tm_3','bias_Tm_3','SDEP_Tm_3','gradient_Tm_3','b_Tm_3','mse_Tm_3','mae_Tm_3',\n",
    "               'r2_Tm_4','rmsd_Tm_4','bias_Tm_4','SDEP_Tm_4','gradient_Tm_4','b_Tm_4','mse_Tm_4','mae_Tm_4',\n",
    "               'r2_Tm_mean','rmsd_Tm_mean','bias_Tm_mean','SDEP_Tm_mean','gradient_Tm_mean','b_Tm_mean','mse_Tm_mean','mae_Tm_mean',\n",
    "               'r2_Tm_std','rmsd_Tm_std','bias_Tm_std','SDEP_Tm_std','gradient_Tm_std','b_Tm_std','mse_Tm_std','mae_Tm_std',\n",
    "              ]\n",
    "    \n",
    "    n = dict(zip(keyList, [None]*len(keyList)))\n",
    "    df_grid=pd.DataFrame(n,index=['a'])\n",
    "    \n",
    "    for i_fold, (fold_train_index, fold_test_index) in enumerate(kf.split(x_train)):\n",
    "        # print('fold: ', i_fold)\n",
    "        print('RUN fold',i_fold)\n",
    "        # add a fold skipper\n",
    "        x_fold_train, x_fold_test, y_fold_train, y_fold_test = wrapped_train_val_split(fold_train_index,fold_test_index,pd.DataFrame(x_train),y_train)\n",
    "\n",
    "        path=\"{}/CV/{}/{}/{}/{}/fold_{}\".format('/users/qdb16186/CNN_multitask_grid',resample,model_name,prop,GSHT,i_fold)\n",
    "        tuner = kt.GridSearch(build_model,\n",
    "                                   objective=kt.Objective('val_loss', 'min'),\n",
    "                                    # loss = 'val_loss',\n",
    "                                   # objective = ['val_mse','epoch_entropy_pred_mse','val_free_energy_pred_mse'],\n",
    "                                  directory=f'{path}/hyper_param_tunning_tunner_fold_{i_fold}',\n",
    "                                  overwrite=False,\n",
    "                                  project_name=f'{batch}')\n",
    "        # Find all trial number and cycle through them\n",
    "        selection=os.listdir(tuner.project_dir)\n",
    "        for dir_ in selection:\n",
    "            if \"trial_\" not in dir_:\n",
    "                continue\n",
    "            # trial_number='0036'\n",
    "            trial_number=str(dir_.replace(\"trial_\",\"\"))\n",
    "            trial=tuner.oracle.get_trial(trial_number)\n",
    "            model=tuner.load_model(trial)\n",
    "            \n",
    "            y_pred_train_cv = model.predict(x_fold_test)\n",
    "            y_pred_train_cv_enthalpy = y_pred_train_cv[0].squeeze()\n",
    "            y_pred_train_cv_entropy = y_pred_train_cv[1].squeeze()\n",
    "            y_pred_train_cv_free_energy = y_pred_train_cv[2].squeeze()\n",
    "            y_pred_train_cv_Tm = y_pred_train_cv[3].squeeze()\n",
    "    \n",
    "            y_train_resample_output = pd.DataFrame()\n",
    "            y_train_resample_output['ID'] = train_idx[fold_test_index]\n",
    "            # y_train_resample_output['Temp'] = train_resample['Temp']\n",
    "            \n",
    "            y_train_resample_output['y_dH'] = y_fold_test[0]\n",
    "            y_train_resample_output['y_dS'] = y_fold_test[1]\n",
    "            y_train_resample_output['y_dG'] = y_fold_test[2]\n",
    "            y_train_resample_output['y_Tm'] = y_fold_test[3]\n",
    "            \n",
    "            y_train_resample_output[\"y_dH_pred\"]=y_pred_train_cv_enthalpy\n",
    "            y_train_resample_output[\"y_dS_pred\"]=y_pred_train_cv_entropy\n",
    "            y_train_resample_output[\"y_dG_pred\"]=y_pred_train_cv_free_energy\n",
    "            y_train_resample_output[\"y_Tm_pred\"]=y_pred_train_cv_Tm\n",
    "    \n",
    "            y_train_resample_output.to_csv(f'{path}/true_pred_fold_{i_fold}_trial_{trial_number}.csv')\n",
    "            for string in GSHT_list:\n",
    "                r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats(y_train_resample_output,string)\n",
    "                if trial_number in df_grid.index:\n",
    "                    n = df_grid.loc[trial_number].to_dict()\n",
    "                # else:\n",
    "                # n = df.loc[trial_number].to_dict() if trial_number in df_grid.index else {}\n",
    "                n.update({\"trial\":trial_number,})\n",
    "                n.update({f'r2_{string}_{i_fold}':r2,f'rmsd_{string}_{i_fold}': rmsd, \n",
    "                          f'bias_{string}_{i_fold}': bias, f'SDEP_{string}_{i_fold}': sdep,\n",
    "                          f'gradient_{string}_{i_fold}': plot_a, f'b_{string}_{i_fold}': plot_b, \n",
    "                          f'mse_{string}_{i_fold}':mse, f'mae_{string}_{i_fold}':mae,})\n",
    "                \n",
    "                original_stdout = sys.stdout \t\n",
    "    \n",
    "                with open('temp.txt', 'w') as f:\n",
    "                    sys.stdout = f\n",
    "                    # tunner.results_summary(5)\n",
    "                    trial.display_hyperparameters()\n",
    "                    # Reset the standard output\n",
    "                    sys.stdout = original_stdout \n",
    "                f.close\n",
    "                \n",
    "                # with open('temp_monitor.txt', 'a+') as f:\n",
    "                #     sys.stdout = f\n",
    "                #     # tunner.results_summary(5)\n",
    "                #     trial.display_hyperparameters()\n",
    "                #     # Reset the standard output\n",
    "                #     sys.stdout = original_stdout \n",
    "                # f.close\n",
    "    \n",
    "                data = open('temp.txt', 'r').read()\n",
    "                data_hyp_param=data.replace(': ','\\n').split('\\n')\n",
    "                i_index=0\n",
    "                while i_index+1 < len(data_hyp_param):\n",
    "                    # print(i_index,len(data_hyp_param))\n",
    "                    n.update({data_hyp_param[i_index]:data_hyp_param[i_index+1]})\n",
    "                    i_index+=2\n",
    "                    \n",
    "                df_grid.loc[f'{trial_number}']=n\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "ca800c4d-e733-4b5a-829d-508829904065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0028'"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# n\n",
    "# df_grid= df_grid.drop('a',axis=0)\n",
    "df_grid=df_grid.sort_values('trial')\n",
    "df_temp=pd.DataFrame()\n",
    "# df_temp['Tm_min']=df_grid[['r2_Tm_0', 'r2_Tm_1', 'r2_Tm_2','r2_Tm_3','r2_Tm_4']].min(axis=1)\n",
    "# df_temp['Tm_min'].min(axis=1)\n",
    "\n",
    "df_grid.iloc[:, 5:] = df_grid.iloc[:, 5:].astype(float)\n",
    "for string in GSHT_list:\n",
    "\n",
    "    df_temp[f'{string}_mean']=df_grid[[f'r2_{string}_0', f'r2_{string}_1', f'r2_{string}_2',f'r2_{string}_3',f'r2_{string}_4']].mean(axis=1)\n",
    "    df_temp[f'{string}_std'] =df_grid[[f'r2_{string}_0', f'r2_{string}_1', f'r2_{string}_2',f'r2_{string}_3',f'r2_{string}_4']].std(axis=1)\n",
    "    # df_temp[f'{string}_mean']=df_grid[[f'rmsd_{string}_0', f'rmsd_{string}_1', f'rmsd_{string}_2',f'rmsd_{string}_3',f'rmsd_{string}_4']].mean(axis=1)\n",
    "    # df_temp[f'{string}_std'] =df_grid[[f'rmsd_{string}_0', f'rmsd_{string}_1', f'rmsd_{string}_2',f'rmsd_{string}_3',f'rmsd_{string}_4']].std(axis=1)\n",
    "# df_temp\n",
    "# df_temp.idxmax()\n",
    "# df_temp.iloc[138,:]\n",
    "df_temp\n",
    "# \n",
    "df_grid[['r2_Tm_0', 'r2_Tm_1', 'r2_Tm_2','r2_Tm_3','r2_Tm_4']].max()\n",
    "# df_temp['Tm_min'].min(axis=1)\n",
    "df_temp=df_temp.apply(pd.to_numeric)\n",
    "\n",
    "df_temp['ratio_dH']=(df_temp[\"dH_mean\"]/df_temp[\"dH_mean\"].min())\n",
    "df_temp['ratio_dG']=(df_temp[\"dG_mean\"]/df_temp[\"dG_mean\"].min())\n",
    "df_temp['ratio_dS']=(df_temp[\"dS_mean\"]/df_temp[\"dS_mean\"].min())\n",
    "df_temp['ratio_Tm']=(df_temp[\"Tm_mean\"]/df_temp[\"Tm_mean\"].min())\n",
    "df_temp['selection']=df_temp['ratio_dH']+df_temp['ratio_dG']+df_temp['ratio_dS']+df_temp['ratio_Tm']\n",
    "\n",
    "# df_temp['sum_ratio_rmsd_rmsd_best']=((df_temp[\"dH_mean\"]/df_temp[\"dH_mean\"].min()))\n",
    "df_temp.dtypes\n",
    "df_temp['selection'].min()\n",
    "df_temp.index[df_temp['selection'] == df_temp['selection'].min()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "53c188cc-f0b1-461b-acc9-8b113e0b05e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321285786079102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'convn': 2, 'dense1': 16, 'dense2': 16, 'dense3': 8}\""
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['dH_mean'].min()\n",
    "st_grid=pd.read_csv('/users/qdb16186/CNN_single_task_grid/CV/1/CNN_single_task_Granulated_dH_10/Granulated/dH/gridsearch_resample_1_pipe_cond_No_scalling.csv')\n",
    "print(st_grid['mean_test_r2'].max())\n",
    "# st_grid.columns\n",
    "best_index=st_grid.index[st_grid['mean_test_r2'] == st_grid['mean_test_r2'].max()]\n",
    "st_grid.loc[best_index[0]]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "cfca1461-3dcf-4c34-8f4e-d10ff4025c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid.to_csv(\"{}/CV/{}/{}/{}/{}\".format('/users/qdb16186/CNN_multitask_grid',resample,model_name,prop,GSHT)+f'/Gridsearch_resample_{resample}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7e4d2897-56fb-436a-aac9-60d2fc3f57bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 4 43\n",
      "170 4 170\n",
      "213 92\n",
      "213 92 4 4\n",
      "170 43\n",
      "170 43 170 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([135, 103, 260, 223, 226, 190, 270,  92, 297, 205, 261, 290, 273,\n",
       "       301, 126, 280,  80, 148,  63, 219,  54, 274,   3, 218,   9, 209,\n",
       "        43, 201, 197,  51, 215, 241, 245, 288, 242, 200, 208, 165,  40,\n",
       "         5, 172, 222,  38])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_fold_test[0])\n",
    "len(fold_test_index)\n",
    "# len(y_train_resample_output)\n",
    "len(x_fold_train)#x_fold_test, y_fold_train, y_fold_test = wrapped_train_val_split(fold_train_index,fold_test_index\n",
    "len(fold_train_index)\n",
    "fold_train_index\n",
    "fold_test_index\n",
    "x_train\n",
    "fold_test_index\n",
    "print(len(x_fold_test),len(y_fold_test),len(x_fold_test))\n",
    "print(len(x_fold_train),len(y_fold_train),len(x_fold_train))\n",
    "print(len(train_idx),len(test_idx))\n",
    "\n",
    "print( len(x_train), len(x_test), len(y_train), len(y_test))\n",
    "print(len(fold_train_index), len(fold_test_index))\n",
    "print(len(x_fold_train), len(x_fold_test), len(y_fold_train[0]), len(y_fold_test[0]))\n",
    "train_idx[fold_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "d002e930-fc5f-4c9e-8420-237e303fb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df2,string):\n",
    "    y_test = df2[f'y_{string}']\n",
    "    y_pred_test = df2[f'y_{string}_pred']\n",
    "    y_test_np = y_test.to_numpy()\n",
    "    y_pred_test_np = y_pred_test.to_numpy()\n",
    "    r2_test = r2_score(y_test_np, y_pred_test_np)\n",
    "    rmsd_test = (mean_squared_error(y_test_np, y_pred_test_np))**0.5\n",
    "    bias_test = np.mean(y_pred_test_np - y_test_np)\n",
    "    sdep_test = (np.mean((y_pred_test_np - y_test_np - bias_test)**2))**0.5\n",
    "    r2 = '{:.3f}'.format(r2_test)\n",
    "    rmsd = '{:.3f}'.format(rmsd_test)\n",
    "    bias = '{:.3f}'.format(bias_test)\n",
    "    sdep = '{:.3f}'.format(sdep_test)\n",
    "    \n",
    "    \n",
    "    mse=mean_squared_error(y_test_np, y_pred_test_np)\n",
    "    mse='{:.3f}'.format(mse)\n",
    "    mae=mean_absolute_error(y_test_np, y_pred_test_np)\n",
    "    mae='{:.3f}'.format(mae)\n",
    "    try:\n",
    "        a, b = np.polyfit(df2[f'y_{string}'], df2[f'y_{string}_pred'], 1)\n",
    "        plot_a = '{:.3f}'.format(a)\n",
    "        plot_b = '{:.3f}'.format(b)\n",
    "    except np.linalg.LinAlgError:\n",
    "        pass\n",
    "    \n",
    "    return r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "db907e0d-9b07-4fdf-b057-1d4754fec415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y_dH</th>\n",
       "      <th>y_dS</th>\n",
       "      <th>y_dG</th>\n",
       "      <th>y_Tm</th>\n",
       "      <th>y_dH_pred</th>\n",
       "      <th>y_dS_pred</th>\n",
       "      <th>y_dG_pred</th>\n",
       "      <th>y_Tm_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>-60.7</td>\n",
       "      <td>-167.0</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-61.500645</td>\n",
       "      <td>-169.027771</td>\n",
       "      <td>-8.984352</td>\n",
       "      <td>37.476528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290</td>\n",
       "      <td>-50.5</td>\n",
       "      <td>-137.0</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-60.055622</td>\n",
       "      <td>-165.068069</td>\n",
       "      <td>-8.752142</td>\n",
       "      <td>36.608147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192</td>\n",
       "      <td>-59.2</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>45.8</td>\n",
       "      <td>-62.001411</td>\n",
       "      <td>-170.477493</td>\n",
       "      <td>-9.067005</td>\n",
       "      <td>37.777699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224</td>\n",
       "      <td>-61.2</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>31.9</td>\n",
       "      <td>-60.062950</td>\n",
       "      <td>-164.951431</td>\n",
       "      <td>-8.784157</td>\n",
       "      <td>36.585480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>284</td>\n",
       "      <td>-58.8</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-7.8</td>\n",
       "      <td>36.9</td>\n",
       "      <td>-61.623791</td>\n",
       "      <td>-169.437164</td>\n",
       "      <td>-9.003925</td>\n",
       "      <td>37.559486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>267</td>\n",
       "      <td>-62.6</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>38.9</td>\n",
       "      <td>-60.805748</td>\n",
       "      <td>-167.027512</td>\n",
       "      <td>-8.888939</td>\n",
       "      <td>37.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>-62.8</td>\n",
       "      <td>-174.0</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>-62.632305</td>\n",
       "      <td>-172.098099</td>\n",
       "      <td>-9.159590</td>\n",
       "      <td>38.164021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>-104.4</td>\n",
       "      <td>-281.0</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>67.3</td>\n",
       "      <td>-116.605957</td>\n",
       "      <td>-321.178894</td>\n",
       "      <td>-16.950855</td>\n",
       "      <td>71.466354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91</td>\n",
       "      <td>-38.6</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>20.1</td>\n",
       "      <td>-44.779964</td>\n",
       "      <td>-122.986465</td>\n",
       "      <td>-6.475912</td>\n",
       "      <td>27.279692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>208</td>\n",
       "      <td>-59.3</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>44.6</td>\n",
       "      <td>-63.594009</td>\n",
       "      <td>-174.698700</td>\n",
       "      <td>-9.309534</td>\n",
       "      <td>38.759052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>174</td>\n",
       "      <td>-67.3</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>41.9</td>\n",
       "      <td>-61.848953</td>\n",
       "      <td>-169.924149</td>\n",
       "      <td>-9.048605</td>\n",
       "      <td>37.676212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44</td>\n",
       "      <td>-53.1</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-62.269424</td>\n",
       "      <td>-171.119095</td>\n",
       "      <td>-9.087887</td>\n",
       "      <td>37.959454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>171</td>\n",
       "      <td>-56.3</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>38.5</td>\n",
       "      <td>-61.502449</td>\n",
       "      <td>-168.878647</td>\n",
       "      <td>-9.004814</td>\n",
       "      <td>37.468472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>150</td>\n",
       "      <td>-119.6</td>\n",
       "      <td>-325.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>67.9</td>\n",
       "      <td>-114.980453</td>\n",
       "      <td>-316.653229</td>\n",
       "      <td>-16.718834</td>\n",
       "      <td>70.460312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>297</td>\n",
       "      <td>-57.6</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>-7.9</td>\n",
       "      <td>37.1</td>\n",
       "      <td>-59.366970</td>\n",
       "      <td>-163.167053</td>\n",
       "      <td>-8.647593</td>\n",
       "      <td>36.185757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>278</td>\n",
       "      <td>-54.7</td>\n",
       "      <td>-149.0</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>42.6</td>\n",
       "      <td>-61.102997</td>\n",
       "      <td>-167.974350</td>\n",
       "      <td>-8.903009</td>\n",
       "      <td>37.255997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>41</td>\n",
       "      <td>-48.7</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>-6.8</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-46.374832</td>\n",
       "      <td>-127.380173</td>\n",
       "      <td>-6.693716</td>\n",
       "      <td>28.279146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>79</td>\n",
       "      <td>-122.9</td>\n",
       "      <td>-336.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>66.7</td>\n",
       "      <td>-114.941933</td>\n",
       "      <td>-316.607971</td>\n",
       "      <td>-16.712296</td>\n",
       "      <td>70.429260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>288</td>\n",
       "      <td>-51.7</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>29.8</td>\n",
       "      <td>-59.139492</td>\n",
       "      <td>-162.537613</td>\n",
       "      <td>-8.646002</td>\n",
       "      <td>36.025311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>231</td>\n",
       "      <td>-62.8</td>\n",
       "      <td>-172.0</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>44.6</td>\n",
       "      <td>-63.305618</td>\n",
       "      <td>-174.008377</td>\n",
       "      <td>-9.259054</td>\n",
       "      <td>38.576027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>164</td>\n",
       "      <td>-66.2</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-61.123360</td>\n",
       "      <td>-167.907867</td>\n",
       "      <td>-8.937117</td>\n",
       "      <td>37.244137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>117</td>\n",
       "      <td>-76.7</td>\n",
       "      <td>-216.0</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>44.3</td>\n",
       "      <td>-68.941452</td>\n",
       "      <td>-189.559967</td>\n",
       "      <td>-10.108204</td>\n",
       "      <td>42.024044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>256</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>-157.0</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>27.5</td>\n",
       "      <td>-60.044678</td>\n",
       "      <td>-164.962204</td>\n",
       "      <td>-8.773567</td>\n",
       "      <td>36.582729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>99</td>\n",
       "      <td>-62.1</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>44.3</td>\n",
       "      <td>-62.769733</td>\n",
       "      <td>-172.525772</td>\n",
       "      <td>-9.175699</td>\n",
       "      <td>38.251621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>266</td>\n",
       "      <td>-128.0</td>\n",
       "      <td>-352.0</td>\n",
       "      <td>-18.9</td>\n",
       "      <td>65.8</td>\n",
       "      <td>-117.575027</td>\n",
       "      <td>-323.923157</td>\n",
       "      <td>-17.106289</td>\n",
       "      <td>72.029922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>-56.9</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>31.7</td>\n",
       "      <td>-59.353443</td>\n",
       "      <td>-162.985809</td>\n",
       "      <td>-8.692444</td>\n",
       "      <td>36.135601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>236</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-64.177086</td>\n",
       "      <td>-176.364990</td>\n",
       "      <td>-9.384655</td>\n",
       "      <td>39.115761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>189</td>\n",
       "      <td>-58.1</td>\n",
       "      <td>-156.0</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>46.7</td>\n",
       "      <td>-63.382935</td>\n",
       "      <td>-174.227600</td>\n",
       "      <td>-9.277328</td>\n",
       "      <td>38.626846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>197</td>\n",
       "      <td>-60.4</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>47.1</td>\n",
       "      <td>-63.017776</td>\n",
       "      <td>-173.225327</td>\n",
       "      <td>-9.215366</td>\n",
       "      <td>38.402020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>-44.2</td>\n",
       "      <td>-123.0</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>24.2</td>\n",
       "      <td>-52.203266</td>\n",
       "      <td>-143.623627</td>\n",
       "      <td>-7.549551</td>\n",
       "      <td>31.842447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>293</td>\n",
       "      <td>-61.6</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>41.7</td>\n",
       "      <td>-62.452305</td>\n",
       "      <td>-171.654404</td>\n",
       "      <td>-9.130613</td>\n",
       "      <td>38.061607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>55</td>\n",
       "      <td>-69.1</td>\n",
       "      <td>-195.0</td>\n",
       "      <td>-8.7</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-74.179382</td>\n",
       "      <td>-203.796646</td>\n",
       "      <td>-10.887451</td>\n",
       "      <td>45.231174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>28</td>\n",
       "      <td>-60.7</td>\n",
       "      <td>-171.0</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>35.6</td>\n",
       "      <td>-61.148373</td>\n",
       "      <td>-167.939224</td>\n",
       "      <td>-8.953857</td>\n",
       "      <td>37.240925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>291</td>\n",
       "      <td>-57.5</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>34.9</td>\n",
       "      <td>-60.677689</td>\n",
       "      <td>-166.658188</td>\n",
       "      <td>-8.878469</td>\n",
       "      <td>36.956135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>246</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>47.3</td>\n",
       "      <td>-63.109322</td>\n",
       "      <td>-173.481094</td>\n",
       "      <td>-9.213728</td>\n",
       "      <td>38.472977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>175</td>\n",
       "      <td>-57.5</td>\n",
       "      <td>-162.0</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>-59.721329</td>\n",
       "      <td>-164.012863</td>\n",
       "      <td>-8.739475</td>\n",
       "      <td>36.365532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>-76.3</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>37.5</td>\n",
       "      <td>-76.864960</td>\n",
       "      <td>-211.305756</td>\n",
       "      <td>-11.297855</td>\n",
       "      <td>46.871342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>249</td>\n",
       "      <td>-56.8</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>45.5</td>\n",
       "      <td>-62.838963</td>\n",
       "      <td>-172.741104</td>\n",
       "      <td>-9.169207</td>\n",
       "      <td>38.310787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>33</td>\n",
       "      <td>-54.6</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>-61.603630</td>\n",
       "      <td>-169.334854</td>\n",
       "      <td>-9.010004</td>\n",
       "      <td>37.535450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>35</td>\n",
       "      <td>-59.3</td>\n",
       "      <td>-167.0</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>34.7</td>\n",
       "      <td>-61.344913</td>\n",
       "      <td>-168.508621</td>\n",
       "      <td>-8.980696</td>\n",
       "      <td>37.367928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>212</td>\n",
       "      <td>-59.6</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>48.6</td>\n",
       "      <td>-48.617367</td>\n",
       "      <td>-133.631424</td>\n",
       "      <td>-7.042521</td>\n",
       "      <td>29.642229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>63</td>\n",
       "      <td>-66.2</td>\n",
       "      <td>-186.0</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>39.7</td>\n",
       "      <td>-74.806274</td>\n",
       "      <td>-205.479584</td>\n",
       "      <td>-10.972466</td>\n",
       "      <td>45.629162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>225</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>38.7</td>\n",
       "      <td>-61.629089</td>\n",
       "      <td>-169.374756</td>\n",
       "      <td>-9.003639</td>\n",
       "      <td>37.556637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID   y_dH   y_dS  y_dG  y_Tm   y_dH_pred   y_dS_pred  y_dG_pred  \\\n",
       "0   188  -60.7 -167.0  -8.8  42.0  -61.500645 -169.027771  -8.984352   \n",
       "1   290  -50.5 -137.0  -8.2  40.0  -60.055622 -165.068069  -8.752142   \n",
       "2   192  -59.2 -160.0  -9.5  45.8  -62.001411 -170.477493  -9.067005   \n",
       "3   224  -61.2 -175.0  -6.9  31.9  -60.062950 -164.951431  -8.784157   \n",
       "4   284  -58.8 -164.0  -7.8  36.9  -61.623791 -169.437164  -9.003925   \n",
       "5   267  -62.6 -175.0  -8.3  38.9  -60.805748 -167.027512  -8.888939   \n",
       "6    19  -62.8 -174.0  -8.8  41.4  -62.632305 -172.098099  -9.159590   \n",
       "7    14 -104.4 -281.0 -17.3  67.3 -116.605957 -321.178894 -16.950855   \n",
       "8    91  -38.6 -106.0  -5.7  20.1  -44.779964 -122.986465  -6.475912   \n",
       "9   208  -59.3 -161.0  -9.5  44.6  -63.594009 -174.698700  -9.309534   \n",
       "10  174  -67.3 -188.0  -9.0  41.9  -61.848953 -169.924149  -9.048605   \n",
       "11   44  -53.1 -145.0  -8.3  38.0  -62.269424 -171.119095  -9.087887   \n",
       "12  171  -56.3 -155.0  -8.3  38.5  -61.502449 -168.878647  -9.004814   \n",
       "13  150 -119.6 -325.0 -18.7  67.9 -114.980453 -316.653229 -16.718834   \n",
       "14  297  -57.6 -160.0  -7.9  37.1  -59.366970 -163.167053  -8.647593   \n",
       "15  278  -54.7 -149.0  -8.6  42.6  -61.102997 -167.974350  -8.903009   \n",
       "16   41  -48.7 -135.0  -6.8  30.0  -46.374832 -127.380173  -6.693716   \n",
       "17   79 -122.9 -336.0 -18.7  66.7 -114.941933 -316.607971 -16.712296   \n",
       "18  288  -51.7 -145.0  -6.6  29.8  -59.139492 -162.537613  -8.646002   \n",
       "19  231  -62.8 -172.0  -9.5  44.6  -63.305618 -174.008377  -9.259054   \n",
       "20  164  -66.2 -183.0  -9.3  44.2  -61.123360 -167.907867  -8.937117   \n",
       "21  117  -76.7 -216.0  -9.7  44.3  -68.941452 -189.559967 -10.108204   \n",
       "22  256  -54.5 -157.0  -5.9  27.5  -60.044678 -164.962204  -8.773567   \n",
       "23   99  -62.1 -170.0  -9.4  44.3  -62.769733 -172.525772  -9.175699   \n",
       "24  266 -128.0 -352.0 -18.9  65.8 -117.575027 -323.923157 -17.106289   \n",
       "25    8  -56.9 -161.0  -6.9  31.7  -59.353443 -162.985809  -8.692444   \n",
       "26  236  -59.0 -161.0  -9.0  43.0  -64.177086 -176.364990  -9.384655   \n",
       "27  189  -58.1 -156.0  -9.7  46.7  -63.382935 -174.227600  -9.277328   \n",
       "28  197  -60.4 -163.0  -9.8  47.1  -63.017776 -173.225327  -9.215366   \n",
       "29   31  -44.2 -123.0  -6.1  24.2  -52.203266 -143.623627  -7.549551   \n",
       "30  293  -61.6 -170.0  -8.8  41.7  -62.452305 -171.654404  -9.130613   \n",
       "31   55  -69.1 -195.0  -8.7  40.0  -74.179382 -203.796646 -10.887451   \n",
       "32   28  -60.7 -171.0  -7.7  35.6  -61.148373 -167.939224  -8.953857   \n",
       "33  291  -57.5 -161.0  -7.6  34.9  -60.677689 -166.658188  -8.878469   \n",
       "34  246  -60.0 -163.0  -9.4  47.3  -63.109322 -173.481094  -9.213728   \n",
       "35  175  -57.5 -162.0  -7.3  33.3  -59.721329 -164.012863  -8.739475   \n",
       "36   48  -76.3 -220.0  -8.2  37.5  -76.864960 -211.305756 -11.297855   \n",
       "37  249  -56.8 -154.0  -9.1  45.5  -62.838963 -172.741104  -9.169207   \n",
       "38   33  -54.6 -152.0  -7.5  34.2  -61.603630 -169.334854  -9.010004   \n",
       "39   35  -59.3 -167.0  -7.6  34.7  -61.344913 -168.508621  -8.980696   \n",
       "40  212  -59.6 -161.0  -9.6  48.6  -48.617367 -133.631424  -7.042521   \n",
       "41   63  -66.2 -186.0  -8.6  39.7  -74.806274 -205.479584 -10.972466   \n",
       "42  225  -61.0 -170.0  -8.3  38.7  -61.629089 -169.374756  -9.003639   \n",
       "\n",
       "    y_Tm_pred  \n",
       "0   37.476528  \n",
       "1   36.608147  \n",
       "2   37.777699  \n",
       "3   36.585480  \n",
       "4   37.559486  \n",
       "5   37.046700  \n",
       "6   38.164021  \n",
       "7   71.466354  \n",
       "8   27.279692  \n",
       "9   38.759052  \n",
       "10  37.676212  \n",
       "11  37.959454  \n",
       "12  37.468472  \n",
       "13  70.460312  \n",
       "14  36.185757  \n",
       "15  37.255997  \n",
       "16  28.279146  \n",
       "17  70.429260  \n",
       "18  36.025311  \n",
       "19  38.576027  \n",
       "20  37.244137  \n",
       "21  42.024044  \n",
       "22  36.582729  \n",
       "23  38.251621  \n",
       "24  72.029922  \n",
       "25  36.135601  \n",
       "26  39.115761  \n",
       "27  38.626846  \n",
       "28  38.402020  \n",
       "29  31.842447  \n",
       "30  38.061607  \n",
       "31  45.231174  \n",
       "32  37.240925  \n",
       "33  36.956135  \n",
       "34  38.472977  \n",
       "35  36.365532  \n",
       "36  46.871342  \n",
       "37  38.310787  \n",
       "38  37.535450  \n",
       "39  37.367928  \n",
       "40  29.642229  \n",
       "41  45.629162  \n",
       "42  37.556637  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resample_output\n",
    "# x_fold_test\n",
    "# fold_test_index\n",
    "# fold_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ee2992f3-0409-4d37-a185-cc3ed588b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ -60.7,  -50.5,  -59.2,  -61.2,  -58.8,  -62.6,  -62.8, -104.4,\n",
       "         -38.6,  -59.3,  -67.3,  -53.1,  -56.3, -119.6,  -57.6,  -54.7,\n",
       "         -48.7, -122.9,  -51.7,  -62.8,  -66.2,  -76.7,  -54.5,  -62.1,\n",
       "        -128. ,  -56.9,  -59. ,  -58.1,  -60.4,  -44.2,  -61.6,  -69.1,\n",
       "         -60.7,  -57.5,  -60. ,  -57.5,  -76.3,  -56.8,  -54.6,  -59.3,\n",
       "         -59.6,  -66.2,  -61. ]),\n",
       " array([-167., -137., -160., -175., -164., -175., -174., -281., -106.,\n",
       "        -161., -188., -145., -155., -325., -160., -149., -135., -336.,\n",
       "        -145., -172., -183., -216., -157., -170., -352., -161., -161.,\n",
       "        -156., -163., -123., -170., -195., -171., -161., -163., -162.,\n",
       "        -220., -154., -152., -167., -161., -186., -170.]),\n",
       " array([ -8.8,  -8.2,  -9.5,  -6.9,  -7.8,  -8.3,  -8.8, -17.3,  -5.7,\n",
       "         -9.5,  -9. ,  -8.3,  -8.3, -18.7,  -7.9,  -8.6,  -6.8, -18.7,\n",
       "         -6.6,  -9.5,  -9.3,  -9.7,  -5.9,  -9.4, -18.9,  -6.9,  -9. ,\n",
       "         -9.7,  -9.8,  -6.1,  -8.8,  -8.7,  -7.7,  -7.6,  -9.4,  -7.3,\n",
       "         -8.2,  -9.1,  -7.5,  -7.6,  -9.6,  -8.6,  -8.3]),\n",
       " array([42. , 40. , 45.8, 31.9, 36.9, 38.9, 41.4, 67.3, 20.1, 44.6, 41.9,\n",
       "        38. , 38.5, 67.9, 37.1, 42.6, 30. , 66.7, 29.8, 44.6, 44.2, 44.3,\n",
       "        27.5, 44.3, 65.8, 31.7, 43. , 46.7, 47.1, 24.2, 41.7, 40. , 35.6,\n",
       "        34.9, 47.3, 33.3, 37.5, 45.5, 34.2, 34.7, 48.6, 39.7, 38.7])]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fold_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82676f2-d998-4cb8-926d-d4e251398363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada8dc6-7578-406c-b663-38cae1a56bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a55c1197-266e-478b-b77a-6bb9be7caa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "678f9a58-7299-407d-9459-5e78f994b229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type1: CNN3\n",
      "model_type: Dense3\n",
      "layer_1: 128\n",
      "layer_2_2: 32\n",
      "layer_3_3: 16\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.45\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.46\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.47\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.48\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.49\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.50\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.51\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.52\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x154623ee1790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x154623ebd550>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tuner.load_model(trial=\"trail_0030\")\n",
    "# tuner.search_space_summary()\n",
    "# tuner.get_best_models()\n",
    "# tuner.get_best_hyperparameters()\n",
    "# tuner.results_summary()\n",
    "# trial=tuner.oracle.get_trial(\"0039\")\n",
    "# # tuner.get_best_models()[0].predict(pd.read_csv(f'{path}/Split_x_1_type_train_hp_fold_0_pipe_cond_No_scalling.csv'))\n",
    "# # tuner.oracle.get_trial(\"0039\").predict(pd.read_csv(f'{path}/Split_x_1_type_train_hp_fold_0_pipe_cond_No_scalling.csv'))\n",
    "# # tuner\n",
    "# print(trial.trial_id)\n",
    "# print(trial.best_step)\n",
    "# trial.display_hyperparameters()\n",
    "# print(trial.score)\n",
    "# trial.summary()\n",
    "# # print(trial.trial_id)\n",
    "# # trial.save('test.csv')\n",
    "# trial\n",
    "# current_model2=tuner.load_model(trial)\n",
    "# y_pred=current_model2.predict(pd.read_csv(f'{path}/Split_x_1_type_train_hp_fold_0_pipe_cond_No_scalling.csv'))\n",
    "# y_true=pd.read_csv(f'{path}/Split_y_1_type_train_hp_fold_0_pipe_cond_No_scalling.csv')\n",
    "# y_true\n",
    "# # y_pred\n",
    "\n",
    "\n",
    "trial=tuner.oracle.get_trial(\"0039\")\n",
    "trial.display_hyperparameters()\n",
    "\n",
    "X_train_cv=pd.read_csv(f'{path}/Split_x_1_type_val_hp_fold_0_pipe_cond_No_scalling.csv')\n",
    "train_resample=pd.read_csv(f'{path}/Split_y_1_type_val_hp_fold_0_pipe_cond_No_scalling.csv')\n",
    "\n",
    "model=tuner.load_model(trial)\n",
    "# model.expect_partial()\n",
    "# save_model_predictions_trial(current_model2,X_train_cv,X_train_true,'triela_0039',\"train\")\n",
    "\n",
    "y_pred_train_cv = model.predict(X_train_cv)\n",
    "y_pred_train_cv_enthalpy = y_pred_train_cv[0].squeeze()\n",
    "y_pred_train_cv_entropy = y_pred_train_cv[1].squeeze()\n",
    "y_pred_train_cv_free_energy = y_pred_train_cv[2].squeeze()\n",
    "y_pred_train_cv_Tm = y_pred_train_cv[3].squeeze()\n",
    "\n",
    "y_train_resample_output = pd.DataFrame()\n",
    "y_train_resample_output['DNA'] = train_resample.iloc[:, 0]\n",
    "# y_train_resample_output['Temp'] = train_resample['Temp']\n",
    "\n",
    "y_train_resample_output['y_dH'] = train_resample.iloc[:, 1]\n",
    "y_train_resample_output['y_dS'] = train_resample.iloc[:, 2]\n",
    "y_train_resample_output['y_dG'] = train_resample.iloc[:, 3]\n",
    "y_train_resample_output['y_Tm'] = train_resample.iloc[:, 4]\n",
    "\n",
    "y_train_resample_output[\"y_dH_pred\"]=y_pred_train_cv_enthalpy\n",
    "y_train_resample_output[\"y_dS_pred\"]=y_pred_train_cv_entropy\n",
    "y_train_resample_output[\"y_dG_pred\"]=y_pred_train_cv_free_energy\n",
    "y_train_resample_output[\"y_Tm_pred\"]=y_pred_train_cv_Tm\n",
    "y_train_resample_output\n",
    "plt.scatter(y_train_resample_output['y_dH'],y_train_resample_output['y_dH_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8cac0-bf9a-4630-b872-21b9c4fa0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(trial_number,set_type):\n",
    "    test=os.getcwd()+f'/trial_predictions/trial_predictions_{trial_number}_{set_type}plot.csv'\n",
    "    df=pd.read_csv(test)\n",
    "    df2=df\n",
    "    y_test = df2[f'y_{string}']\n",
    "    y_pred_test = df2[f'y_{string}_pred']\n",
    "    y_test_np = y_test.to_numpy()\n",
    "    y_pred_test_np = y_pred_test.to_numpy()\n",
    "    r2_test = r2_score(y_test_np, y_pred_test_np)\n",
    "    rmsd_test = (mean_squared_error(y_test_np, y_pred_test_np))**0.5\n",
    "    bias_test = np.mean(y_pred_test_np - y_test_np)\n",
    "    sdep_test = (np.mean((y_pred_test_np - y_test_np - bias_test)**2))**0.5\n",
    "    r2 = '{:.3f}'.format(r2_test)\n",
    "    rmsd = '{:.3f}'.format(rmsd_test)\n",
    "    bias = '{:.3f}'.format(bias_test)\n",
    "    sdep = '{:.3f}'.format(sdep_test)\n",
    "    \n",
    "    \n",
    "    mse=mean_squared_error(y_test_np, y_pred_test_np)\n",
    "    mse='{:.3f}'.format(mse)\n",
    "    mae=mean_absolute_error(y_test_np, y_pred_test_np)\n",
    "    mae='{:.3f}'.format(mae)\n",
    "    try:\n",
    "        a, b = np.polyfit(df2[f'y_{string}'], df2[f'y_{string}_pred'], 1)\n",
    "        plot_a = '{:.3f}'.format(a)\n",
    "        plot_b = '{:.3f}'.format(b)\n",
    "    except np.linalg.LinAlgError:\n",
    "        pass\n",
    "    \n",
    "    return r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b8a44d20-07d8-4eb4-9269-8f1643d687f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -60.0\n",
       "1      -56.3\n",
       "2      -56.8\n",
       "3      -53.6\n",
       "4      -60.4\n",
       "       ...  \n",
       "114    -40.4\n",
       "115    -71.3\n",
       "116   -125.6\n",
       "117    -49.2\n",
       "118    -56.8\n",
       "Name: y_true_0, Length: 119, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = dict(zip(keyList, [None]*len(keyList)))\n",
    "df=pd.DataFrame(n,index=['a'])\n",
    "string_options=['entropy']\n",
    "set_type='test'\n",
    "for string in string_options:\n",
    "    for dir_ in selection:\n",
    "        if \"trial_\" in dir_:\n",
    "            trial_number=str(dir_.replace(\"trial_\",\"\"))\n",
    "            trial=tunner.oracle.get_trial(trial_number)\n",
    "            # current_model2=tuner.load_model(trial)\n",
    "            \n",
    "            \n",
    "            # plot(trial_number,set_type)\n",
    "            r2, rmsd, bias, sdep, plot_a, plot_b, mse, mae = stats(trial_number,set_type)\n",
    "            n = dict(zip(keyList, [None]*len(keyList)))\n",
    "            n.update({\"trial\":trial_number,\"property\": string,'set_type': set_type,})\n",
    "            n.update({'r2':r2,'rmsd': rmsd, 'bias': bias, 'SDEP': sdep,'gradient': plot_a, 'b': plot_b, 'mse':mse, 'mae':mae,})\n",
    "            \n",
    "            original_stdout = sys.stdout \t\n",
    "\n",
    "            with open('temp.txt', 'w') as f:\n",
    "                sys.stdout = f\n",
    "                # tunner.results_summary(5)\n",
    "                trial.display_hyperparameters()\n",
    "                # Reset the standard output\n",
    "                sys.stdout = original_stdout \n",
    "            f.close\n",
    "            \n",
    "            # with open('temp_monitor.txt', 'a+') as f:\n",
    "            #     sys.stdout = f\n",
    "            #     # tunner.results_summary(5)\n",
    "            #     trial.display_hyperparameters()\n",
    "            #     # Reset the standard output\n",
    "            #     sys.stdout = original_stdout \n",
    "            # f.close\n",
    "\n",
    "            data = open('temp.txt', 'r').read()\n",
    "            data_hyp_param=data.replace(': ','\\n').split('\\n')\n",
    "            i_index=0\n",
    "            while i_index+1 < len(data_hyp_param):\n",
    "                # print(i_index,len(data_hyp_param))\n",
    "                n.update({data_hyp_param[i_index]:data_hyp_param[i_index+1]})\n",
    "                i_index+=2\n",
    "                \n",
    "            df.loc[f'{trial_number}']=n\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9839fb45-e7a6-4d7d-8763-b5c0e93e2388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>174.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>119.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>227.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>391.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>173.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>174.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows  250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6       7      8      9  \\\n",
       "0    173.0   83.0  205.0   72.0  149.0  426.0  129.0   533.0  378.0  216.0   \n",
       "1    173.0   52.0  207.0   87.0  135.0  414.0  136.0   528.0  378.0  216.0   \n",
       "2    173.0  131.0  150.0   42.0  236.0  413.0   40.0   544.0  387.0  221.0   \n",
       "3    174.0  101.0  170.0   62.0  200.0  401.0   58.0   538.0  382.0  218.0   \n",
       "4    174.0   90.0  194.0   73.0  163.0  402.0   85.0   534.0  380.0  218.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...     ...    ...    ...   \n",
       "114  119.0   84.0  116.0   36.0  144.0  282.0   40.0   392.0  278.0  161.0   \n",
       "115  227.0   42.0  226.0  102.0  237.0  560.0  185.0   708.0  505.0  291.0   \n",
       "116  391.0  237.0  372.0  134.0  463.0  911.0  126.0  1172.0  845.0  476.0   \n",
       "117  173.0   86.0  155.0   56.0  212.0  421.0   87.0   547.0  392.0  226.0   \n",
       "118  174.0   87.0  195.0   75.0  161.0  403.0   85.0   549.0  392.0  226.0   \n",
       "\n",
       "     ...  240  241  242  243  244  245  246  247  248  249  \n",
       "0    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "114  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "115  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "116  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "117  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[119 rows x 250 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f'{path}/Split_x_1_type_train_hp_fold_0_pipe_cond_No_scalling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0a4ab65-1cd2-4c85-99fb-998517eb38b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CNN_multitask_grid/CV/0/Granulated_multi_task_CNN/Granulated/dH_dS_dG_Tm/fold_0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path+'/Split_x_1_type_train_hp_fold_0_pipe_cond_No_scalling.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207193f7-edcb-4dec-b0c1-dbf2da0b4308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d47dfd-de87-419d-b08a-b62ac0ac7b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f495d1d-8c3e-41d8-84ed-6570e8fa449a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6444b705-83af-4283-a9bc-0751aaee676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_predictions_trial(model,X_train_cv,train_resample,trial,set_type):\n",
    "    \n",
    "    # Make predictions on train, val and test set using trained model:\n",
    "    y_pred_train_cv = model.predict(X_train_cv)\n",
    "    y_pred_train_cv_enthalpy = y_pred_train_cv[0].squeeze()\n",
    "    y_pred_train_cv_entropy = y_pred_train_cv[1].squeeze()\n",
    "    y_pred_train_cv_free_energy = y_pred_train_cv[2].squeeze()\n",
    "    y_pred_train_cv_Tm = y_pred_train_cv[3].squeeze()\n",
    "\n",
    "    y_train_resample_output = pd.DataFrame()\n",
    "    y_train_resample_output['DNA'] = train_resample.iloc[:, 0]\n",
    "    # y_train_resample_output['Temp'] = train_resample['Temp']\n",
    "    \n",
    "    y_train_resample_output['y_dH'] = train_resample[:, 1]\n",
    "    y_train_resample_output['y_dS'] = train_resample[:, 2]\n",
    "    y_train_resample_output['y_dG'] = train_resample[:, 3]\n",
    "    y_train_resample_output['y_Tm'] = train_resample[:, 4]\n",
    "    \n",
    "    y_train_resample_output[\"y_dH_pred\"]=y_pred_train_cv_enthalpy\n",
    "    y_train_resample_output[\"y_dS_pred\"]=y_pred_train_cv_entropy\n",
    "    y_train_resample_output[\"y_dG_pred\"]=y_pred_train_cv_free_energy\n",
    "    y_train_resample_output[\"y_Tm_pred\"]=y_pred_train_cv_Tm\n",
    "    \n",
    "    return y_train_resample_output\n",
    "    # y_train_resample_output.to_csv(f'trial_predictions/trial_predictions_{trial}_{set_type}plot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7853b789-b0f6-4049-be0c-25ef916ad342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir\n",
    "selection=os.listdir(\"/users/qdb16186/ml_evaluation/dir/grid6/\")\n",
    "\n",
    "for dir_ in selection:\n",
    "    if \"trial_\" in dir_:\n",
    "        trial=tunner.oracle.get_trial(str(dir_.replace(\"trial_\",\"\")))\n",
    "        try:\n",
    "            current_model2=tunner.load_model(trial)\n",
    "            save_model_predictions_trial(current_model2,X_train_cv,train,str(dir_.replace(\"trial_\",\"\")),\"train\")\n",
    "            save_model_predictions_trial(current_model2,X_val_cv,val,str(dir_.replace(\"trial_\",\"\")),\"val\")\n",
    "            save_model_predictions_trial(current_model2,X_test_cv,test,str(dir_.replace(\"trial_\",\"\")),\"test\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78a6f3-c825-4b02-8179-506a18ec05cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502386c-9610-4db7-8131-a337f6ca7a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0232b-3066-42cc-9aba-6617996bc916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b36b5-811e-4ddf-94b3-3ac0b7b4e2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
