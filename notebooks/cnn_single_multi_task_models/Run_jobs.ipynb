{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0c4191-29a5-489c-b5b5-04a4c362a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2890200c-61ee-4242-bb7d-425a020d6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir='CNN_single_task_grid'\n",
    "os.chdir('/users/qdb16186')\n",
    "path=os.getcwd()\n",
    "abs_dir=path+\"/\"+workdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4ca3a-4720-4cf6-9249-a2cfe078557f",
   "metadata": {},
   "source": [
    "# Bash_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf25f0f-7096-4366-89f2-f690d6d8c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_script_gen(abs_dir):\n",
    "    dir=abs_dir\n",
    "    cores=15\n",
    "    HH=48\n",
    "    job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/sbatch_.sh','w') as f:\n",
    "        f.write(f\"\"\"#!/bin/bash\n",
    "\n",
    "#======================================================\n",
    "#\n",
    "# Job script for running on a single node\n",
    "#\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# Propogate environment variables to the compute node\n",
    "#SBATCH --export=ALL\n",
    "#\n",
    "# Run in the standard partition (queue)\n",
    "#SBATCH --partition=standard\n",
    "#\n",
    "# Specify project account\n",
    "#SBATCH --account=palmer-addmd\n",
    "#\n",
    "# No. of tasks required\n",
    "#SBATCH --ntasks=1 --cpus-per-task={cores}\n",
    "#\n",
    "# Distribute processes in round-robin fashion for load balancing\n",
    "#SBATCH --distribution=cyclic\n",
    "#\n",
    "#\n",
    "# Specify (hard) runtime (HH:MM:SS)\n",
    "#SBATCH --time={HH}:00:00\n",
    "#\n",
    "# Job name\n",
    "#SBATCH --job-name={job_name}\n",
    "#\n",
    "# Output file\n",
    "#SBATCH --output={job_name}_slurm-%j.out\n",
    "#======================================================\n",
    "\n",
    "module purge\n",
    "module load anaconda/python-3.9.7/2021.11\n",
    "source activate tf\n",
    "module purge\n",
    "\n",
    "#======================================================\n",
    "# Prologue script to record job details\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_prologue.sh\n",
    "#------------------------------------------------------\n",
    "\n",
    "python {python_script}\n",
    "#mpirun -np $SLURM_NTASKS namd2 HEWL_002M_D2.inp > HEWL_002M_D2.out.$SLURM_JOB_ID \n",
    "\n",
    "#======================================================\n",
    "# Epilogue script to record job endtime and runtime\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_epilogue.sh\n",
    "#------------------------------------------------------\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d8718d-a054-410e-a4f3-b314cff72172",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script_gen(abs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d233b-bfea-4510-aff6-5135859f64d5",
   "metadata": {},
   "source": [
    "# Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e720b1-dc4b-49ae-94b5-d11b9b0440c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d14fe077-98fe-4f83-acab-8a956e9226f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_script(abs_dir):\n",
    "    dir=abs_dir\n",
    "    cores=15\n",
    "    HH=48\n",
    "    job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/python_script.py','w') as f:\n",
    "        f.write(\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# sklearn inports\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "# import pickle as pk\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, X = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep\n",
    "\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    y_outputs['y_true'] = true\n",
    "    y_outputs['y_pred'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(dense1,dense2,dense3,convn,**kwargs):\n",
    "    \n",
    "    print(\"dense1: \", dense1)\n",
    "    \n",
    "    print(\"dense2: \", dense2)\n",
    "    \n",
    "    print(\"dense3: \", dense3)\n",
    "\n",
    "    print(\"convn: \", convn)\n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    if convn > 0:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            input_shape=(250,1),\n",
    "                            name = 'conv1d_1'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x)\n",
    "        x = keras.layers.BatchNormalization(name = 'batchnorm_1')(x)\n",
    "    \n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    if convn > 1:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_2'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_2')(x)   \n",
    "    if convn > 2:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_3'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_3')(x)            \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    if convn > -1:\n",
    "        x = keras.layers.Flatten(name = 'flatten')(x)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    if dense1 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                    dense1,\n",
    "                    activation='relu',\n",
    "                    use_bias=True,\n",
    "                    # name='layer_1',\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=None,\n",
    "                    bias_regularizer=None,\n",
    "                    activity_regularizer=None,\n",
    "                    kernel_constraint=None,\n",
    "                    bias_constraint=None\n",
    "                )(x)\n",
    "    if dense2 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "    if dense3 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "#     OUTPUT LAYERS\n",
    "    output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x)\n",
    "\n",
    "    # model = Model(inputs=inputs, outputs=[output_1, output_2, output_3])\n",
    "    model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS ADAPTIVE LEARNING RATE   \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "### MODEL END Start Workflow\n",
    "\n",
    "### Keras Model REgressor Ready for Grid Search\n",
    "def model_for_grid(resultdir,resample,epochs,**kwargs):\n",
    "#Needs variable epochs \n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min', \n",
    "                       verbose=1, \n",
    "                       patience=2000, \n",
    "                       restore_best_weights=True)\n",
    "    # CSV Logger\n",
    "    csv_logger = CSVLogger(f\"{resultdir}/csv_logger/model_history_log_resample_{resample}.csv\", \n",
    "                           append=True)\n",
    "    # # CP_callbacks\n",
    "    # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "    #                                                  save_weights_only=True,\n",
    "    #                                                  verbose=1)\n",
    "    \n",
    "    # TensorBoard\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{resultdir}/tensorboard_logs/{resample}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                          update_freq = 1,\n",
    "                                                          # histogram_freq=1, \n",
    "                                                          write_graph=False, \n",
    "                                                          write_images=False)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    \n",
    "    # Covert to list and provide to Keras Regressor\n",
    "    keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "        \n",
    "    ###initialise model\n",
    "    model = KerasRegressor(model=build_model, \n",
    "                           batch=32,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_split=0.2,\n",
    "                           callbacks=keras_callbacks,\n",
    "                           dense1=1,\n",
    "                            dense2=1,\n",
    "                            dense3=1,\n",
    "                           convn=1\n",
    "                          )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "### Input Parameters and Workflow\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\n",
    "prop='Granulated'\n",
    "# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=50\n",
    "n_folds=5\n",
    "n_jobs=1\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(n_folds, test_size=0.3, random_state=1)\n",
    "\n",
    "###define scoring dict for cv\n",
    "scorers = {\n",
    "    'r2':make_scorer(r2_func), \n",
    "    'rmse':make_scorer(rmse_func, greater_is_better=False), \n",
    "    'bias':make_scorer(bias_func, greater_is_better=False), \n",
    "    'sdep':make_scorer(sdep_func, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1   \n",
    "\n",
    "    \n",
    "    for prop in desc_type:\n",
    "    #     adjust y[2] to * Temperature /1000 dS*T kcal/mol\n",
    "        #for CNN with padding\n",
    "        x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "\n",
    "\n",
    "        i=-1\n",
    "        for GSHT in GSHT_list:\n",
    "            i+=1\n",
    "\n",
    "            ### DEFINE MODEL\n",
    "            model_name = f'CNN_single_task_grid_{grid_number}'\n",
    "            \n",
    "            path=\"{}/CV/{}/{}/{}/{}\".format(os.getcwd(),resample,model_name,prop,GSHT)\n",
    "            \n",
    "            model = model_for_grid(path,resample,epochs) #resultdir\n",
    "            \n",
    "            create_dir(home,resample,model_name,prop,GSHT)\n",
    "            \n",
    "            ### Grid            \n",
    "            pipe_cond='No_scalling'\n",
    "            for pipe_cond in ['No_scalling']:#,'Scalling']: \n",
    "                # Until we find a way to normalise the data in this pipeline. \n",
    "                # We can do it with hyper_tunner \n",
    "                # NB! Do not forget to adjust the keras callbacks becasue they will overwrite No_scalling outputs\n",
    "                if pipe_cond==\"Scalling\":        \n",
    "                    ### PIPE\n",
    "                    # Define inputs for pipe\n",
    "                    scaler = StandardScaler()\n",
    "                    pipe = Pipeline(steps=[(\"scaler\", scaler), (f\"{model_name}\",model)])\n",
    "            \n",
    "                     ###parameter grid\n",
    "                    param_grid_model = {\n",
    "                                     f\"{model_name}__dense1\":[8,16,32],\n",
    "                                     f\"{model_name}__dense2\":[8,16,32],\n",
    "                                     f\"{model_name}_dense3\":[8,16,32],\n",
    "                                        f\"{model_name}__convn\":[0]}\n",
    "                    \n",
    "                    ###create CV using sklearn.GridSearchCV\n",
    "                    grid = GridSearchCV(\n",
    "                        estimator=pipe, \n",
    "                        param_grid=param_grid_model,\n",
    "                        n_jobs=n_jobs, \n",
    "                        cv=n_folds, \n",
    "                        refit='rmse', \n",
    "                        scoring=scorers, \n",
    "                        return_train_score=True,\n",
    "                        )\n",
    "        \n",
    "                else:\n",
    "                    ###parameter grid\n",
    "                    param_grid_model = {                     \n",
    "                                 \"dense1\":[32],\n",
    "                                 \"dense2\":[32],\n",
    "                                 \"dense3\":[8,32],\n",
    "                                 'convn':[3]}\n",
    "                    \n",
    "                    ###create CV using sklearn.GridSearchCV\n",
    "                    grid = GridSearchCV(\n",
    "                        estimator=model,\n",
    "                        param_grid=param_grid_model,\n",
    "                        n_jobs=n_jobs, \n",
    "                        cv=n_folds, \n",
    "                        refit='rmse', \n",
    "                        scoring=scorers, \n",
    "                        return_train_score=True,\n",
    "                        )\n",
    "    \n",
    "        ########### Fid Model with Scikeras and scikit learn.\n",
    "                history = grid.fit(x_train, y_train[i])\n",
    "       ############ Store results\n",
    "    \n",
    "                results=pd.DataFrame(history.cv_results_)\n",
    "                results.to_csv(path+f\"/gridsearch_resample_{resample}_pipe_cond_{pipe_cond}.csv\")\n",
    "    \n",
    "                y_pred_test=history.predict(x_test)\n",
    "                y_pred_train=history.predict(x_train)\n",
    "    \n",
    "                save_splits(train_idx,y_train[i],y_pred_train,resample,path,f\"train_pipe_cond_{pipe_cond}\")\n",
    "                save_splits(test_idx,y_test[i],y_pred_test,resample,path,f\"test_pipe_cond_{pipe_cond}\")\n",
    "        \n",
    "# total_time=time.time()-time_start\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17512409-6f4d-4dca-a6d8-e37019bee5f7",
   "metadata": {},
   "source": [
    "# Bash Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9750898-9227-45f7-bc02-102ec9b3d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84dc5f8b-e587-4eb8-b8b2-b19c20aedc78",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569bc58-5922-4b7b-b7e8-1fbdf7a7a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "463da9c1-6b02-4a1b-b731-b7f21591a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash_script_gen(abs_dir)\n",
    "python_script(abs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "344b63e0-d808-441f-875e-3a614f2865f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3362522159.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    os.(f'./ {abs_dir}/sbatch_.sh')\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "os.(f'./ {abs_dir}/sbatch_.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cbf1ef0-bd0f-468e-b06e-ebbc24b1c122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CNN_single_task'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fa28224-816c-4f03-989a-5ea410b0e4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/users/qdb16186/Input')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29304407-9f2e-47e6-88ed-6e51a314cbfb",
   "metadata": {},
   "source": [
    "# Python Single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a9fa8-de6a-451e-ba69-70a182ae7d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e00962dc-482c-4c25-bf88-fd64e419daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_script(abs_dir,prop,GSHT,cores):\n",
    "    dir=abs_dir\n",
    "    job_name=f\"CNN_single_task_{prop}_{GSHT}_{cores}\"\n",
    "    python_script=f\"CNN_single_task_{prop}_{GSHT}_{cores}.py\"\n",
    "    with open(f'{dir}/{python_script}','w') as f:\n",
    "        f.write(\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# sklearn inports\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "# import pickle as pk\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, X = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep\n",
    "\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    y_outputs['y_true'] = true\n",
    "    y_outputs['y_pred'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(dense1,dense2,dense3,convn,**kwargs):\n",
    "    \n",
    "    print(\"dense1: \", dense1)\n",
    "    \n",
    "    print(\"dense2: \", dense2)\n",
    "    \n",
    "    print(\"dense3: \", dense3)\n",
    "\n",
    "    print(\"convn: \", convn)\n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    if convn > 0:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            input_shape=(250,1),\n",
    "                            name = 'conv1d_1'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x)\n",
    "        x = keras.layers.BatchNormalization(name = 'batchnorm_1')(x)\n",
    "    \n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    if convn > 1:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_2'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_2')(x)   \n",
    "    if convn > 2:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_3'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_3')(x)            \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    if convn > -1:\n",
    "        x = keras.layers.Flatten(name = 'flatten')(x)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    if dense1 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                    dense1,\n",
    "                    activation='relu',\n",
    "                    use_bias=True,\n",
    "                    # name='layer_1',\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=None,\n",
    "                    bias_regularizer=None,\n",
    "                    activity_regularizer=None,\n",
    "                    kernel_constraint=None,\n",
    "                    bias_constraint=None\n",
    "                )(x)\n",
    "    if dense2 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "    if dense3 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "\n",
    "    # if dropout > 0:\n",
    "    #     x = keras.layers.Dropout(dropout)(x)\n",
    "        \n",
    "#     OUTPUT LAYERS\n",
    "    output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x)\n",
    "\n",
    "    # model = Model(inputs=inputs, outputs=[output_1, output_2, output_3])\n",
    "    model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS ADAPTIVE LEARNING RATE   \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "### MODEL END Start Workflow\n",
    "\n",
    "### Keras Model REgressor Ready for Grid Search\n",
    "def model_for_grid(resultdir,resample,epochs,**kwargs):\n",
    "#Needs variable epochs \n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min', \n",
    "                       verbose=1, \n",
    "                       patience=2000, \n",
    "                       restore_best_weights=True)\n",
    "    # CSV Logger\n",
    "    csv_logger = CSVLogger(f\"{resultdir}/csv_logger/model_history_log_resample_{resample}.csv\", \n",
    "                           append=True)\n",
    "    # # CP_callbacks\n",
    "    # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "    #                                                  save_weights_only=True,\n",
    "    #                                                  verbose=1)\n",
    "    \n",
    "    # TensorBoard\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{resultdir}/tensorboard_logs/{resample}', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                          update_freq = 1,\n",
    "                                                          # histogram_freq=1, \n",
    "                                                          write_graph=False, \n",
    "                                                          write_images=False)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    \n",
    "    # Covert to list and provide to Keras Regressor\n",
    "    keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "        \n",
    "    ###initialise model\n",
    "    model = KerasRegressor(model=build_model, \n",
    "                           batch=32,\n",
    "                            epochs=epochs,\n",
    "                            verbose=3,\n",
    "                            validation_split=0.2,\n",
    "                           callbacks=keras_callbacks,\n",
    "                           dense1=1,\n",
    "                            dense2=1,\n",
    "                            dense3=1,\n",
    "                           convn=1\n",
    "                          )\n",
    "\n",
    "    return model\"\"\")\n",
    "\n",
    "        f.write(f\"\"\"\n",
    "### Input Parameters and Workflow\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\n",
    "# prop='Granulated'\n",
    "prop='{prop}'\n",
    "GSHT='{GSHT}'\n",
    "n_jobs={cores}\n",
    "\"\"\")\n",
    "\n",
    "        f.write(\"\"\"# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=50\n",
    "n_folds=5\n",
    "\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(n_folds, test_size=0.3, random_state=1)\n",
    "\n",
    "###define scoring dict for cv\n",
    "scorers = {\n",
    "    'r2':make_scorer(r2_func), \n",
    "    'rmse':make_scorer(rmse_func, greater_is_better=False), \n",
    "    'bias':make_scorer(bias_func, greater_is_better=False), \n",
    "    'sdep':make_scorer(sdep_func, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1   \n",
    "    print(f'resample: {resample}')\n",
    "    \n",
    "    \n",
    "    #     adjust y[2] to * Temperature /1000 dS*T kcal/mol\n",
    "        #for CNN with padding\n",
    "    x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### DEFINE MODEL \"\"\")\n",
    "\n",
    "        f.write(f\"\"\"\n",
    "    model_name = \"CNN_single_task_{prop}_{GSHT}_{cores}\"\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "        f.write(\"\"\"\n",
    "    \n",
    "    path=\"{}/CV/{}/{}/{}/{}\".format(os.getcwd(),resample,model_name,prop,GSHT)\n",
    "    \n",
    "    model = model_for_grid(path,resample,epochs) #resultdir\n",
    "    \n",
    "    create_dir(home,resample,model_name,prop,GSHT)\n",
    "    \n",
    "    ### Grid            \n",
    "    pipe_cond='No_scalling'\n",
    "    for pipe_cond in ['No_scalling']:#,'Scalling']: \n",
    "        # Until we find a way to normalise the data in this pipeline. \n",
    "        # We can do it with hyper_tunner \n",
    "        # NB! Do not forget to adjust the keras callbacks becasue they will overwrite No_scalling outputs\n",
    "        if pipe_cond==\"Scalling\":        \n",
    "            ### PIPE\n",
    "            # Define inputs for pipe\n",
    "            scaler = StandardScaler()\n",
    "            pipe = Pipeline(steps=[(\"scaler\", scaler), (f\"{model_name}\",model)])\n",
    "    \n",
    "             ###parameter grid\n",
    "            param_grid_model = {\n",
    "                             f\"{model_name}__dense1\":[8,16,32],\n",
    "                             f\"{model_name}__dense2\":[8,16,32],\n",
    "                             f\"{model_name}_dense3\":[8,16,32],\n",
    "                                f\"{model_name}__convn\":[0]}\n",
    "            \n",
    "            ###create CV using sklearn.GridSearchCV\n",
    "            grid = GridSearchCV(\n",
    "                estimator=pipe, \n",
    "                param_grid=param_grid_model,\n",
    "                n_jobs=n_jobs, \n",
    "                cv=n_folds, \n",
    "                refit='rmse', \n",
    "                scoring=scorers, \n",
    "                return_train_score=True,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            ###parameter grid\n",
    "            param_grid_model = {                     \n",
    "                         \"dense1\":[16,32,64],\n",
    "                         \"dense2\":[16,32,64],\n",
    "                         \"dense3\":[8,16,32,64],\n",
    "                         'convn':[3,2,1]}\n",
    "            \n",
    "            ###create CV using sklearn.GridSearchCV\n",
    "            grid = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grid_model,\n",
    "                n_jobs=n_jobs, \n",
    "                cv=n_folds, \n",
    "                refit='rmse', \n",
    "                scoring=scorers, \n",
    "                return_train_score=True,\n",
    "                )\n",
    "\n",
    "        i = GSHT_list.index(GSHT)\n",
    "        \n",
    "########### Fid Model with Scikeras and scikit learn.\n",
    "        history = grid.fit(x_train, y_train[i])\n",
    "############ Store results\n",
    "\n",
    "        results=pd.DataFrame(history.cv_results_)\n",
    "        results.to_csv(path+f\"/gridsearch_resample_{resample}_pipe_cond_{pipe_cond}.csv\")\n",
    "\n",
    "        y_pred_test=history.predict(x_test)\n",
    "        y_pred_train=history.predict(x_train)\n",
    "\n",
    "        save_splits(train_idx,y_train[i],y_pred_train,resample,path,f\"train_pipe_cond_{pipe_cond}\")\n",
    "        save_splits(test_idx,y_test[i],y_pred_test,resample,path,f\"test_pipe_cond_{pipe_cond}\")\n",
    "        \n",
    "# total_time=time.time()-time_start\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6c3ee-6123-44b5-9ae3-b85a047c0efb",
   "metadata": {},
   "source": [
    "# Bash Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86de57bb-33c7-419c-8466-d5bd295dee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_script_gen(abs_dir,prop,GSHT,cores):\n",
    "    dir=abs_dir\n",
    "    # cores=15\n",
    "    HH=48\n",
    "    job_name=f\"{prop}_{GSHT}_{cores}_CNN_single_task\"\n",
    "    python_script=f\"CNN_single_task_{prop}_{GSHT}_{cores}.py\"\n",
    "    \n",
    "    # job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    # python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/sbatch_{job_name}.sh','w') as f:\n",
    "        f.write(f\"\"\"#!/bin/bash\n",
    "\n",
    "#======================================================\n",
    "#\n",
    "# Job script for running on a single node\n",
    "#\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# Propogate environment variables to the compute node\n",
    "#SBATCH --export=ALL\n",
    "#\n",
    "# Run in the standard partition (queue)\n",
    "#SBATCH --partition=standard\n",
    "#\n",
    "# Specify project account\n",
    "#SBATCH --account=palmer-addmd\n",
    "#\n",
    "# No. of tasks required\n",
    "#SBATCH --ntasks=1 --cpus-per-task={cores}\n",
    "#\n",
    "# Distribute processes in round-robin fashion for load balancing\n",
    "#SBATCH --distribution=cyclic\n",
    "#\n",
    "#\n",
    "# Specify (hard) runtime (HH:MM:SS)\n",
    "#SBATCH --time={HH}:00:00\n",
    "#\n",
    "# Job name\n",
    "#SBATCH --job-name={job_name}\n",
    "#\n",
    "# Output file\n",
    "#SBATCH --output={job_name}_slurm-%j.out\n",
    "#======================================================\n",
    "\n",
    "module purge\n",
    "module load anaconda/python-3.9.7/2021.11\n",
    "source activate tf\n",
    "module purge\n",
    "\n",
    "#======================================================\n",
    "# Prologue script to record job details\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_prologue.sh\n",
    "#------------------------------------------------------\n",
    "\n",
    "python {python_script}\n",
    "#mpirun -np $SLURM_NTASKS namd2 HEWL_002M_D2.inp > HEWL_002M_D2.out.$SLURM_JOB_ID \n",
    "\n",
    "#======================================================\n",
    "# Epilogue script to record job endtime and runtime\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_epilogue.sh\n",
    "#------------------------------------------------------\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e227de7-0aa6-4e20-970d-83f5852153a0",
   "metadata": {},
   "source": [
    "# Run Jupyter book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "35b2cc7d-298b-45d9-bf0f-fed2c745802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_type = ['DNA-Groups','Granulated','OHEP','LP_dec2']\n",
    "GSHT_list=['dH','dS','dG','Tm']\n",
    "cores=10\n",
    "\n",
    "os.chdir('/users/qdb16186')\n",
    "os.chdir(abs_dir)\n",
    "for prop in desc_type:\n",
    "    for GSHT in GSHT_list:\n",
    "        python_script(abs_dir,prop,GSHT,cores)\n",
    "        bash_script_gen(abs_dir,prop,GSHT,cores)\n",
    "\n",
    "\n",
    "# def python_script(abs_dir,prop,GSHT,cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5fe6bee-e536-46e6-b550-602de3434db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CNN_single_task'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa681b67-ea19-46ee-b849-c199d457db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628061\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628062\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628063\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_DNA-Groups_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628064\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628065\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628066\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628067\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_Granulated_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628068\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628069\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628070\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628071\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_OHEP_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628072\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_dH_10_CNN_single_task.sh\n",
      "Submitted batch job 10628073\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_dS_10_CNN_single_task.sh\n",
      "Submitted batch job 10628074\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_dG_10_CNN_single_task.sh\n",
      "Submitted batch job 10628075\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_LP_dec2_Tm_10_CNN_single_task.sh\n",
      "Submitted batch job 10628076\n"
     ]
    }
   ],
   "source": [
    "desc_type = ['DNA-Groups','Granulated','OHEP','LP_dec2']\n",
    "GSHT_list=['dH','dS','dG','Tm']\n",
    "cores=10\n",
    "os.chdir('/users/qdb16186')\n",
    "os.chdir(abs_dir)\n",
    "for prop in desc_type:\n",
    "    for GSHT in GSHT_list:\n",
    "        job_name=f\"{prop}_{GSHT}_{cores}_CNN_single_task\"\n",
    "        print(f'{abs_dir}/sbatch_{job_name}.sh')\n",
    "        bashCommand = f\"sbatch {abs_dir}/sbatch_{job_name}.sh\"\n",
    "        os.system(bashCommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1776cd4-8c0f-4ea6-825a-4e2346435391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSHT = 'dH'\n",
    "# GSHT in \n",
    "GSHT_list=['dH','dS','dG','Tm']\n",
    "GSHT_list.index(GSHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adf9a010-a6ad-46e0-8f38-c15a91c62281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/qdb16186/CNN_single_task'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a5a5b51-b3e1-48ae-909b-c84e17cf6bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    if i <3:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bc60c-7e46-41b5-adfb-a2931451a75e",
   "metadata": {},
   "source": [
    "# Access Grid Search rerun with best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f4bea-803b-46b7-919c-4bcfd7d389e6",
   "metadata": {},
   "source": [
    "## Single_task_single_core_bash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "222f8580-7a2b-4365-a71e-14c2cc1b19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_script_gen_single(abs_dir,prop,GSHT,cores,resample):\n",
    "    dir=abs_dir\n",
    "    cores=1\n",
    "    HH='01'\n",
    "    job_name=f\"S_{resample}_{GSHT}_{prop}_{cores}_CNN_single_task\"\n",
    "    python_script=f\"CNN_single_task_{prop}_{GSHT}_S_{resample}.py\"\n",
    "    \n",
    "    # job_name=\"CNN_single_task_Granulated_15_cores\"\n",
    "    # python_script=\"CNN_single_task_Granulated.py\"\n",
    "    with open(f'{dir}/sbatch_S_{resample}_{prop}_{GSHT}_{cores}_CNN_single_task.sh','w') as f:\n",
    "        f.write(f\"\"\"#!/bin/bash\n",
    "\n",
    "#======================================================\n",
    "#\n",
    "# Job script for running on a single node\n",
    "#\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# Propogate environment variables to the compute node\n",
    "#SBATCH --export=ALL\n",
    "#\n",
    "# Run in the standard partition (queue)\n",
    "#SBATCH --partition=standard\n",
    "#\n",
    "# Specify project account\n",
    "#SBATCH --account=palmer-addmd\n",
    "#\n",
    "# No. of tasks required\n",
    "#SBATCH --ntasks=1 --cpus-per-task={cores}\n",
    "#\n",
    "# Distribute processes in round-robin fashion for load balancing\n",
    "#SBATCH --distribution=cyclic\n",
    "#\n",
    "#\n",
    "# Specify (hard) runtime (HH:MM:SS)\n",
    "#SBATCH --time={HH}:20:00\n",
    "#\n",
    "# Job name\n",
    "#SBATCH --job-name={job_name}\n",
    "#\n",
    "# Output file\n",
    "#SBATCH --output={job_name}_slurm-%j.out\n",
    "#======================================================\n",
    "\n",
    "module purge\n",
    "module load anaconda/python-3.9.7/2021.11\n",
    "source activate tf\n",
    "module purge\n",
    "\n",
    "#======================================================\n",
    "# Prologue script to record job details\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_prologue.sh\n",
    "#------------------------------------------------------\n",
    "\n",
    "python {python_script}\n",
    "#mpirun -np $SLURM_NTASKS namd2 HEWL_002M_D2.inp > HEWL_002M_D2.out.$SLURM_JOB_ID \n",
    "\n",
    "#======================================================\n",
    "# Epilogue script to record job endtime and runtime\n",
    "# Do not change the line below\n",
    "#======================================================\n",
    "/opt/software/scripts/job_epilogue.sh\n",
    "#------------------------------------------------------\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bd406-e6fd-44cd-8fc9-0e95e6f7fa95",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d3697527-3a79-4000-9612-38e817a1ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_script_best_grid(abs_dir,prop,GSHT,cores,my_dict,filter_count):\n",
    "    dir=abs_dir\n",
    "    resample=filter_count\n",
    "    job_name=f\"S_{filter_count}_{prop}_{GSHT}_{cores}_CNN_single_task\"\n",
    "    python_script=f\"CNN_single_task_{prop}_{GSHT}_S_{filter_count}.py\"\n",
    "    with open(f'{dir}/{python_script}','w') as f:\n",
    "        f.write(\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# sklearn inports\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "# import pickle as pk\n",
    "\n",
    "### Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, models, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Check if these are in model\n",
    "# from tensorflow.keras.losses import Reduction \n",
    "# from tensorflow.keras.losses import MeanAbsoluteError\n",
    "# Finish check\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "#Functions\n",
    "## Loading Data from CSV file\n",
    "def padding(X_descr_train_scaled):\n",
    "#     Padding function so X data is always 250 dimensions\n",
    "# Must be coupled with load_data. NB! double check if the scalling is not affected\n",
    "# https://www.geeksforgeeks.org/python-call-function-from-another-function/\n",
    "    a=X_descr_train_scaled.to_numpy()\n",
    "    b=np.zeros((len(X_descr_train_scaled), \n",
    "                (250-int(X_descr_train_scaled.to_numpy().shape[1]))\n",
    "               )\n",
    "              )\n",
    "    padded=np.concatenate((a,b),\n",
    "                           axis=1, \n",
    "                          out=None, \n",
    "                          dtype=None\n",
    "                         )\n",
    "    return padded\n",
    "\n",
    "def df_np(y):\n",
    "#     y is a list\n",
    "    y_out=[]\n",
    "    for y_i in y:\n",
    "        y_ic=y_i.to_numpy()\n",
    "        y_ic=y_ic.reshape(y_ic.shape[0])\n",
    "        y_out.append(y_ic)\n",
    "    return y_out\n",
    "\n",
    "def load_data(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Convert y data into required input shape\n",
    "    y_1 = y_1.to_numpy()\n",
    "    y_1 = y_1.reshape(y_1.shape[0])\n",
    "    y_2 = y_2.to_numpy()\n",
    "    y_2 = y_2.reshape(y_2.shape[0])\n",
    "    y_3 = y_3.to_numpy()\n",
    "    y_3 = y_3.reshape(y_3.shape[0])\n",
    "    y_4 = y_4.to_numpy()\n",
    "    y_4 = y_4.reshape(y_4.shape[0])\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, padding(X), X\n",
    "\n",
    "\n",
    "def load_data_df(file,prop):\n",
    "# Universal funciton for loading\n",
    "# y_1, y_2, y_3, y_4 and x data from input csv (All, Train, Val or Train)\n",
    "    y_1 = file[['dH']].copy()\n",
    "    y_2 = file[['dS']].copy()\n",
    "    y_3 = file[['dG']].copy()\n",
    "    y_4 = file[['Tm']].copy()\n",
    "    \n",
    "    # Load features based on prop\n",
    "    X = file[[col for col in file.columns if f'{prop}_'in col]]\n",
    "    \n",
    "    return y_1, y_2, y_3, y_4, X\n",
    "\n",
    "def wrapped_train_test_split(train_idx,test_idx,file,prop):\n",
    "#     capittal Y and X stand for pandas dataframe like file\n",
    "    Y_1, Y_2, Y_3, Y_4, X = load_data_df(file,prop)\n",
    "\n",
    "    # Separate data into training and test sets:\n",
    "    x_train = X.iloc[train_idx]\n",
    "    x_test = X.iloc[test_idx]\n",
    "#     The next two lines (y) will vary depending on the CNN output\n",
    "    y_train = [Y_1.iloc[train_idx],Y_2.iloc[train_idx],Y_3.iloc[train_idx],Y_4.iloc[train_idx]]\n",
    "    y_test  = [Y_1.iloc[test_idx] ,Y_2.iloc[test_idx] ,Y_3.iloc[test_idx] ,Y_4.iloc[test_idx]]\n",
    "    \n",
    "    return padding(x_train), padding(x_test), df_np(y_train), df_np(y_test)\n",
    "\n",
    "\n",
    "def create_dir(home,resample,model_name,prop,GSHT):\n",
    "    if home==None:\n",
    "        home=os.getcwd()\n",
    "    try:\n",
    "            os.mkdir(\"{}/CV/\".format(home))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}/CV/\".format(home))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(resample))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(resample))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(model_name))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(model_name))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(prop))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(prop))\n",
    "    try:\n",
    "        os.mkdir(\"{}\".format(GSHT))\n",
    "    except:\n",
    "            pass\n",
    "    os.chdir(\"{}\".format(GSHT))\n",
    "    try:\n",
    "        os.mkdir('{}'.format('csv_logger'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('model_checkpoint'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir('{}'.format('tensorboard_logs'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format('model_checkpoint'))\n",
    "    try:\n",
    "        os.mkdir('training_{}'.format(resample))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"{}\".format(home))\n",
    "    return\n",
    "\n",
    "def r2_func(y_true, y_pred, **kwargs):\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "def rmse_func(y_true, y_pred, **kwargs):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))  \n",
    "def bias_func(y_true, y_pred, **kwargs):\n",
    "    return np.mean(y_true-y_pred)\n",
    "def sdep_func(y_true, y_pred, **kwargs):\n",
    "    return (np.mean((y_true-y_pred-(np.mean(y_true-y_pred)))**2))**0.5\n",
    "#these 4 are for tensorflow formats\n",
    "def r2_func_tf(y_true, y_pred, **kwargs):\n",
    "    numerator = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    denominator = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - numerator / denominator\n",
    "    return r2\n",
    "def rmse_func_tf(y_true, y_pred, **kwargs):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "def bias_func_tf(y_true, y_pred, **kwargs):\n",
    "    bias = tf.reduce_mean(y_true - y_pred)\n",
    "    return bias\n",
    "def sdep_func_tf(y_true, y_pred, **kwargs):\n",
    "    diff = y_true - y_pred\n",
    "    mean_diff = tf.reduce_mean(diff)\n",
    "    sdep = tf.sqrt(tf.reduce_mean(tf.square(diff - mean_diff)))\n",
    "    return sdep\n",
    "\n",
    "def save_splits(idx,true,pred,resample,path,split_type):\n",
    "    y_outputs = pd.DataFrame()\n",
    "    y_outputs['ID'] = idx\n",
    "    y_outputs['y_true'] = true\n",
    "    y_outputs['y_pred'] = pred\n",
    "    y_outputs.to_csv(f'{path}/Split_{resample}_type_{split_type}.csv', index=False)\n",
    "    return\n",
    "\n",
    "def build_model(dense1,dense2,dense3,convn,**kwargs):\n",
    "    \n",
    "    print(\"dense1: \", dense1)\n",
    "    \n",
    "    print(\"dense2: \", dense2)\n",
    "    \n",
    "    print(\"dense3: \", dense3)\n",
    "\n",
    "    print(\"convn: \", convn)\n",
    "    \n",
    "#     INPUT for NN\n",
    "    \n",
    "    inputs = keras.Input(shape=(250,1))\n",
    "    x=inputs\n",
    "    \n",
    "#     MANDATORY CNN (optional to move into first condition hp.cond_scope\n",
    "    if convn > 0:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            input_shape=(250,1),\n",
    "                            name = 'conv1d_1'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = 'maxpooling_1')(x)\n",
    "        x = keras.layers.BatchNormalization(name = 'batchnorm_1')(x)\n",
    "    \n",
    "#     CONDITIONAL CONVOLUTION LAYERS (Consider moving the above into CNN1) test 0-3 CNN and 0-3 Dense\n",
    "    if convn > 1:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_2'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_2')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_2')(x)   \n",
    "    if convn > 2:\n",
    "        x = keras.layers.Conv1D(32, \n",
    "                            kernel_size=(3), \n",
    "                            strides=(2), \n",
    "                            padding='valid', \n",
    "                            activation='relu', \n",
    "                            name = f'conv1d_3'\n",
    "                            )(x)\n",
    "        x = keras.layers.MaxPooling1D((2), name = f'maxpooling_3')(x)\n",
    "        x = keras.layers.BatchNormalization(name = f'batchnorm_3')(x)            \n",
    "#     FLATTEN AFTER CONVOLUTIONS\n",
    "    if convn > -1:\n",
    "        x = keras.layers.Flatten(name = 'flatten')(x)\n",
    "    \n",
    "#     CONDITIONAL DENSE LAYERS\n",
    "    if dense1 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                    dense1,\n",
    "                    activation='relu',\n",
    "                    use_bias=True,\n",
    "                    # name='layer_1',\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=None,\n",
    "                    bias_regularizer=None,\n",
    "                    activity_regularizer=None,\n",
    "                    kernel_constraint=None,\n",
    "                    bias_constraint=None\n",
    "                )(x)\n",
    "    if dense2 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense2,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "    if dense3 >0:\n",
    "        x = keras.layers.Dense(\n",
    "                        dense3,\n",
    "                        activation='relu',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='zeros',\n",
    "                        kernel_regularizer=None,\n",
    "                        bias_regularizer=None,\n",
    "                        activity_regularizer=None,\n",
    "                        kernel_constraint=None,\n",
    "                        bias_constraint=None\n",
    "                    )(x)\n",
    "\n",
    "    # if dropout > 0:\n",
    "    #     x = keras.layers.Dropout(dropout)(x)\n",
    "        \n",
    "#     OUTPUT LAYERS\n",
    "    output_1 = keras.layers.Dense(1, name='enthalpy_pred')(x)\n",
    "    # output_2 = keras.layers.Dense(1, name='entropy_pred')(x)\n",
    "    # output_3 = keras.layers.Dense(1, name='free_energy_pred')(x)\n",
    "\n",
    "    # model = Model(inputs=inputs, outputs=[output_1, output_2, output_3])\n",
    "    model = Model(inputs=inputs, outputs=output_1)\n",
    "    \n",
    "#     SETTINGS ADAPTIVE LEARNING RATE   \n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 10.0\n",
    "    decay_rate = 0.5\n",
    "    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate, decay_steps, decay_rate)\n",
    "    \n",
    "#     SETTING ADAM OPTIMISER\n",
    "    optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "    \n",
    "#     COMPILE MODEl\n",
    "    model.compile(loss = \"mse\" , \n",
    "                  optimizer = optimiser, \n",
    "                  metrics = [\"mse\",'mean_absolute_error',r2_func_tf, rmse_func_tf, bias_func_tf, sdep_func_tf])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "### MODEL END Start Workflow\n",
    "\n",
    "### Keras Model REgressor Ready for Grid Search\n",
    "def model_for_grid(resultdir,resample,epochs,**kwargs):\n",
    "#Needs variable epochs \n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min', \n",
    "                       verbose=1, \n",
    "                       patience=2000, \n",
    "                       restore_best_weights=True)\n",
    "    # CSV Logger\n",
    "    csv_logger = CSVLogger(f\"{resultdir}/csv_logger/best_grid_model_history_log_resample_{resample}.csv\", \n",
    "                           append=True)\n",
    "    # # CP_callbacks\n",
    "    # cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{resultdir}/model_checkpoint/training_{resample}/cp.ckpt',\n",
    "    #                                                  save_weights_only=True,\n",
    "    #                                                  verbose=1)\n",
    "    \n",
    "    # TensorBoard\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{resultdir}/tensorboard_logs/{resample}_best_grid', #/{batch}', # _ADAPTIVELEARNIGNRATE_01_10_Dense3_64_3CNN_lr_3_es\n",
    "                                                          update_freq = 1,\n",
    "                                                          # histogram_freq=1, \n",
    "                                                          write_graph=False, \n",
    "                                                          write_images=False)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    \n",
    "    # Covert to list and provide to Keras Regressor\n",
    "    keras_callbacks = [es, csv_logger, tensorboard_callback]\n",
    "    \n",
    "    ###initialise model\n",
    "        \"\"\")\n",
    "\n",
    "        f.write(f\"\"\"\n",
    "    model = KerasRegressor(model=build_model, \n",
    "                           batch=32,\n",
    "                            epochs=epochs,\n",
    "                            verbose=3,\n",
    "                            validation_split=0.2,\n",
    "                           callbacks=keras_callbacks,\n",
    "                           dense1={my_dict['dense1']},\n",
    "                            dense2={my_dict['dense2']},\n",
    "                            dense3={my_dict['dense3']},\n",
    "                           convn={my_dict['convn']}\n",
    "                          )\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "        f.write(\"\"\"\n",
    "\n",
    "    return model\"\"\")\n",
    "\n",
    "        f.write(f\"\"\"\n",
    "### Input Parameters and Workflow\n",
    "\n",
    "# Initialise\n",
    "file=pd.read_csv(\"Lomzov_dataset_IY.csv\")\n",
    "# parameters to work with\n",
    "# prop='Granulated'\n",
    "prop='{prop}'\n",
    "GSHT='{GSHT}'\n",
    "n_jobs={cores}\n",
    "\"\"\")\n",
    "\n",
    "        f.write(\"\"\"# obtain y and x data\n",
    "y_1, y_2, y_3, y_4, x, X= load_data(file,prop)\n",
    "\n",
    "# desc_type = ['RF-Score','H-Bonding','Granulated','DNA-Groups','OHEP','LP_dec2','CountDNA','CountDNAp']\n",
    "desc_type = ['Granulated','OHEP','LP_dec2']\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm'] #get the order correct\n",
    "#### WORK\n",
    "#### MAC sklearn for CNN\n",
    "\n",
    "###set global variables\n",
    "# train test split\n",
    "test_frac = 0.3\n",
    "home=os.getcwd()\n",
    "mc_cv=50\n",
    "n_folds=5\n",
    "\n",
    "epochs=200\n",
    "grid_number=1\n",
    "# Initialise train test split:\n",
    "train_test_split = ShuffleSplit(mc_cv, test_size=test_frac, random_state=1)\n",
    "train_test_split_hp = ShuffleSplit(n_folds, test_size=0.3, random_state=1)\n",
    "\n",
    "###define scoring dict for cv\n",
    "scorers = {\n",
    "    'r2':make_scorer(r2_func), \n",
    "    'rmse':make_scorer(rmse_func, greater_is_better=False), \n",
    "    'bias':make_scorer(bias_func, greater_is_better=False), \n",
    "    'sdep':make_scorer(sdep_func, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "# Monte Carlo CV:\n",
    "resample=0\n",
    "for train_idx, test_idx in train_test_split.split(x):\n",
    "    resample+=1   \n",
    "    print(f'resample: {resample}')\n",
    "    \n",
    "    \n",
    "    #     adjust y[2] to * Temperature /1000 dS*T kcal/mol\n",
    "        #for CNN with padding\n",
    "    x_train, x_test, y_train, y_test = wrapped_train_test_split(train_idx,test_idx,file,prop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### DEFINE MODEL \"\"\")\n",
    "\n",
    "        f.write(f\"\"\"\n",
    "    model_name = \"CNN_single_task_{prop}_{GSHT}_{cores}\"\n",
    "\n",
    "    if resample=={filter_count}:\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "        f.write(\"\"\"\n",
    "    \n",
    "        path=\"{}/CV/{}/{}/{}/{}\".format(os.getcwd(),resample,model_name,prop,GSHT)\n",
    "        \n",
    "        model = model_for_grid(path,resample,epochs) #resultdir\n",
    "        \n",
    "        create_dir(home,resample,model_name,prop,GSHT)\n",
    "        \n",
    "        ### Grid            \n",
    "        #provided in model_for_grid\n",
    "    \n",
    "        i = GSHT_list.index(GSHT)\n",
    "        \n",
    "    ########### Fid Model with Scikeras and scikit learn.\n",
    "        history = model.fit(x_train, y_train[i])\n",
    "    ############ Store results\n",
    "    \n",
    "        # results=pd.DataFrame(history.cv_results_)\n",
    "        # results.to_csv(path+f\"/gridsearch_resample_{resample}_best_grid.csv\")\n",
    "    \n",
    "        y_pred_test=history.predict(x_test)\n",
    "        y_pred_train=history.predict(x_train)\n",
    "    \n",
    "        save_splits(train_idx,y_train[i],y_pred_train,resample,path,f\"train_best_grid\")\n",
    "        save_splits(test_idx,y_test[i],y_pred_test,resample,path,f\"test_best_grid\")\n",
    "        \n",
    "# total_time=time.time()-time_start\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "        f.close\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09031987-a1a9-49ed-b45a-4f4c980ecf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f265f7-b491-4cc3-844a-f0d1c5eb31e2",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8f7bec76-dee8-4cbe-8c0e-a92ba4906539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "33353183-eca4-463a-b0b4-5f44870fb1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_1_DNA-Groups_dH_1_CNN_single_task.sh\n",
      "Submitted batch job 10653970\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_2_DNA-Groups_dH_1_CNN_single_task.sh\n",
      "Submitted batch job 10653971\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_3_DNA-Groups_dH_1_CNN_single_task.sh\n",
      "Submitted batch job 10653972\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_1_DNA-Groups_dS_1_CNN_single_task.sh\n",
      "Submitted batch job 10653973\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_2_DNA-Groups_dS_1_CNN_single_task.sh\n",
      "Submitted batch job 10653974\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_3_DNA-Groups_dS_1_CNN_single_task.sh\n",
      "Submitted batch job 10653975\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_1_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653976\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_2_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653977\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_3_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653978\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_4_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653979\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_5_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653980\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_6_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653981\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_7_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653982\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_8_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653983\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_9_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653984\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_10_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653985\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_11_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653986\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_12_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653987\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_13_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653988\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_14_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653989\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_15_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653990\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_16_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653991\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_17_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653992\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_18_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653993\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_19_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653994\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_20_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653995\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_21_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653996\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_22_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653997\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_23_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653998\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_24_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10653999\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_25_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654000\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_26_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654001\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_27_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654002\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_28_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654003\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_29_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654004\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_30_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654005\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_31_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654006\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_32_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654007\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_33_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654008\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_34_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654009\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_35_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654010\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_36_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654011\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_37_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654012\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_38_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654013\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_39_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654014\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_40_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654015\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_41_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654016\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_42_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654017\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_43_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654018\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_44_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654019\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_45_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654020\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_46_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654021\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_47_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654022\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_48_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654023\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_49_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654024\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_50_DNA-Groups_dG_1_CNN_single_task.sh\n",
      "Submitted batch job 10654025\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_1_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654026\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_2_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654027\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_3_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654028\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_4_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654029\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_5_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654030\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_6_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654031\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_7_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654032\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_8_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654033\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_9_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654034\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_10_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654035\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_11_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654036\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_12_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654037\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_13_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654038\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_14_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654039\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_15_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654040\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_16_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654041\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_17_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654042\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_18_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654043\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_19_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654044\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_20_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654045\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_21_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654046\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_22_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654047\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_23_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654048\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_24_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654049\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_25_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654050\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_26_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654051\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_27_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654052\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_28_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654053\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_29_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654054\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_30_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654055\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_31_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654056\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_32_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654057\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_33_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654058\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_34_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654059\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_35_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654060\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_36_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654061\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_37_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654062\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_38_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654063\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_39_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654064\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_40_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654065\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_41_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654066\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_42_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654067\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_43_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654068\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_44_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654069\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_45_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654070\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_46_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654071\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_47_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654072\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_48_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654073\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_49_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654074\n",
      "/users/qdb16186/CNN_single_task_grid/sbatch_S_50_DNA-Groups_Tm_1_CNN_single_task.sh\n",
      "Submitted batch job 10654075\n"
     ]
    }
   ],
   "source": [
    "scalling='No_scalling'\n",
    "set_name= ' Test' \n",
    "model_name= 'CNN_single_task'\n",
    "descriptor_name = \"Granulated\" \n",
    "prop='G'\n",
    "test_data1=pd.DataFrame(columns=['params'])\n",
    "\n",
    "\n",
    "GSHT_list=['dH','dS','dG','Tm']\n",
    "prop_list=['H', 'S', 'G', 'T']\n",
    "cores=10\n",
    "\n",
    "os.chdir('/users/qdb16186')\n",
    "for scalling in ['No_scalling']:\n",
    "    for set_name in ['Test']:#, ' Train']:\n",
    "        for model_name in ['CNN_single_task']:\n",
    "            for descriptor_name in ['DNA-Groups']:#['DNA-Groups','Granulated','OHEP','LP_dec2']:\n",
    "                for prop in ['H', 'S', 'G', 'T']:#['H', 'S', 'G', 'T']:\n",
    "                    GSHT = GSHT_list[prop_list.index(prop)]\n",
    "                    os.chdir('/users/qdb16186')\n",
    "                    home=os.getcwd()\n",
    "                    \n",
    "                    p = Path('./CNN_single_task_grid/CV/')                  \n",
    "                    if prop == 'T':\n",
    "                        somelist=list(p.glob(f'**/{model_name}_{descriptor_name}_{prop}m_10/{descriptor_name}/{prop}m/gridsearch_*_{scalling}.csv'))\n",
    "                    else:\n",
    "                        somelist=list(p.glob(f'**/{model_name}_{descriptor_name}_d{prop}_10/{descriptor_name}/d{prop}/gridsearch_*_{scalling}.csv'))\n",
    "                    if len(somelist) ==0:\n",
    "                        print('somelist is empty')\n",
    "                        break\n",
    "\n",
    "                    filter_count=0\n",
    "                    for i in somelist:\n",
    "                            filter_count+=1\n",
    "                            df=pd.read_csv(i.resolve().absolute().as_posix())\n",
    "                            string_representation = df.loc[df[f'mean_test_rmse'].idxmax(),'params']\n",
    "\n",
    "                            my_dict = ast.literal_eval(string_representation)\n",
    "                        \n",
    "                            # GSHT = GSHT_list[prop_list.index(prop)]\n",
    "                        \n",
    "                            python_script_best_grid(abs_dir,descriptor_name,GSHT,cores,my_dict,filter_count)\n",
    "                            # descriptor_name substituted prop\n",
    "                            bash_script_gen_single(abs_dir,descriptor_name,GSHT,cores,filter_count)\n",
    "                            # descriptor_name substituted prop\n",
    "                            \n",
    "                            job_name=f\"S_{filter_count}_{descriptor_name}_{GSHT}_1_CNN_single_task\"\n",
    "                            print(f'{abs_dir}/sbatch_{job_name}.sh')\n",
    "                            bashCommand = f\"sbatch {abs_dir}/sbatch_{job_name}.sh\"\n",
    "                            os.chdir(abs_dir)\n",
    "                            os.system(bashCommand)\n",
    "                            os.chdir('/users/qdb16186')\n",
    "                            # run()\n",
    "                            # if filter_count>=1:\n",
    "                            #     print(f'break on {filter_count}')\n",
    "                            #     break\n",
    "    \n",
    "                            \n",
    "                    \n",
    "                           \n",
    "                    # try:\n",
    "                    #     os.mkdir(f'{scalling}')\n",
    "                    # except:\n",
    "                    #     pass\n",
    "                    # os.chdir(f\"{scalling}\")\n",
    "                    os.chdir(home)\n",
    "\n",
    "os.chdir('/users/qdb16186')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3127216e-8465-4e11-ba51-6449987653e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA-Groups\n",
      "3\n",
      "3\n",
      "50\n",
      "50\n",
      "Granulated\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "OHEP\n",
      "15\n",
      "3\n",
      "50\n",
      "50\n",
      "LP_dec2\n",
      "50\n",
      "5\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/users/qdb16186')\n",
    "for scalling in ['No_scalling']:\n",
    "    for set_name in ['Test']:#, ' Train']:\n",
    "        for model_name in ['CNN_single_task']:\n",
    "            for descriptor_name in ['DNA-Groups','Granulated','OHEP','LP_dec2']:#['DNA-Groups','Granulated','OHEP','LP_dec2']:\n",
    "                print(descriptor_name)\n",
    "                for prop in ['H', 'S', 'G', 'T']:#['H', 'S', 'G', 'T']:\n",
    "                    GSHT = GSHT_list[prop_list.index(prop)]\n",
    "                    os.chdir('/users/qdb16186')\n",
    "                    home=os.getcwd()\n",
    "                    \n",
    "                    p = Path('./CNN_single_task_grid/CV/')                  \n",
    "                    if prop == 'T':\n",
    "                        somelist=list(p.glob(f'**/{model_name}_{descriptor_name}_{prop}m_10/{descriptor_name}/{prop}m/gridsearch_*_{scalling}.csv'))\n",
    "                    else:\n",
    "                        somelist=list(p.glob(f'**/{model_name}_{descriptor_name}_d{prop}_10/{descriptor_name}/d{prop}/gridsearch_*_{scalling}.csv'))\n",
    "                    if len(somelist) ==0:\n",
    "                        print('somelist is empty')\n",
    "                        break\n",
    "                    print(len(somelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "726905a6-2f35-42be-a944-c70e9409c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "77298bd7-ae09-4237-b46b-719a3de716d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for scalling in ['No_scalling']:\n",
    "    for set_name in ['Test']:#, ' Train']:\n",
    "        for model_name in ['CNN_single_task']:\n",
    "            for descriptor_name in ['Granulated']:#['DNA-Groups','Granulated','OHEP','LP_dec2']:\n",
    "                for prop in ['T']:#['H', 'S', 'G', 'T']:\n",
    "                    GSHT = GSHT_list[prop_list.index(prop)]\n",
    "                    os.chdir('/users/qdb16186')\n",
    "                    home=os.getcwd()\n",
    "                    \n",
    "                    p = Path('./CNN_single_task_grid/CV/')                  \n",
    "                    if prop == 'T':\n",
    "                        somelist=list(p.glob(f'**/{model_name}_{descriptor_name}_{prop}m_10/{descriptor_name}/{prop}m/*best_grid.csv'))\n",
    "                    else:\n",
    "                        somelist=list(p.glob(f'**/{model_name}_{descriptor_name}_d{prop}_10/{descriptor_name}/d{prop}/*best_grid.csv'))\n",
    "                    if len(somelist) ==0:\n",
    "                        print('somelist is empty')\n",
    "                        break\n",
    "\n",
    "                    filter_count=0\n",
    "                    for i in somelist:\n",
    "                            filter_count+=1\n",
    "                            df=pd.read_csv(i.resolve().absolute().as_posix())\n",
    "                            plt.scatter(df.iloc[:,1], df.iloc[:,2])\n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f7d9d95b-8f1b-4780-9f92-f32e300667ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>42.0</td>\n",
       "      <td>37.916820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.239986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192</td>\n",
       "      <td>45.8</td>\n",
       "      <td>41.808280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224</td>\n",
       "      <td>31.9</td>\n",
       "      <td>33.078354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>284</td>\n",
       "      <td>36.9</td>\n",
       "      <td>38.034073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>203</td>\n",
       "      <td>53.8</td>\n",
       "      <td>44.823300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>255</td>\n",
       "      <td>43.9</td>\n",
       "      <td>41.710487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>72</td>\n",
       "      <td>37.9</td>\n",
       "      <td>37.820720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>235</td>\n",
       "      <td>46.5</td>\n",
       "      <td>44.538330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>37</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.475510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  y_true     y_pred\n",
       "0    188    42.0  37.916820\n",
       "1    290    40.0  37.239986\n",
       "2    192    45.8  41.808280\n",
       "3    224    31.9  33.078354\n",
       "4    284    36.9  38.034073\n",
       "..   ...     ...        ...\n",
       "208  203    53.8  44.823300\n",
       "209  255    43.9  41.710487\n",
       "210   72    37.9  37.820720\n",
       "211  235    46.5  44.538330\n",
       "212   37    28.0  29.475510\n",
       "\n",
       "[213 rows x 3 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548032ff-ecf0-4742-8d52-ec05d286a935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
